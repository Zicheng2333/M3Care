{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T16:55:24.928791Z",
     "start_time": "2021-01-19T16:55:24.492755Z"
    }
   },
   "outputs": [],
   "source": [
    "# modality interpolation\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "new_X = pickle.load(open(\"X_afm.pkl\",'rb'))\n",
    "new_Y = pickle.load(open(\"Y_afm.pkl\",'rb'))\n",
    "new_f = pickle.load(open(\"total_f.pkl\",'rb'))\n",
    "\n",
    "x_silicon = pickle.load(open(\"x_silicon_afm.pkl\",'rb'))\n",
    "last_one_procceed_silicon_value = pickle.load(open(\"x_silicon_last_once_afm.pkl\",'rb'))\n",
    "x_silicon_len = pickle.load(open(\"x_silicon_len_afm.pkl\",'rb'))\n",
    "silicon_time = pickle.load(open(\"silicon_time_afm.pkl\",'rb'))\n",
    "sili_feas = pickle.load(open(\"silicon_faeas_afm.pkl\",'rb'))\n",
    "\n",
    "x_med = pickle.load(open(\"x_med_afm.pkl\",'rb'))\n",
    "# last_one_procceed_silicon_value = pickle.load(open(\"x_silicon_last_once_afm.pkl\",'rb'))\n",
    "x_med_len = pickle.load(open(\"x_med_len_afm.pkl\",'rb'))\n",
    "med_time = pickle.load(open(\"med_time_afm.pkl\",'rb'))\n",
    "med_feas = pickle.load(open(\"med_feas_afm.pkl\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831\n"
     ]
    }
   ],
   "source": [
    "jieba_procceed_dp = pickle.load(open(\"jieba_procceed_dp_afm.pkl\",'rb'))\n",
    "dp_time = pickle.load(open(\"dp_time_afm.pkl\",'rb'))\n",
    "\n",
    "dp_mask = []\n",
    "for i in jieba_procceed_dp:\n",
    "    if len(i)== 1 and (i[0][0] == '无'):\n",
    "        dp_mask.append(0)\n",
    "    else:\n",
    "        dp_mask.append(1)\n",
    "print(sum(dp_mask))\n",
    "\n",
    "cls_procceed_dp = []\n",
    "for i in range(len(jieba_procceed_dp)):\n",
    "    cur_cls_dp = []\n",
    "    for j in range(len(jieba_procceed_dp[i])):\n",
    "        cur_cur_dp = ['[CLS]']\n",
    "        cur_cur_dp.extend(jieba_procceed_dp[i][j])\n",
    "        cur_cls_dp.append(cur_cur_dp)\n",
    "    cls_procceed_dp.append(cur_cls_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n"
     ]
    }
   ],
   "source": [
    "jieba_procceed_in = pickle.load(open(\"jieba_procceed_in_afm.pkl\",'rb'))\n",
    "in_time = pickle.load(open(\"in_time_afm.pkl\",'rb'))\n",
    "\n",
    "in_mask = []\n",
    "for i in jieba_procceed_in:\n",
    "    if len(i)== 1 and (i[0][0] == '无'):\n",
    "        in_mask.append(0)\n",
    "    else:\n",
    "        in_mask.append(1)\n",
    "print(sum(in_mask))\n",
    "\n",
    "cls_procceed_in = []\n",
    "for i in range(len(jieba_procceed_in)):\n",
    "    cur_cls_dp = []\n",
    "    for j in range(len(jieba_procceed_in[i])):\n",
    "        cur_cur_dp = ['[CLS]']\n",
    "        cur_cur_dp.extend(jieba_procceed_in[i][j])\n",
    "        cur_cls_dp.append(cur_cur_dp)\n",
    "    cls_procceed_in.append(cur_cls_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746\n"
     ]
    }
   ],
   "source": [
    "jieba_procceed_out = pickle.load(open(\"jieba_procceed_out_afm.pkl\",'rb'))\n",
    "out_time = pickle.load(open(\"out_time_afm.pkl\",'rb'))\n",
    "\n",
    "out_mask = []\n",
    "for i in jieba_procceed_out:\n",
    "    if len(i)== 1 and (i[0][0] == '无'):\n",
    "        out_mask.append(0)\n",
    "    else:\n",
    "        out_mask.append(1)\n",
    "print(sum(out_mask))\n",
    "\n",
    "cls_procceed_out = []\n",
    "for i in range(len(jieba_procceed_out)):\n",
    "    cur_cls_dp = []\n",
    "    for j in range(len(jieba_procceed_out[i])):\n",
    "        cur_cur_dp = ['[CLS]']\n",
    "        cur_cur_dp.extend(jieba_procceed_out[i][j])\n",
    "        cur_cls_dp.append(cur_cur_dp)\n",
    "    cls_procceed_out.append(cur_cls_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798\n",
      "704\n",
      "831\n",
      "746\n",
      "746\n"
     ]
    }
   ],
   "source": [
    "sili_mask = []\n",
    "for i in x_silicon:\n",
    "    if len(i)== 1 and (i[0] == np.zeros(28)).all():\n",
    "        sili_mask.append(0)\n",
    "    else:\n",
    "        sili_mask.append(1)\n",
    "print(sum(sili_mask))\n",
    "\n",
    "med_mask = []\n",
    "for i in x_med:\n",
    "    if len(i)== 1 and (i[0] == np.zeros(853)).all():\n",
    "        med_mask.append(0)\n",
    "    else:\n",
    "        med_mask.append(1)\n",
    "print(sum(med_mask))\n",
    "\n",
    "dp_mask = []\n",
    "for i in jieba_procceed_dp:\n",
    "    if len(i)== 1 and (i[0][0] == '无'):\n",
    "        dp_mask.append(0)\n",
    "    else:\n",
    "        dp_mask.append(1)\n",
    "print(sum(dp_mask))\n",
    "\n",
    "in_mask = []\n",
    "for i in jieba_procceed_in:\n",
    "    if len(i)== 1 and (i[0][0] == '无'):\n",
    "        in_mask.append(0)\n",
    "    else:\n",
    "        in_mask.append(1)\n",
    "print(sum(in_mask))\n",
    "\n",
    "out_mask = []\n",
    "for i in jieba_procceed_out:\n",
    "    if len(i)== 1 and (i[0][0] == '无'):\n",
    "        out_mask.append(0)\n",
    "    else:\n",
    "        out_mask.append(1)\n",
    "print(sum(out_mask))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0925480769230769"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(x_silicon[i]) for i in range(len(x_silicon))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T16:55:40.436043Z",
     "start_time": "2021-01-19T16:55:40.429144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() == True else 'cpu')\n",
    "# device = torch.device('cuda')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T16:56:02.379666Z",
     "start_time": "2021-01-19T16:56:02.366034Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(x, y, dp, in_hos, out_hos, med, med_lens, sili, sili_lens,  \\\n",
    "               dp_mask, in_mask, out_mask, med_mask, sili_mask,batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size)  # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]  # fetch out all the induces\n",
    "\n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx], dp[idx], in_hos[idx], out_hos[idx], med[idx], med_lens[idx], \\\n",
    "                             sili[idx], sili_lens[idx], dp_mask[idx], in_mask[idx], out_mask[idx], \\\n",
    "                             med_mask[idx], sili_mask[idx]))\n",
    "\n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_dp = [e[2] for e in examples]\n",
    "        batch_in_hos = [e[3] for e in examples]\n",
    "        batch_out_hos = [e[4] for e in examples]\n",
    "        batch_med = [e[5] for e in examples]\n",
    "        batch_med_lens = [e[6] for e in examples]\n",
    "        batch_sili = [e[7] for e in examples]\n",
    "        batch_sili_lens = [e[8] for e in examples]\n",
    "        batch_dp_mask = [e[9] for e in examples]\n",
    "        batch_in_mask = [e[10] for e in examples]\n",
    "        batch_out_mask = [e[11] for e in examples]\n",
    "        batch_med_mask = [e[12] for e in examples]\n",
    "        batch_sili_mask = [e[13] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_dp, batch_in_hos, batch_out_hos, \\\n",
    "        batch_med, batch_med_lens, batch_sili, batch_sili_lens, \\\n",
    "        [batch_dp_mask, batch_in_mask, batch_out_mask, batch_med_mask, batch_sili_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 120)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import math, copy, time\n",
    "from model_embeddings import ModelEmbeddings\n",
    "# Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.selu = nn.SELU()\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.selu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "class NMT_tran(nn.Module):\n",
    "   \n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        \"\"\" Init NMT Model.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param hidden_size (int): Hidden Size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        @param dropout_rate (float): Dropout probability, for attention\n",
    "        \"\"\"\n",
    "        super(NMT_tran, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(hidden_size, vocab)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "      \n",
    "        c = copy.deepcopy\n",
    "        attn = MultiHeadedAttention(8, self.hidden_size)\n",
    "        ff = PositionwiseFeedForward(self.hidden_size, self.hidden_size*4, self.dropout_rate)\n",
    "        self.position = PositionalEncoding(embed_size, dropout_rate)\n",
    "        self.encoder = Encoder(EncoderLayer(hidden_size, c(attn), c(ff), dropout_rate), 1)\n",
    "\n",
    "        self.high_encoder = Encoder(EncoderLayer(hidden_size, c(attn), c(ff), dropout_rate), 1)\n",
    "       \n",
    "        self.opt = nn.Linear(\n",
    "            in_features=(hidden_size), out_features=1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.selu = nn.SELU()\n",
    "        self.elu = nn.ELU()\n",
    "      \n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "\n",
    "    def forward(self, source: List[List[List[str]]]) -> torch.Tensor:\n",
    "        \n",
    "        # Compute sentence lengths\n",
    "        source_lengths = [len(s) for s in source]\n",
    "\n",
    "        total_src = []\n",
    "        for i in source:\n",
    "            total_src.extend(i)\n",
    "\n",
    "        # Convert list of lists into tensors\n",
    "        total_src_padded = self.vocab.src.to_input_tensor(\n",
    "            total_src, device=self.device)   # Tensor: (src_len, b)\n",
    "        \n",
    "        if total_src_padded.shape[0]>400:\n",
    "            total_src_padded = total_src_padded[:400,:]\n",
    "#         print(total_src_padded.shape)\n",
    "        \n",
    "        total_src_lengths = [len(s) for s in total_src]\n",
    "        \n",
    "       \n",
    "        enc_hiddens, first_hidden = self.encode(\n",
    "            total_src_padded)\n",
    "\n",
    "        last_hiddens = first_hidden\n",
    "#         print(last_hiddens)\n",
    "\n",
    "        return2source = []\n",
    "        return2source_counter = 0\n",
    "        for i in source_lengths:\n",
    "            return2source.append(last_hiddens[return2source_counter:return2source_counter+i])\n",
    "            return2source_counter += i\n",
    "\n",
    "        assert(len(total_src_lengths) == return2source_counter)\n",
    "        \n",
    "#         print(return2source[0])\n",
    "\n",
    "        \n",
    "        \n",
    "        return2source_padded = self.pad_sentence_embeddings(return2source, torch.zeros_like(return2source[0][0],device=device))\n",
    "#         print(return2source_padded[1])\n",
    "        \n",
    "        return2source_padded = torch.stack(return2source_padded)\n",
    "#         print(return2source_padded.shape)\n",
    "        return2source_padded_mask = (return2source_padded != torch.zeros_like(return2source[0][0],device=device))[:,:,0].unsqueeze(-2)\n",
    "#         print(return2source_padded_mask.shape)\n",
    "        \n",
    "      \n",
    "#         X_padded = pack_padded_sequence(return2source_padded.permute(1,0,2), source_lengths,enforce_sorted=False)\n",
    "        \n",
    "        enc_hiddens = self.high_encoder(return2source_padded, return2source_padded_mask)\n",
    "#         print(enc_hiddens.shape)#b t h\n",
    "\n",
    "#         opt = self.sigmoid(self.opt(enc_hiddens[:,0,:].squeeze(0)))\n",
    "       \n",
    "#         os = []\n",
    "#         for i in range(len(source_lengths)):\n",
    "#             os.append(enc_hiddens[i,source_lengths[i]-1].squeeze(0))\n",
    "\n",
    "            \n",
    "#         os = torch.stack(os)\n",
    "\n",
    "        \n",
    "        \n",
    "        return enc_hiddens, source_lengths\n",
    "#     enc_hiddens[:,0,:].squeeze(0)\n",
    "\n",
    "    def pad_sentence_embeddings(self, sents, pad_token):\n",
    "        \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
    "        @param sents (list[list[str]]): list of sentences, where each sentence\n",
    "                                        is represented as a list of words\n",
    "        @param pad_token (str): padding token\n",
    "        @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
    "            than the max length sentence are padded out with the pad_token, such that\n",
    "            each sentences in the batch now has equal length.\n",
    "        \"\"\"\n",
    "        sents_padded = []\n",
    "        mask = []\n",
    "\n",
    "        # YOUR CODE HERE (~6 Lines)\n",
    "        max_sent_count = max([len(sent) for sent in sents])\n",
    "        for sent in sents:\n",
    "            if len(sent) < max_sent_count:\n",
    "                leftovers = [pad_token] * (max_sent_count - len(sent))\n",
    "                sent = torch.cat((sent,torch.stack(leftovers)),dim=0)\n",
    "                \n",
    "\n",
    "                \n",
    "            sents_padded.append(sent)\n",
    "#             mask.append(cur_mask)\n",
    "        # END YOUR CODE\n",
    "\n",
    "        return sents_padded\n",
    "\n",
    "    \n",
    "    def encode(self, source_padded: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
    "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
    "\n",
    "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
    "                                        b = batch_size, src_len = maximum source sentence length. Note that\n",
    "                                       these have already been sorted in order of longest to shortest sentence.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
    "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
    "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
    "                                                hidden state and cell.\n",
    "        \"\"\"\n",
    "        enc_hiddens, dec_init_state = None, None\n",
    "\n",
    "        # print(source_padded.shape)\n",
    "        source_padded = source_padded.permute(1,0) # b t \n",
    "#         print(source_padded.shape)\n",
    "        src_mask = (source_padded != 0).unsqueeze(-2)\n",
    "#         print(src_mask.shape)# b 1 t \n",
    "#         X = self.position(self.model_embeddings.source(source_padded))  # (src_len, b, e) new: b t e\n",
    "        X = self.model_embeddings.source(source_padded)\n",
    "#         print(X.shape)\n",
    "       \n",
    "        # X = pack_padded_sequence(X, source_lengths)\n",
    "        \n",
    "        # enc_hiddens, (last_hidden, last_cell) = self.encoder(X)\n",
    "        \n",
    "        enc_hiddens = self.encoder(X, src_mask) # b t h\n",
    "        first_hidden = enc_hiddens[:,0,:]\n",
    "\n",
    "       \n",
    "        return enc_hiddens, first_hidden\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
    "        \"\"\"\n",
    "        return self.model_embeddings.source.weight.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize source vocabulary ..\n",
      "number of word types: 10672, number of word types w/ frequency >= 2: 7969\n"
     ]
    }
   ],
   "source": [
    "from docopt import docopt\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from utils1 import read_corpus\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "vocab = Vocab.build(cls_procceed_dp+cls_procceed_in+cls_procceed_out, 50000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "# def tile(a, dim, n_tile):\n",
    "#     init_dim = a.size(dim)\n",
    "#     repeat_idx = [1] * a.dim()\n",
    "#     repeat_idx[dim] = n_tile\n",
    "#     a = a.repeat(*(repeat_idx))\n",
    "#     order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n",
    "#     return torch.index_select(a, dim, order_index)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module): # new added\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.selu = nn.SELU()\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.selu(self.w_1(x))))\n",
    "\n",
    "# class PositionwiseFeedForwardConv(nn.Module):\n",
    "\n",
    "#     def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
    "#         super(PositionalWiseFeedForward, self).__init__()\n",
    "#         self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "#         self.w2 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = x.transpose(1, 2)\n",
    "#         output = self.w2(F.relu(self.w1(output)))\n",
    "#         output = self.dropout(output.transpose(1, 2))\n",
    "\n",
    "#         # add residual and norm layer\n",
    "#         output = self.layer_norm(x + output)\n",
    "#         return output\n",
    "\n",
    "class PositionalEncoding(nn.Module): # new added\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=400):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(self.pe.shape)\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0 # 下三角矩阵\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)# b h t d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # b h t t\n",
    "    \n",
    "    \n",
    "#     if print_flagflag:\n",
    "#         print(\"scores :\")\n",
    "#         print(scores[0][0])\n",
    "    if mask is not None:# 1 1 t t\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)# b h t t 下三角\n",
    "    p_attn = F.softmax(scores, dim = -1)# b h t t\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "#     if print_flag:\n",
    "#         print(\"p_attn :\")\n",
    "#         print(p_attn[0][0])\n",
    "    return torch.matmul(p_attn, value), p_attn # b h t v (d_k) \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, self.d_k * self.h), 3)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 1 1 t t\n",
    "\n",
    "        nbatches = query.size(0)# b\n",
    "        \n",
    "        # d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))] # b h t d_k\n",
    "        \n",
    "       \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)# b h t v (d_k) \n",
    "        \n",
    "#         print()\n",
    "        \n",
    "      \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)# b t d_model\n",
    "\n",
    "        return self.final_linear(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-7):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class vanilla_transformer_encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(vanilla_transformer_encoder, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        #self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        #self.d_k = d_k  \n",
    "        #self.d_v = d_v # the two can be equal\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.embed = nn.Linear(self.input_dim, self.d_model)\n",
    "\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 5000)\n",
    "       \n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "        self.output = nn.Linear(self.d_model, self.output_dim)\n",
    "        \n",
    "        self.output = nn.Linear(128, self.output_dim)\n",
    "        \n",
    "#         self.reduce_dim = nn.Linear(self.input_dim * 48, 128)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "        #self.sparsemax = Sparsemax(dim=0)\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demographic = demo_input\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "     \n",
    "        embed_input = self.embed(input)\n",
    "#         print(input.shape)\n",
    "        posi_input = self.PositionalEncoding(embed_input)# b t d_model\n",
    "#         posi_input = embed_input# b t d_model\n",
    "\n",
    "        \n",
    "#         mask = subsequent_mask(time_step).to(device) # 1 t t 下三角\n",
    "#         print(mask)\n",
    "        contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, mask))# b t d_model\n",
    "        #contexts = self.MultiHeadedAttention(qs, ks, vs, mask)# b t h\n",
    "\n",
    "        contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))# b t d_model\n",
    "        \n",
    "#         contexts = contexts.view(batch_size, 16 * time_step)\n",
    "\n",
    "#\n",
    "#         os = []\n",
    "#         for j in range(contexts.shape[0]):\n",
    "#             os.append(contexts[j,lens[j]-1])\n",
    "            \n",
    "#         os = torch.stack(os)\n",
    "    \n",
    "#         output = self.output(os)# b t 1\n",
    "#         output = self.sigmoid(output)\n",
    "          \n",
    "        return contexts\n",
    "    #, self.MultiHeadedAttention.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T17:16:09.837915Z",
     "start_time": "2021-01-19T17:16:09.823413Z"
    }
   },
   "outputs": [],
   "source": [
    "class MM_transformer_encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(MM_transformer_encoder, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        #self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        #self.d_k = d_k  \n",
    "        #self.d_v = d_v # the two can be equal\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.embed = nn.Linear(self.input_dim, self.d_model)\n",
    "\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 5000)\n",
    "       \n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "        self.output = nn.Linear(self.d_model, self.output_dim)\n",
    "        \n",
    "#         self.output = nn.Linear(128, self.output_dim)\n",
    "        \n",
    "#         self.reduce_dim = nn.Linear(self.input_dim * 48, 128)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "        #self.sparsemax = Sparsemax(dim=0)\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demographic = demo_input\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "     \n",
    "        input = self.embed(input)\n",
    "#         print(input.shape)\n",
    "        input = self.PositionalEncoding(input)# b t d_model\n",
    "#         posi_input = embed_input# b t d_model\n",
    "\n",
    "        \n",
    "#         mask = subsequent_mask(time_step).to(device) # 1 t t 下三角\n",
    "#         print(mask)\n",
    "        contexts = self.SublayerConnection(input, lambda x: self.MultiHeadedAttention(input, input, input, mask))# b t d_model\n",
    "        #contexts = self.MultiHeadedAttention(qs, ks, vs, mask)# b t h\n",
    "\n",
    "        contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))# b t d_model\n",
    "        \n",
    "#         contexts = contexts.view(batch_size, 16 * time_step)\n",
    "\n",
    "#\n",
    "#         os = []\n",
    "#         for j in range(contexts.shape[0]):\n",
    "#             os.append(contexts[j,lens[j]-1])\n",
    "            \n",
    "#         os = torch.stack(os)\n",
    "    \n",
    "#         output = self.output(os)# b t 1\n",
    "#         output = self.sigmoid(output)\n",
    "          \n",
    "        return contexts # b t h\n",
    "    #, self.MultiHeadedAttention.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    b = x.size(0)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(b, b)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(b, b).t()\n",
    "    dist = xx+yy-2*torch.mm(x, y.t())\n",
    "    return dist\n",
    "\n",
    "def guassian_kernel(source, kernel_mul=2.0, kernel_num=1, fix_sigma=None):\n",
    "    n = source.size(0)\n",
    "    L2_distance = euclidean_dist(source, source)\n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n**2-n)\n",
    "        \n",
    "#     print(bandwidth)\n",
    "    bandwidth /= kernel_mul ** (kernel_num//2)\n",
    "    bandwidth_list = [bandwidth*(kernel_mul**i) for i in range(kernel_num)]\n",
    "    kernel_val = [torch.exp(-L2_distance/bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "    return sum(kernel_val)/len(kernel_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our MAPLE framework\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "#         print(self.dropout)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha# 4 leakyrelu\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.attention = None\n",
    "\n",
    "    def forward(self, input, adj):# V N, V V\n",
    "        h = torch.mm(input, self.W)# V O\n",
    "        N = h.size()[0]# NUM OF V\n",
    "\n",
    "        # V*V O ->123412341234, V*V O -> 111222333444, V V 2O\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))# V V\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)# V V\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        self.attention = attention\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)# V V\n",
    "        h_prime = torch.matmul(attention, h)# V N\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "        \n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "    \n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features).float())\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features).float())\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        std = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, adj, x):\n",
    "        y = torch.mm(x.float(), self.weight.float())\n",
    "        output = torch.mm(adj.float(), y.float())\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias.float()\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3Care(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_size, output_dim=1, keep_prob=1):\n",
    "        super(M3Care, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "        self.modal_num = 6\n",
    "        \n",
    "        self.NLP_model = NMT_tran(embed_size=embed_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                dropout_rate= 1 - self.keep_prob,\n",
    "                vocab=vocab).to(device)\n",
    "        \n",
    "        self.in_model = NMT_tran(embed_size=embed_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                dropout_rate= 1 - self.keep_prob,\n",
    "                vocab=vocab).to(device)\n",
    "        \n",
    "        self.out_model = NMT_tran(embed_size=embed_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                dropout_rate= 1 - self.keep_prob,\n",
    "                vocab=vocab).to(device)\n",
    "        \n",
    "        self.med_model = vanilla_transformer_encoder(input_dim = 853, d_model = self.hidden_dim, \\\n",
    "                                                     MHD_num_head = 4 , d_ff = self.hidden_dim*4, output_dim = 1).to(device)\n",
    "        self.med2current = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.sili_model = vanilla_transformer_encoder(input_dim = 28, d_model = self.hidden_dim,  \\\n",
    "                                                      MHD_num_head = 4 , d_ff = self.hidden_dim*4, output_dim = 1).to(device)\n",
    "        self.sili2current = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.linear_s = nn.Linear(28, self.hidden_dim)\n",
    "#         self.linear2 = nn.Linear(self.hidden_dim *5, self.output_dim)\n",
    "        self.MM_model = MM_transformer_encoder(input_dim = self.hidden_dim, d_model = self.hidden_dim,  \\\n",
    "                                                      MHD_num_head = 4 , d_ff = self.hidden_dim*4, output_dim = 1).to(device)\n",
    "        self.MM_model2 = MM_transformer_encoder(input_dim = self.hidden_dim, d_model = self.hidden_dim,  \\\n",
    "                                                      MHD_num_head = 1 , d_ff = self.hidden_dim*4, output_dim = 1).to(device)\n",
    "\n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(6, self.hidden_dim)\n",
    "        self.token_type_embeddings.apply(init_weights)\n",
    "        \n",
    "        \n",
    "        self.sep_token_embeddings = nn.Embedding(6, self.hidden_dim)\n",
    "        self.sep_token_embeddings.apply(init_weights)\n",
    "        \n",
    "        self.PositionalEncoding = PositionalEncoding(self.hidden_dim, dropout = 0, max_len = 5000)\n",
    "#         emb_data = self.token_type_embeddings.weight.data\n",
    "#         self.token_type_embeddings = nn.Embedding(3, hs)\n",
    "#         self.token_type_embeddings.apply(objectives.init_weights)\n",
    "#         self.token_type_embeddings.weight.data[0, :] = emb_data[0, :]\n",
    "#         self.token_type_embeddings.weight.data[1, :] = emb_data[1, :]\n",
    "#         self.token_type_embeddings.weight.data[2, :] = emb_data[1, :]\n",
    "\n",
    "        self.threshold = nn.Parameter(torch.zeros(size=(1,)))\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "        \n",
    "        self.proj1 = nn.Linear(self.hidden_dim *7, self.hidden_dim*2 )\n",
    "        self.proj2 = nn.Linear(self.hidden_dim *5, self.hidden_dim *5)\n",
    "        self.out_layer = nn.Linear(self.hidden_dim *2, self.output_dim)\n",
    "        \n",
    "        self.proj_in_out = nn.Linear(self.hidden_dim *2, self.hidden_dim)\n",
    "        self.proj_med_sili = nn.Linear(self.hidden_dim *2, self.hidden_dim)\n",
    "        \n",
    "        self.simiProj = clones(torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "            self.selu,\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "            self.selu,\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "#             torch.nn.Softplus(),\n",
    "#             torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "        ), self.modal_num)\n",
    "        \n",
    "        self.eps0 = nn.Parameter(torch.zeros(size=(1,))-0.3)\n",
    "        self.eps1 = nn.Parameter(torch.zeros(size=(1,))-0.3)\n",
    "        self.eps2 = nn.Parameter(torch.zeros(size=(1,))-0.3)\n",
    "        self.eps3 = nn.Parameter(torch.zeros(size=(1,))-0.3)\n",
    "        self.eps4 = nn.Parameter(torch.zeros(size=(1,))-0.3)\n",
    "        self.eps5 = nn.Parameter(torch.zeros(size=(1,))-0.3)\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    \n",
    "        self.GCN = clones(GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True),self.modal_num)\n",
    "        self.GCN_2 = clones(GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True),self.modal_num)\n",
    "        self.GCN_3 = clones(GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True),self.modal_num)\n",
    "        \n",
    "        self.weight1 = clones(nn.Linear(self.hidden_dim, 1),self.modal_num)\n",
    "        self.weight2 = clones(nn.Linear(self.hidden_dim, 1),self.modal_num)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "\n",
    "    def forward(self, input,  pds, in_hos, out_hos, meds, med_lens, sili, sili_lens, d_i_o_m_s_masks):\n",
    "        \n",
    "        \n",
    "        values_hidden00 = self.selu(self.linear1(input))# b 1\n",
    "\n",
    "        \n",
    "        pd_contexts, pd_lens = self.NLP_model(pds)# b t h\n",
    "#         pd_contexts = self.selu(pd_contexts)\n",
    "        dp_mask = length_to_mask(torch.from_numpy(np.array(pd_lens))).unsqueeze(1).to(device)\n",
    "        \n",
    " \n",
    "        med_mask = length_to_mask(med_lens).unsqueeze(1)\n",
    "        med_contexts = self.med2current(self.selu(self.med_model(meds, med_mask)))# b 1\n",
    "#         med_contexts = self.selu(self.med2current(self.selu(self.med_model(meds, med_mask))))# b 1\n",
    "        \n",
    "        \n",
    "        in_contexts, in_lens = self.in_model(in_hos)# b 1\n",
    "#         in_contexts = self.selu(in_contexts)\n",
    "        \n",
    "\n",
    "        \n",
    "        out_contexts, out_lens = self.out_model(out_hos)# b 1\n",
    "#         out_contexts = self.selu(out_contexts)\n",
    "\n",
    "        \n",
    "        sili_mask = length_to_mask(sili_lens).unsqueeze(1)\n",
    "        sili_contexts = self.sili2current(self.selu(self.sili_model(sili, sili_mask)))# b 1\n",
    "#         sili_contexts = self.selu(self.sili2current(self.selu(self.sili_model(sili, sili_mask))))# b 1\n",
    "#         \n",
    "#         print(\"sili_contexts0\")\n",
    "#         print(sili_contexts)\n",
    "       \n",
    "        \n",
    "   \n",
    "        nl_hidden00 = torch.zeros_like(pd_contexts[:, 0 ])\n",
    "        in_hidden00 = torch.zeros_like(pd_contexts[:, 0 ])\n",
    "        out_hidden00 = torch.zeros_like(pd_contexts[:, 0 ])\n",
    "        med_hidden00 = torch.zeros_like(pd_contexts[:, 0 ])\n",
    "        sili_hidden00 = torch.zeros_like(pd_contexts[:, 0 ])\n",
    "        \n",
    "#         for j in range(values_hidden00.shape[0]):\n",
    "#             left_diag_hidden0 = left_diag_contexts[j, 0 ]\n",
    "#             right_diag_hidden0 = right_diag_contexts[j, 0]\n",
    "#             left_f_hidden0 = left_f[j]\n",
    "#             right_f_hidden0 = right_f[j]\n",
    "      \n",
    "        for j in range(values_hidden00.shape[0]):\n",
    "            nl_hidden00[j]=pd_contexts[j,pd_lens[j]-1]\n",
    "            in_hidden00[j]=in_contexts[j,in_lens[j]-1]\n",
    "            out_hidden00[j]=out_contexts[j,out_lens[j]-1]\n",
    "            med_hidden00[j]=med_contexts[j,med_lens[j]-1]\n",
    "            sili_hidden00[j]=sili_contexts[j,sili_lens[j]-1]\n",
    "        \n",
    "\n",
    "        \n",
    "        dp_mask_ = torch.from_numpy(np.array(d_i_o_m_s_masks[0])).to(device).unsqueeze(1)# b 1\n",
    "        in_mask_ = torch.from_numpy(np.array(d_i_o_m_s_masks[1])).to(device).unsqueeze(1)\n",
    "        out_mask_ = torch.from_numpy(np.array(d_i_o_m_s_masks[2])).to(device).unsqueeze(1)\n",
    "        med_mask_ = torch.from_numpy(np.array(d_i_o_m_s_masks[3])).to(device).unsqueeze(1)\n",
    "        sili_mask_ = torch.from_numpy(np.array(d_i_o_m_s_masks[4])).to(device).unsqueeze(1)\n",
    "        \n",
    "        dp_mask2 = dp_mask_ * dp_mask_.permute(1,0)\n",
    "        in_mask2 = in_mask_ * in_mask_.permute(1,0)\n",
    "        out_mask2 = out_mask_ * out_mask_.permute(1,0)\n",
    "        med_mask2 = med_mask_ * med_mask_.permute(1,0)\n",
    "        sili_mask2 = sili_mask_ * sili_mask_.permute(1,0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        nl_hidden_mat = guassian_kernel(self.bn(self.simiProj[0](nl_hidden00)), kernel_mul=2.0, kernel_num=3)\n",
    "        nl_hidden_mat2 = guassian_kernel(self.bn(nl_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        nl_hidden_mat = (1-self.sigmoid(self.eps0))*nl_hidden_mat+self.sigmoid(self.eps0)*nl_hidden_mat2\n",
    "        nl_hidden_mat = nl_hidden_mat*dp_mask2 \n",
    "        \n",
    "        in_hidden_mat = guassian_kernel(self.bn(self.simiProj[1](in_hidden00)), kernel_mul=2.0, kernel_num=3)\n",
    "        in_hidden_mat2 = guassian_kernel(self.bn(in_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        in_hidden_mat = (1-self.sigmoid(self.eps1))*in_hidden_mat+self.sigmoid(self.eps1)*in_hidden_mat2\n",
    "        in_hidden_mat = in_hidden_mat*in_mask2 \n",
    "        \n",
    "        \n",
    "        out_hidden_mat = guassian_kernel(self.bn(self.simiProj[2](out_hidden00)), kernel_mul=2.0, kernel_num=3)\n",
    "        out_hidden_mat2 = guassian_kernel(self.bn(out_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        out_hidden_mat = (1-self.sigmoid(self.eps2))*out_hidden_mat+self.sigmoid(self.eps2)*out_hidden_mat2\n",
    "        out_hidden_mat = out_hidden_mat*out_mask2\n",
    "        \n",
    "        med_hidden_mat = guassian_kernel(self.bn(self.simiProj[3](med_hidden00)), kernel_mul=2.0, kernel_num=3)\n",
    "        med_hidden_mat2 = guassian_kernel(self.bn(med_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        med_hidden_mat = (1-self.sigmoid(self.eps3))*med_hidden_mat+self.sigmoid(self.eps3)*med_hidden_mat2\n",
    "        med_hidden_mat = med_hidden_mat*med_mask2\n",
    "        \n",
    "        sili_hidden_mat = guassian_kernel(self.bn(self.simiProj[4](sili_hidden00)), kernel_mul=2.0, kernel_num=3)\n",
    "        sili_hidden_mat2 = guassian_kernel(self.bn(sili_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        sili_hidden_mat = (1-self.sigmoid(self.eps4))*sili_hidden_mat+self.sigmoid(self.eps4)*sili_hidden_mat2\n",
    "        sili_hidden_mat = sili_hidden_mat*sili_mask2\n",
    "        \n",
    "        \n",
    "        val_mat = guassian_kernel(self.bn(self.simiProj[5](values_hidden00)), kernel_mul=2.0, kernel_num=3)\n",
    "        val_mat2 = guassian_kernel(self.bn(values_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        val_mat = (1-self.sigmoid(self.eps5))*val_mat+self.sigmoid(self.eps5)*val_mat2\n",
    "#         val_mat = val_mat*dp_mask\n",
    "        val_mask2 = torch.ones_like(sili_mask2)\n",
    "    \n",
    "\n",
    "        \n",
    "        diff0 = torch.abs(torch.norm(self.simiProj[0](nl_hidden00)) - torch.norm(nl_hidden00))\n",
    "        diff1 = torch.abs(torch.norm(self.simiProj[1](in_hidden00)) - torch.norm(in_hidden00))\n",
    "        diff2 = torch.abs(torch.norm(self.simiProj[2](out_hidden00)) - torch.norm(out_hidden00))\n",
    "        diff3 = torch.abs(torch.norm(self.simiProj[3](med_hidden00)) - torch.norm(med_hidden00))\n",
    "        diff4 = torch.abs(torch.norm(self.simiProj[4](sili_hidden00)) - torch.norm(sili_hidden00))\n",
    "        diff5 = torch.abs(torch.norm(self.simiProj[5](values_hidden00)) - torch.norm(values_hidden00))\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "        sum_of_diff = diff1+diff2+diff3+diff4+diff5\n",
    "        \n",
    "        similar_score = (val_mat+ nl_hidden_mat + in_hidden_mat + out_hidden_mat + med_hidden_mat + sili_hidden_mat)/ \\\n",
    "        (val_mask2+ dp_mask2 + in_mask2 + out_mask2 + med_mask2 + sili_mask2 )\n",
    "#         similar_score = self.relu(similar_score - self.sigmoid(self.threshold)[0].detach())  \n",
    "#         temp_thresh = self.sigmoid(self.threshold)[0]\n",
    "#         bin_mask = similar_score>0\n",
    "#         similar_score += bin_mask * temp_thresh\n",
    "        similar_score = self.relu(similar_score - self.sigmoid(self.threshold)[0])  \n",
    "        temp_thresh = self.sigmoid(self.threshold)[0]\n",
    "        bin_mask = similar_score>0\n",
    "        similar_score = similar_score + bin_mask * temp_thresh.detach()\n",
    "        \n",
    "#         similar_score = self.relu(similar_score - self.sigmoid(self.threshold))\n",
    "        \n",
    "\n",
    "        nl_hidden0 = self.selu(self.GCN[0](similar_score*dp_mask2, nl_hidden00))\n",
    "        nl_hidden0 = self.selu(self.GCN_2[0](similar_score*dp_mask2, nl_hidden0))\n",
    "#         nl_hidden0 = self.relu(self.GCN_3(similar_score*dp_mask2, nl_hidden0))\n",
    "\n",
    "        in_hidden0 = self.selu(self.GCN[1](similar_score*in_mask2, in_hidden00))\n",
    "        in_hidden0 = self.selu(self.GCN_2[1](similar_score*in_mask2, in_hidden0))  \n",
    "#         in_hidden0 = self.relu(self.GCN_3(similar_score*in_mask2, in_hidden0))\n",
    "        \n",
    "#         right_diag_hidden0 = self.relu(self.GCN_3(similar_score2*right_diag_mask2, right_diag_hidden0)) \n",
    "\n",
    "        out_hidden0 = self.selu(self.GCN[2](similar_score*out_mask2, out_hidden00))\n",
    "        out_hidden0 = self.selu(self.GCN_2[2](similar_score*out_mask2, out_hidden0))\n",
    "#         out_hidden0 = self.relu(self.GCN_3(similar_score*out_mask2, out_hidden0))\n",
    "        \n",
    "        \n",
    "        med_hidden0 = self.selu(self.GCN[3](similar_score*med_mask2, med_hidden00))\n",
    "        med_hidden0 = self.selu(self.GCN_2[3](similar_score*med_mask2, med_hidden0))\n",
    "        \n",
    "        \n",
    "        sili_hidden0 = self.selu(self.GCN[4](similar_score*sili_mask2, sili_hidden00))\n",
    "        sili_hidden0 = self.selu(self.GCN_2[4](similar_score*sili_mask2, sili_hidden0))\n",
    "#         sili_hidden0 = self.relu(self.GCN_3(similar_score*sili_mask2, sili_hidden0))\n",
    "        \n",
    "        \n",
    "        \n",
    "        dp_weight1 = torch.sigmoid(self.weight1[0](nl_hidden0))\n",
    "        dp_weight2 = torch.sigmoid(self.weight2[0](nl_hidden00 ))\n",
    "        dp_weight1 = dp_weight1/(dp_weight1+dp_weight2)\n",
    "        dp_weight2 = 1-dp_weight1\n",
    "        \n",
    "        final_dp = dp_weight1*nl_hidden0+dp_weight2*nl_hidden00\n",
    "        \n",
    "        \n",
    "        in_weight1 = torch.sigmoid(self.weight1[1](in_hidden0))\n",
    "        in_weight2 = torch.sigmoid(self.weight2[1](in_hidden00 ))\n",
    "        in_weight1 = in_weight1/(in_weight1+in_weight2)\n",
    "        in_weight2 = 1-in_weight1\n",
    "        \n",
    "        final_in = in_weight1*in_hidden0+in_weight2*in_hidden00\n",
    "        \n",
    "        \n",
    "        out_weight1 = torch.sigmoid(self.weight1[2](out_hidden0))\n",
    "        out_weight2 = torch.sigmoid(self.weight2[2](out_hidden00 ))\n",
    "        out_weight1 = out_weight1/(out_weight1+out_weight2)\n",
    "        out_weight2 = 1-out_weight1\n",
    "        \n",
    "        final_out = out_weight1*out_hidden0+out_weight2*out_hidden00\n",
    "        \n",
    "        \n",
    "        med_weight1 = torch.sigmoid(self.weight1[3](med_hidden0))\n",
    "        med_weight2 = torch.sigmoid(self.weight2[3](med_hidden00 ))\n",
    "        med_weight1 = med_weight1/(med_weight1+med_weight2)\n",
    "        med_weight2 = 1-med_weight1\n",
    "        \n",
    "        final_med = med_weight1*med_hidden0+med_weight2*med_hidden00\n",
    "        \n",
    "        \n",
    "        sili_weight1 = torch.sigmoid(self.weight1[4](sili_hidden0))\n",
    "        sili_weight2 = torch.sigmoid(self.weight2[4](sili_hidden00 ))\n",
    "        sili_weight1 = sili_weight1/(sili_weight1+sili_weight2)\n",
    "        sili_weight2 = 1-sili_weight1\n",
    "        \n",
    "        final_sili = sili_weight1*sili_hidden0+sili_weight2*sili_hidden00\n",
    "            \n",
    "        \n",
    "            \n",
    "        for i in range(dp_mask_.shape[0]):\n",
    "            if dp_mask_[i][0] != 1:\n",
    "                pd_contexts[i,pd_lens[j]-1] = nl_hidden0[i]\n",
    "            else:\n",
    "                pd_contexts[i,pd_lens[j]-1] = final_dp[i]\n",
    "                \n",
    "        for i in range(dp_mask_.shape[0]):\n",
    "            if in_mask_[i][0] != 1:\n",
    "                in_contexts[i,in_lens[j]-1] = in_hidden0[i]\n",
    "            else:\n",
    "                in_contexts[i,in_lens[j]-1] = final_in[i] \n",
    "                \n",
    "        for i in range(dp_mask_.shape[0]):\n",
    "            if out_mask_[i][0] != 1:\n",
    "                out_contexts[i,out_lens[j]-1] = out_hidden0[i]\n",
    "            else:\n",
    "                out_contexts[i,out_lens[j]-1] = final_out[i]\n",
    "                \n",
    "        for i in range(dp_mask_.shape[0]):\n",
    "            if med_mask_[i][0] != 1:\n",
    "                med_contexts[i,med_lens[i]-1] = med_hidden0[i]\n",
    "            else:\n",
    "                med_contexts[i,med_lens[i]-1] = final_med[i]\n",
    "                \n",
    "        for i in range(dp_mask_.shape[0]):\n",
    "            if sili_mask_[i][0] != 1:\n",
    "                sili_contexts[i,sili_lens[i]-1] = sili_hidden0[i]\n",
    "            else:\n",
    "                sili_contexts[i,sili_lens[i]-1] = final_sili[i]\n",
    "                \n",
    "        \n",
    "        \n",
    "        pd_contexts = self.PositionalEncoding(pd_contexts)  +  \\\n",
    "            self.token_type_embeddings(torch.zeros_like(dp_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "#         pd_contexts = torch.cat([self.sep_token_embeddings(torch.zeros_like(med_lens).to(device).long()).unsqueeze(1), pd_contexts], dim=1)\n",
    "#         dp_mask = torch.cat([torch.ones_like(med_lens).to(device).unsqueeze(-1).unsqueeze(-1), dp_mask.int()], dim=-1)\n",
    "\n",
    "        \n",
    "        med_contexts = self.PositionalEncoding(med_contexts)+  \\\n",
    "            self.token_type_embeddings(torch.ones_like(med_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "#         med_contexts = torch.cat([self.sep_token_embeddings(torch.ones_like(med_lens).to(device).long()).unsqueeze(1), med_contexts], dim=1)\n",
    "#         med_mask = torch.cat([torch.ones_like(med_lens).to(device).unsqueeze(-1).unsqueeze(-1), med_mask.int()], dim=-1)\n",
    "\n",
    "\n",
    "        in_mask = length_to_mask(torch.from_numpy(np.array(in_lens))).unsqueeze(1).to(device)\n",
    "        in_contexts = self.PositionalEncoding(in_contexts) +  \\\n",
    "            self.token_type_embeddings( 2*torch.ones_like(in_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "#         in_contexts = torch.cat([self.sep_token_embeddings(2*torch.ones_like(med_lens).to(device).long()).unsqueeze(1), in_contexts], dim=1)\n",
    "#         in_mask = torch.cat([torch.ones_like(med_lens).to(device).unsqueeze(-1).unsqueeze(-1), in_mask.int()], dim=-1)\n",
    "\n",
    "        \n",
    "        out_mask = length_to_mask(torch.from_numpy(np.array(out_lens))).unsqueeze(1).to(device)\n",
    "        out_contexts = self.PositionalEncoding(out_contexts) +  \\\n",
    "            self.token_type_embeddings(3*torch.ones_like(out_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "#         out_contexts = torch.cat([self.sep_token_embeddings(3*torch.ones_like(med_lens).to(device).long()).unsqueeze(1), out_contexts], dim=1)\n",
    "#         out_mask = torch.cat([torch.ones_like(med_lens).to(device).unsqueeze(-1).unsqueeze(-1), out_mask.int()], dim=-1)\n",
    "\n",
    "        \n",
    "        sili_contexts = self.PositionalEncoding(sili_contexts) + \\\n",
    "            self.token_type_embeddings(4*torch.ones_like(sili_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "        z0 = torch.cat([pd_contexts, in_contexts, out_contexts, med_contexts, sili_contexts], dim=1)\n",
    "        z0_mask = torch.cat([dp_mask, in_mask, out_mask, med_mask, sili_mask], dim=-1).int()\n",
    "        \n",
    "        z1 = self.selu(self.MM_model(z0, z0_mask))# b 1\n",
    "        val_mask = length_to_mask(torch.ones((pd_contexts.shape[0],1)).int().squeeze()).unsqueeze(1).to(device).int()\n",
    "#         print(z0_mask.shape)\n",
    "#         print(val_mask.shape)\n",
    "\n",
    "        z1 = torch.cat([values_hidden00.unsqueeze(1), z1], dim=1)\n",
    "        z1_mask = torch.cat([val_mask, z0_mask], dim=-1)\n",
    "\n",
    "        z2 = self.selu(self.MM_model2(z1, z1_mask))# b 1\n",
    "    \n",
    "\n",
    "        nl_hidden = []\n",
    "        in_hidden = []\n",
    "        out_hidden = []\n",
    "        med_hidden = []\n",
    "        sili_hidden = []\n",
    "        for j in range(values_hidden00.shape[0]):\n",
    "            nl_hidden.append(z2[j,1])\n",
    "            in_hidden.append(z2[j,1 + pd_contexts.shape[1]])\n",
    "            out_hidden.append(z2[j,1+pd_contexts.shape[1] + in_contexts.shape[1]])\n",
    "            med_hidden.append(z2[j,1+pd_contexts.shape[1]  + in_contexts.shape[1] +\\\n",
    "                                 out_contexts.shape[1]  ])\n",
    "            sili_hidden.append(z2[j,1+pd_contexts.shape[1]  +in_contexts.shape[1]+ \\\n",
    "                                 out_contexts.shape[1] +med_contexts.shape[1]])\n",
    "            \n",
    "        nl_hidden = torch.stack(nl_hidden)\n",
    "        in_hidden = torch.stack(in_hidden)\n",
    "        out_hidden = torch.stack(out_hidden)\n",
    "        med_hidden = torch.stack(med_hidden)\n",
    "        sili_hidden = torch.stack(sili_hidden)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        combined_hidden = torch.cat((values_hidden00, nl_hidden,in_hidden,out_hidden, \\\n",
    "                                     med_hidden, sili_hidden,z2[:,0,:]),-1)#b n h\n",
    "\n",
    "        \n",
    "        \n",
    "        last_hs_proj = self.dropout(self.selu(self.proj1(combined_hidden)))\n",
    "\n",
    "        \n",
    "        output = self.sigmoid(self.out_layer(last_hs_proj))\n",
    "\n",
    "        return output,sum_of_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true, weight=None):\n",
    "    loss = torch.nn.BCELoss(weight=weight)\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ACC: 0.6071 ------------\n",
      "------------ Save best model - ROC: 0.6070 ------------\n",
      "Fold 1, Epoch 0, acc = 0.6071, roc = 0.6070, prc = 0.4517\n",
      "Fold 1, Epoch 1, acc = 0.4286, roc = 0.5120, prc = 0.4404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2, acc = 0.5714, roc = 0.4099, prc = 0.3416\n",
      "Fold 1, Epoch 3, acc = 0.5238, roc = 0.3131, prc = 0.3118\n",
      "Fold 1, Epoch 4, acc = 0.5714, roc = 0.3394, prc = 0.3431\n",
      "------------ Save best model - ACC: 0.6190 ------------\n",
      "Fold 1, Epoch 5, acc = 0.6190, roc = 0.4717, prc = 0.5166\n",
      "Fold 1, Epoch 6, acc = 0.5714, roc = 0.3061, prc = 0.3985\n",
      "Fold 1, Epoch 7, acc = 0.5000, roc = 0.3085, prc = 0.3945\n",
      "Fold 1, Epoch 8, acc = 0.6190, roc = 0.4222, prc = 0.4339\n",
      "------------ Save best model - ACC: 0.7143 ------------\n",
      "------------ Save best model - ROC: 0.8566 ------------\n",
      "Fold 1, Epoch 9, acc = 0.7143, roc = 0.8566, prc = 0.8411\n",
      "------------ Save best model - ACC: 0.7262 ------------\n",
      "Fold 1, Epoch 10, acc = 0.7262, roc = 0.7901, prc = 0.7609\n",
      "Fold 1, Epoch 11, acc = 0.5714, roc = 0.5813, prc = 0.4453\n",
      "------------ Save best model - ACC: 0.7619 ------------\n",
      "Fold 1, Epoch 23, acc = 0.7619, roc = 0.8426, prc = 0.8104\n",
      "------------ Save best model - ACC: 0.7857 ------------\n",
      "------------ Save best model - ROC: 0.8793 ------------\n",
      "Fold 1, Epoch 24, acc = 0.7857, roc = 0.8793, prc = 0.8512\n",
      "Fold 1, Epoch 25, acc = 0.6429, roc = 0.8350, prc = 0.8011\n",
      "Fold 1, Epoch 26, acc = 0.5952, roc = 0.3901, prc = 0.3994\n",
      "Fold 1, Epoch 27, acc = 0.5000, roc = 0.3271, prc = 0.3782\n",
      "Fold 1, Epoch 28, acc = 0.5476, roc = 0.3860, prc = 0.3984\n",
      "Fold 1, Epoch 29, acc = 0.5238, roc = 0.3318, prc = 0.3735\n",
      "Fold 1, Epoch 30, acc = 0.4048, roc = 0.2233, prc = 0.3270\n",
      "Fold 1, Epoch 31, acc = 0.5119, roc = 0.2484, prc = 0.3401\n",
      "Fold 1, Epoch 32, acc = 0.5119, roc = 0.5609, prc = 0.4864\n",
      "Fold 1, Epoch 33, acc = 0.5952, roc = 0.5213, prc = 0.4224\n",
      "Fold 1, Epoch 34, acc = 0.6071, roc = 0.6962, prc = 0.5550\n",
      "Fold 1, Epoch 35, acc = 0.5357, roc = 0.6391, prc = 0.4794\n",
      "Fold 1, Epoch 36, acc = 0.4762, roc = 0.5557, prc = 0.4288\n",
      "Fold 1, Epoch 37, acc = 0.5833, roc = 0.4939, prc = 0.4306\n",
      "Fold 1, Epoch 38, acc = 0.5714, roc = 0.3778, prc = 0.3607\n",
      "Fold 1, Epoch 39, acc = 0.5833, roc = 0.4262, prc = 0.3681\n",
      "Fold 1, Epoch 40, acc = 0.5357, roc = 0.5300, prc = 0.4732\n",
      "Fold 1, Epoch 41, acc = 0.5714, roc = 0.6087, prc = 0.5009\n",
      "Fold 1, Epoch 42, acc = 0.5476, roc = 0.5825, prc = 0.4577\n",
      "Fold 1, Epoch 43, acc = 0.4643, roc = 0.5364, prc = 0.4362\n",
      "Fold 1, Epoch 44, acc = 0.5833, roc = 0.5224, prc = 0.4213\n",
      "Fold 1, Epoch 45, acc = 0.5714, roc = 0.5388, prc = 0.4596\n",
      "Fold 1, Epoch 46, acc = 0.5238, roc = 0.5312, prc = 0.4370\n",
      "Fold 1, Epoch 47, acc = 0.5595, roc = 0.4840, prc = 0.4008\n",
      "Fold 1, Epoch 48, acc = 0.4881, roc = 0.5318, prc = 0.4276\n",
      "Fold 1, Epoch 49, acc = 0.5357, roc = 0.4443, prc = 0.3688\n",
      "Fold 1, Epoch 50, acc = 0.5476, roc = 0.4455, prc = 0.3787\n",
      "Fold 1, Epoch 51, acc = 0.5357, roc = 0.3580, prc = 0.3298\n",
      "Fold 1, Epoch 52, acc = 0.5714, roc = 0.5149, prc = 0.4150\n",
      "Fold 1, Epoch 53, acc = 0.6310, roc = 0.5878, prc = 0.4616\n",
      "Fold 1, Epoch 54, acc = 0.5952, roc = 0.5609, prc = 0.4625\n",
      "Fold 1, Epoch 55, acc = 0.5833, roc = 0.5592, prc = 0.4654\n",
      "Fold 1, Epoch 56, acc = 0.5476, roc = 0.5399, prc = 0.4577\n",
      "Fold 1, Epoch 57, acc = 0.5238, roc = 0.5510, prc = 0.4886\n",
      "Fold 1, Epoch 58, acc = 0.6071, roc = 0.5837, prc = 0.4705\n",
      "Fold 1, Epoch 59, acc = 0.5357, roc = 0.4886, prc = 0.4155\n",
      "Fold 1, Epoch 60, acc = 0.5238, roc = 0.5306, prc = 0.4332\n",
      "Fold 1, Epoch 61, acc = 0.5833, roc = 0.5055, prc = 0.4237\n",
      "Fold 1, Epoch 62, acc = 0.5238, roc = 0.5259, prc = 0.4439\n",
      "Fold 1, Epoch 63, acc = 0.4524, roc = 0.5230, prc = 0.4475\n",
      "Fold 1, Epoch 64, acc = 0.5119, roc = 0.5901, prc = 0.4774\n",
      "Fold 1, Epoch 65, acc = 0.5714, roc = 0.5510, prc = 0.4548\n",
      "Fold 1, Epoch 66, acc = 0.5476, roc = 0.5318, prc = 0.4305\n",
      "Fold 1, Epoch 67, acc = 0.5952, roc = 0.5434, prc = 0.4473\n",
      "Fold 1, Epoch 68, acc = 0.5476, roc = 0.5417, prc = 0.4326\n",
      "Fold 1, Epoch 69, acc = 0.5833, roc = 0.5359, prc = 0.4338\n",
      "Fold 1, Epoch 70, acc = 0.5833, roc = 0.5493, prc = 0.4371\n",
      "Fold 1, Epoch 71, acc = 0.6071, roc = 0.5312, prc = 0.4262\n",
      "Fold 1, Epoch 72, acc = 0.6190, roc = 0.5545, prc = 0.4623\n",
      "Fold 1, Epoch 73, acc = 0.5238, roc = 0.5656, prc = 0.4593\n",
      "Fold 1, Epoch 74, acc = 0.6190, roc = 0.5563, prc = 0.4557\n",
      "Fold 1, Epoch 75, acc = 0.5714, roc = 0.5376, prc = 0.4303\n",
      "Fold 1, Epoch 76, acc = 0.5000, roc = 0.5131, prc = 0.4185\n",
      "Fold 1, Epoch 77, acc = 0.4762, roc = 0.5184, prc = 0.4251\n",
      "Fold 1, Epoch 78, acc = 0.4524, roc = 0.4437, prc = 0.3686\n",
      "Fold 1, Epoch 79, acc = 0.4643, roc = 0.5085, prc = 0.4130\n",
      "Fold 1, Epoch 80, acc = 0.5714, roc = 0.5895, prc = 0.4632\n",
      "Fold 1, Epoch 81, acc = 0.5595, roc = 0.5312, prc = 0.4293\n",
      "Fold 1, Epoch 82, acc = 0.5357, roc = 0.5743, prc = 0.4613\n",
      "Fold 1, Epoch 83, acc = 0.5000, roc = 0.5230, prc = 0.4234\n",
      "Fold 1, Epoch 84, acc = 0.5119, roc = 0.5440, prc = 0.4270\n",
      "Fold 1, Epoch 85, acc = 0.5357, roc = 0.5411, prc = 0.4357\n",
      "Fold 1, Epoch 86, acc = 0.5952, roc = 0.6292, prc = 0.5021\n",
      "Fold 1, Epoch 87, acc = 0.5476, roc = 0.5691, prc = 0.4475\n",
      "Fold 1, Epoch 88, acc = 0.5595, roc = 0.6000, prc = 0.4652\n",
      "Fold 1, Epoch 89, acc = 0.5833, roc = 0.5959, prc = 0.4623\n",
      "Fold 1, Epoch 90, acc = 0.5595, roc = 0.5825, prc = 0.4536\n",
      "Fold 1, Epoch 91, acc = 0.5595, roc = 0.5586, prc = 0.4497\n",
      "Fold 1, Epoch 92, acc = 0.4762, roc = 0.5341, prc = 0.4156\n",
      "Fold 1, Epoch 93, acc = 0.4881, roc = 0.5376, prc = 0.4253\n",
      "Fold 1, Epoch 94, acc = 0.5238, roc = 0.5504, prc = 0.4424\n",
      "Fold 1, Epoch 95, acc = 0.4881, roc = 0.5108, prc = 0.3973\n",
      "Fold 1, Epoch 96, acc = 0.5000, roc = 0.5478, prc = 0.4128\n",
      "Fold 1, Epoch 97, acc = 0.5595, roc = 0.5819, prc = 0.4420\n",
      "Fold 1, Epoch 98, acc = 0.5714, roc = 0.5662, prc = 0.4378\n",
      "Fold 1, Epoch 99, acc = 0.5000, roc = 0.5569, prc = 0.4553\n",
      "------------ Save best model - ROC: 0.5673 ------------\n",
      "Fold 2, Epoch 0, acc = 0.5000, roc = 0.5673, prc = 0.4459\n",
      "------------ Save best model - ACC: 0.5952 ------------\n",
      "------------ Save best model - ROC: 0.6286 ------------\n",
      "Fold 2, Epoch 1, acc = 0.5952, roc = 0.6286, prc = 0.6231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ROC: 0.6880 ------------\n",
      "Fold 2, Epoch 2, acc = 0.5833, roc = 0.6880, prc = 0.5909\n",
      "------------ Save best model - ACC: 0.6786 ------------\n",
      "------------ Save best model - ROC: 0.8711 ------------\n",
      "Fold 2, Epoch 3, acc = 0.6786, roc = 0.8711, prc = 0.7967\n",
      "Fold 2, Epoch 4, acc = 0.6190, roc = 0.5708, prc = 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5, acc = 0.5833, roc = 0.8035, prc = 0.7237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6, acc = 0.5833, roc = 0.5901, prc = 0.5179\n",
      "Fold 2, Epoch 7, acc = 0.6071, roc = 0.7603, prc = 0.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ROC: 0.9038 ------------\n",
      "Fold 2, Epoch 8, acc = 0.5833, roc = 0.9038, prc = 0.8535\n",
      "Fold 2, Epoch 9, acc = 0.6071, roc = 0.7866, prc = 0.6963\n",
      "Fold 2, Epoch 10, acc = 0.6071, roc = 0.6280, prc = 0.5349\n",
      "------------ Save best model - ACC: 0.7500 ------------\n",
      "Fold 2, Epoch 11, acc = 0.7500, roc = 0.8082, prc = 0.7866\n",
      "Fold 2, Epoch 12, acc = 0.6310, roc = 0.7726, prc = 0.6893\n",
      "Fold 2, Epoch 13, acc = 0.6190, roc = 0.6752, prc = 0.5700\n",
      "Fold 2, Epoch 14, acc = 0.7500, roc = 0.8140, prc = 0.7683\n",
      "Fold 2, Epoch 15, acc = 0.4524, roc = 0.5359, prc = 0.5183\n",
      "Fold 2, Epoch 16, acc = 0.3452, roc = 0.4577, prc = 0.4825\n",
      "Fold 2, Epoch 17, acc = 0.5357, roc = 0.5848, prc = 0.4637\n",
      "Fold 2, Epoch 18, acc = 0.3452, roc = 0.4741, prc = 0.5417\n",
      "Fold 2, Epoch 19, acc = 0.3452, roc = 0.4035, prc = 0.3787\n",
      "Fold 2, Epoch 20, acc = 0.4167, roc = 0.5528, prc = 0.6148\n",
      "Fold 2, Epoch 21, acc = 0.4881, roc = 0.6140, prc = 0.6456\n",
      "Fold 2, Epoch 22, acc = 0.4762, roc = 0.5883, prc = 0.6318\n",
      "Fold 2, Epoch 23, acc = 0.5119, roc = 0.6227, prc = 0.5834\n",
      "Fold 2, Epoch 24, acc = 0.6071, roc = 0.7003, prc = 0.6385\n",
      "------------ Save best model - ACC: 0.7976 ------------\n",
      "Fold 2, Epoch 25, acc = 0.7976, roc = 0.8478, prc = 0.7926\n",
      "Fold 2, Epoch 26, acc = 0.6548, roc = 0.7843, prc = 0.6987\n",
      "Fold 2, Epoch 27, acc = 0.6548, roc = 0.7347, prc = 0.6871\n",
      "Fold 2, Epoch 28, acc = 0.5238, roc = 0.6111, prc = 0.4662\n",
      "Fold 2, Epoch 29, acc = 0.4524, roc = 0.5708, prc = 0.5349\n",
      "Fold 2, Epoch 30, acc = 0.4881, roc = 0.5265, prc = 0.4721\n",
      "Fold 2, Epoch 31, acc = 0.5952, roc = 0.6257, prc = 0.5546\n",
      "Fold 2, Epoch 32, acc = 0.5595, roc = 0.5913, prc = 0.5004\n",
      "Fold 2, Epoch 33, acc = 0.6071, roc = 0.6665, prc = 0.5990\n",
      "Fold 2, Epoch 34, acc = 0.6548, roc = 0.6717, prc = 0.6158\n",
      "Fold 2, Epoch 35, acc = 0.6071, roc = 0.5889, prc = 0.5014\n",
      "Fold 2, Epoch 36, acc = 0.5952, roc = 0.5738, prc = 0.4967\n",
      "Fold 2, Epoch 37, acc = 0.6548, roc = 0.6764, prc = 0.6197\n",
      "Fold 2, Epoch 38, acc = 0.5833, roc = 0.6822, prc = 0.6520\n",
      "Fold 2, Epoch 39, acc = 0.6905, roc = 0.6898, prc = 0.6384\n",
      "Fold 2, Epoch 40, acc = 0.5952, roc = 0.6711, prc = 0.6172\n",
      "Fold 2, Epoch 41, acc = 0.5952, roc = 0.6461, prc = 0.5562\n",
      "Fold 2, Epoch 42, acc = 0.6310, roc = 0.6542, prc = 0.5813\n",
      "Fold 2, Epoch 43, acc = 0.6071, roc = 0.6297, prc = 0.5387\n",
      "Fold 2, Epoch 44, acc = 0.5833, roc = 0.6706, prc = 0.6146\n",
      "Fold 2, Epoch 45, acc = 0.6429, roc = 0.6490, prc = 0.5610\n",
      "Fold 2, Epoch 46, acc = 0.6071, roc = 0.6362, prc = 0.5950\n",
      "Fold 2, Epoch 47, acc = 0.5476, roc = 0.6431, prc = 0.6028\n",
      "Fold 2, Epoch 48, acc = 0.6429, roc = 0.6402, prc = 0.5766\n",
      "Fold 2, Epoch 49, acc = 0.5833, roc = 0.6706, prc = 0.6323\n",
      "Fold 2, Epoch 50, acc = 0.6190, roc = 0.6729, prc = 0.6266\n",
      "Fold 2, Epoch 51, acc = 0.6071, roc = 0.6770, prc = 0.6391\n",
      "Fold 2, Epoch 52, acc = 0.5952, roc = 0.7166, prc = 0.6845\n",
      "Fold 2, Epoch 53, acc = 0.5476, roc = 0.6245, prc = 0.5803\n",
      "Fold 2, Epoch 54, acc = 0.6190, roc = 0.5574, prc = 0.5014\n",
      "Fold 2, Epoch 55, acc = 0.6071, roc = 0.6845, prc = 0.6240\n",
      "Fold 2, Epoch 56, acc = 0.5238, roc = 0.6799, prc = 0.6434\n",
      "Fold 2, Epoch 57, acc = 0.6667, roc = 0.6647, prc = 0.6117\n",
      "Fold 2, Epoch 58, acc = 0.4881, roc = 0.6140, prc = 0.5682\n",
      "Fold 2, Epoch 59, acc = 0.5238, roc = 0.5965, prc = 0.5437\n",
      "Fold 2, Epoch 60, acc = 0.5833, roc = 0.6391, prc = 0.5818\n",
      "Fold 2, Epoch 61, acc = 0.4881, roc = 0.6175, prc = 0.5687\n",
      "Fold 2, Epoch 62, acc = 0.6071, roc = 0.6729, prc = 0.5913\n",
      "Fold 2, Epoch 63, acc = 0.5952, roc = 0.6286, prc = 0.5528\n",
      "Fold 2, Epoch 64, acc = 0.5952, roc = 0.6064, prc = 0.5252\n",
      "Fold 2, Epoch 65, acc = 0.5833, roc = 0.6087, prc = 0.5328\n",
      "Fold 2, Epoch 66, acc = 0.5833, roc = 0.5819, prc = 0.5264\n",
      "Fold 2, Epoch 67, acc = 0.5833, roc = 0.6414, prc = 0.5751\n",
      "Fold 2, Epoch 68, acc = 0.6429, roc = 0.6531, prc = 0.6035\n",
      "Fold 2, Epoch 69, acc = 0.6071, roc = 0.5773, prc = 0.5144\n",
      "Fold 2, Epoch 70, acc = 0.5357, roc = 0.5079, prc = 0.4376\n",
      "Fold 2, Epoch 71, acc = 0.5714, roc = 0.4700, prc = 0.4127\n",
      "Fold 2, Epoch 72, acc = 0.6071, roc = 0.6233, prc = 0.5474\n",
      "Fold 2, Epoch 73, acc = 0.6071, roc = 0.6466, prc = 0.5803\n",
      "Fold 2, Epoch 74, acc = 0.5595, roc = 0.6472, prc = 0.5858\n",
      "Fold 2, Epoch 75, acc = 0.4881, roc = 0.5405, prc = 0.4284\n",
      "Fold 2, Epoch 76, acc = 0.4524, roc = 0.4968, prc = 0.4249\n",
      "Fold 2, Epoch 77, acc = 0.5238, roc = 0.4682, prc = 0.4100\n",
      "Fold 2, Epoch 78, acc = 0.5000, roc = 0.4099, prc = 0.3640\n",
      "Fold 2, Epoch 79, acc = 0.4762, roc = 0.5155, prc = 0.4762\n",
      "Fold 2, Epoch 80, acc = 0.5595, roc = 0.5580, prc = 0.5093\n",
      "Fold 2, Epoch 81, acc = 0.5119, roc = 0.5440, prc = 0.4895\n",
      "Fold 2, Epoch 82, acc = 0.4762, roc = 0.5493, prc = 0.5051\n",
      "Fold 2, Epoch 83, acc = 0.4405, roc = 0.5341, prc = 0.4761\n",
      "Fold 2, Epoch 84, acc = 0.5357, roc = 0.5889, prc = 0.5553\n",
      "Fold 2, Epoch 85, acc = 0.5357, roc = 0.5994, prc = 0.5557\n",
      "Fold 2, Epoch 86, acc = 0.4881, roc = 0.5364, prc = 0.4242\n",
      "Fold 2, Epoch 87, acc = 0.5238, roc = 0.5644, prc = 0.4571\n",
      "Fold 2, Epoch 88, acc = 0.5119, roc = 0.5965, prc = 0.5426\n",
      "Fold 2, Epoch 89, acc = 0.5714, roc = 0.6519, prc = 0.5910\n",
      "Fold 2, Epoch 90, acc = 0.5714, roc = 0.6344, prc = 0.5756\n",
      "Fold 2, Epoch 91, acc = 0.5119, roc = 0.6723, prc = 0.6447\n",
      "Fold 2, Epoch 92, acc = 0.5476, roc = 0.6350, prc = 0.5790\n",
      "Fold 2, Epoch 93, acc = 0.4881, roc = 0.5079, prc = 0.4241\n",
      "Fold 2, Epoch 94, acc = 0.5952, roc = 0.6303, prc = 0.5544\n",
      "Fold 2, Epoch 95, acc = 0.5238, roc = 0.6443, prc = 0.5850\n",
      "Fold 2, Epoch 96, acc = 0.5952, roc = 0.6758, prc = 0.5977\n",
      "Fold 2, Epoch 97, acc = 0.5952, roc = 0.6595, prc = 0.5848\n",
      "Fold 2, Epoch 98, acc = 0.5119, roc = 0.5539, prc = 0.4597\n",
      "Fold 2, Epoch 99, acc = 0.6190, roc = 0.6443, prc = 0.5886\n",
      "------------ Save best model - ROC: 0.7804 ------------\n",
      "Fold 3, Epoch 0, acc = 0.6145, roc = 0.7804, prc = 0.6997\n",
      "Fold 3, Epoch 1, acc = 0.6024, roc = 0.6179, prc = 0.4624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2, acc = 0.5783, roc = 0.5940, prc = 0.5077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3, acc = 0.5783, roc = 0.7512, prc = 0.6032\n",
      "Fold 3, Epoch 4, acc = 0.5783, roc = 0.6821, prc = 0.5432\n",
      "Fold 3, Epoch 5, acc = 0.5783, roc = 0.4661, prc = 0.4544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6, acc = 0.5783, roc = 0.5262, prc = 0.5068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 7, acc = 0.5783, roc = 0.4631, prc = 0.4162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 8, acc = 0.5783, roc = 0.4661, prc = 0.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 9, acc = 0.5663, roc = 0.4512, prc = 0.4118\n",
      "Fold 3, Epoch 10, acc = 0.4819, roc = 0.6655, prc = 0.5414\n",
      "Fold 3, Epoch 11, acc = 0.5542, roc = 0.5571, prc = 0.4283\n",
      "Fold 3, Epoch 12, acc = 0.5663, roc = 0.5952, prc = 0.4488\n",
      "Fold 3, Epoch 13, acc = 0.5422, roc = 0.5315, prc = 0.4098\n",
      "Fold 3, Epoch 14, acc = 0.5181, roc = 0.5833, prc = 0.4368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 15, acc = 0.5783, roc = 0.2970, prc = 0.3119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 16, acc = 0.5783, roc = 0.4054, prc = 0.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 17, acc = 0.5783, roc = 0.3613, prc = 0.3538\n",
      "Fold 3, Epoch 18, acc = 0.5783, roc = 0.4446, prc = 0.4229\n",
      "Fold 3, Epoch 19, acc = 0.5301, roc = 0.3345, prc = 0.3245\n",
      "Fold 3, Epoch 20, acc = 0.5301, roc = 0.3583, prc = 0.3269\n",
      "Fold 3, Epoch 21, acc = 0.4458, roc = 0.7560, prc = 0.7418\n",
      "Fold 3, Epoch 22, acc = 0.5422, roc = 0.5244, prc = 0.4800\n",
      "Fold 3, Epoch 23, acc = 0.5783, roc = 0.4738, prc = 0.4444\n",
      "Fold 3, Epoch 24, acc = 0.6024, roc = 0.6060, prc = 0.5418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 25, acc = 0.5783, roc = 0.3643, prc = 0.3879\n",
      "Fold 3, Epoch 26, acc = 0.5904, roc = 0.4613, prc = 0.4130\n",
      "Fold 3, Epoch 27, acc = 0.5422, roc = 0.3327, prc = 0.3467\n",
      "Fold 3, Epoch 28, acc = 0.5060, roc = 0.3589, prc = 0.3808\n",
      "Fold 3, Epoch 29, acc = 0.4699, roc = 0.7018, prc = 0.6879\n",
      "------------ Save best model - ROC: 0.8250 ------------\n",
      "Fold 3, Epoch 30, acc = 0.4458, roc = 0.8250, prc = 0.8117\n",
      "Fold 3, Epoch 31, acc = 0.4458, roc = 0.8030, prc = 0.7737\n",
      "Fold 3, Epoch 32, acc = 0.5783, roc = 0.6464, prc = 0.5454\n",
      "Fold 3, Epoch 33, acc = 0.6024, roc = 0.6714, prc = 0.5185\n",
      "Fold 3, Epoch 34, acc = 0.6024, roc = 0.6131, prc = 0.5011\n",
      "Fold 3, Epoch 35, acc = 0.4217, roc = 0.4292, prc = 0.3593\n",
      "Fold 3, Epoch 36, acc = 0.4940, roc = 0.5077, prc = 0.4006\n",
      "Fold 3, Epoch 37, acc = 0.4819, roc = 0.6083, prc = 0.4907\n",
      "Fold 3, Epoch 38, acc = 0.5542, roc = 0.4726, prc = 0.4228\n",
      "Fold 3, Epoch 39, acc = 0.5542, roc = 0.6167, prc = 0.4638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 40, acc = 0.5181, roc = 0.3762, prc = 0.3324\n",
      "Fold 3, Epoch 41, acc = 0.4940, roc = 0.4810, prc = 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 42, acc = 0.4819, roc = 0.3702, prc = 0.3293\n",
      "Fold 3, Epoch 43, acc = 0.5301, roc = 0.3238, prc = 0.3127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 44, acc = 0.5181, roc = 0.2988, prc = 0.3075\n",
      "Fold 3, Epoch 45, acc = 0.4458, roc = 0.3560, prc = 0.3246\n",
      "Fold 3, Epoch 46, acc = 0.5181, roc = 0.4857, prc = 0.3810\n",
      "Fold 3, Epoch 47, acc = 0.4940, roc = 0.5720, prc = 0.5353\n",
      "Fold 3, Epoch 48, acc = 0.5060, roc = 0.6827, prc = 0.5966\n",
      "Fold 3, Epoch 49, acc = 0.5060, roc = 0.6554, prc = 0.4944\n",
      "Fold 3, Epoch 50, acc = 0.4819, roc = 0.6208, prc = 0.6018\n",
      "Fold 3, Epoch 51, acc = 0.5422, roc = 0.5881, prc = 0.4517\n",
      "Fold 3, Epoch 52, acc = 0.4578, roc = 0.4935, prc = 0.3870\n",
      "Fold 3, Epoch 53, acc = 0.5542, roc = 0.5685, prc = 0.4779\n",
      "Fold 3, Epoch 54, acc = 0.5301, roc = 0.7048, prc = 0.6643\n",
      "Fold 3, Epoch 55, acc = 0.4819, roc = 0.2202, prc = 0.2860\n",
      "Fold 3, Epoch 56, acc = 0.5181, roc = 0.2250, prc = 0.2932\n",
      "Fold 3, Epoch 57, acc = 0.3373, roc = 0.3310, prc = 0.3142\n",
      "Fold 3, Epoch 58, acc = 0.6024, roc = 0.5958, prc = 0.5237\n",
      "Fold 3, Epoch 59, acc = 0.5663, roc = 0.4958, prc = 0.4189\n",
      "Fold 3, Epoch 60, acc = 0.5783, roc = 0.5304, prc = 0.4206\n",
      "Fold 3, Epoch 61, acc = 0.5422, roc = 0.5411, prc = 0.4445\n",
      "Fold 3, Epoch 62, acc = 0.3494, roc = 0.3774, prc = 0.3691\n",
      "Fold 3, Epoch 63, acc = 0.4699, roc = 0.6726, prc = 0.5419\n",
      "Fold 3, Epoch 64, acc = 0.4337, roc = 0.6792, prc = 0.5244\n",
      "Fold 3, Epoch 65, acc = 0.4337, roc = 0.7574, prc = 0.7231\n",
      "Fold 3, Epoch 66, acc = 0.4699, roc = 0.7571, prc = 0.6609\n",
      "Fold 3, Epoch 79, acc = 0.3855, roc = 0.4363, prc = 0.4074\n",
      "Fold 3, Epoch 80, acc = 0.4699, roc = 0.5315, prc = 0.4729\n",
      "Fold 3, Epoch 81, acc = 0.5663, roc = 0.5952, prc = 0.5162\n",
      "Fold 3, Epoch 82, acc = 0.5542, roc = 0.6286, prc = 0.5817\n",
      "Fold 3, Epoch 83, acc = 0.5422, roc = 0.6756, prc = 0.5984\n",
      "Fold 3, Epoch 84, acc = 0.4458, roc = 0.4804, prc = 0.4987\n",
      "Fold 3, Epoch 85, acc = 0.5663, roc = 0.5893, prc = 0.5105\n",
      "Fold 3, Epoch 86, acc = 0.5542, roc = 0.5685, prc = 0.4868\n",
      "Fold 3, Epoch 87, acc = 0.5422, roc = 0.5774, prc = 0.5095\n",
      "Fold 3, Epoch 88, acc = 0.5663, roc = 0.5738, prc = 0.4928\n",
      "Fold 3, Epoch 89, acc = 0.4819, roc = 0.5167, prc = 0.4764\n",
      "Fold 3, Epoch 90, acc = 0.5422, roc = 0.5780, prc = 0.5224\n",
      "Fold 3, Epoch 91, acc = 0.5542, roc = 0.5994, prc = 0.5345\n",
      "Fold 3, Epoch 92, acc = 0.5542, roc = 0.6268, prc = 0.5746\n",
      "Fold 3, Epoch 93, acc = 0.5301, roc = 0.5798, prc = 0.5035\n",
      "------------ Save best model - ACC: 0.6386 ------------\n",
      "Fold 3, Epoch 94, acc = 0.6386, roc = 0.6982, prc = 0.6136\n",
      "Fold 3, Epoch 95, acc = 0.5422, roc = 0.5869, prc = 0.5113\n",
      "Fold 3, Epoch 96, acc = 0.5301, roc = 0.6232, prc = 0.5522\n",
      "Fold 3, Epoch 97, acc = 0.5301, roc = 0.6375, prc = 0.5495\n",
      "Fold 3, Epoch 98, acc = 0.5181, roc = 0.6012, prc = 0.5709\n",
      "Fold 3, Epoch 99, acc = 0.6265, roc = 0.6476, prc = 0.5628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ROC: 0.1911 ------------\n",
      "Fold 4, Epoch 0, acc = 0.5663, roc = 0.1911, prc = 0.2806\n",
      "------------ Save best model - ACC: 0.5904 ------------\n",
      "------------ Save best model - ROC: 0.3976 ------------\n",
      "Fold 4, Epoch 1, acc = 0.5904, roc = 0.3976, prc = 0.3838\n",
      "------------ Save best model - ROC: 0.5810 ------------\n",
      "Fold 4, Epoch 2, acc = 0.5060, roc = 0.5810, prc = 0.4517\n",
      "------------ Save best model - ROC: 0.6601 ------------\n",
      "Fold 4, Epoch 3, acc = 0.5422, roc = 0.6601, prc = 0.6122\n",
      "------------ Save best model - ROC: 0.7000 ------------\n",
      "Fold 4, Epoch 4, acc = 0.4699, roc = 0.7000, prc = 0.6296\n",
      "------------ Save best model - ACC: 0.6386 ------------\n",
      "------------ Save best model - ROC: 0.7256 ------------\n",
      "Fold 4, Epoch 5, acc = 0.6386, roc = 0.7256, prc = 0.6399\n",
      "Fold 4, Epoch 6, acc = 0.4217, roc = 0.6696, prc = 0.5983\n",
      "Fold 4, Epoch 7, acc = 0.4458, roc = 0.5881, prc = 0.4932\n",
      "Fold 4, Epoch 8, acc = 0.5663, roc = 0.6488, prc = 0.6315\n",
      "Fold 4, Epoch 9, acc = 0.4217, roc = 0.5054, prc = 0.4240\n",
      "Fold 4, Epoch 10, acc = 0.4337, roc = 0.7119, prc = 0.6793\n",
      "Fold 4, Epoch 11, acc = 0.5301, roc = 0.5601, prc = 0.4514\n",
      "Fold 4, Epoch 12, acc = 0.5060, roc = 0.6149, prc = 0.4738\n",
      "Fold 4, Epoch 13, acc = 0.6145, roc = 0.6351, prc = 0.5599\n",
      "Fold 4, Epoch 14, acc = 0.6265, roc = 0.5720, prc = 0.4647\n",
      "Fold 4, Epoch 15, acc = 0.5663, roc = 0.6720, prc = 0.5110\n",
      "Fold 4, Epoch 16, acc = 0.6024, roc = 0.7185, prc = 0.6223\n",
      "------------ Save best model - ROC: 0.7726 ------------\n",
      "Fold 4, Epoch 17, acc = 0.5301, roc = 0.7726, prc = 0.6864\n",
      "------------ Save best model - ACC: 0.6506 ------------\n",
      "Fold 4, Epoch 18, acc = 0.6506, roc = 0.6637, prc = 0.5607\n",
      "Fold 4, Epoch 19, acc = 0.5904, roc = 0.6339, prc = 0.5489\n",
      "------------ Save best model - ACC: 0.7108 ------------\n",
      "------------ Save best model - ROC: 0.7893 ------------\n",
      "Fold 4, Epoch 20, acc = 0.7108, roc = 0.7893, prc = 0.7354\n",
      "Fold 4, Epoch 21, acc = 0.6024, roc = 0.6571, prc = 0.5575\n",
      "Fold 4, Epoch 22, acc = 0.5783, roc = 0.6071, prc = 0.4840\n",
      "Fold 4, Epoch 23, acc = 0.5783, roc = 0.6399, prc = 0.5102\n",
      "Fold 4, Epoch 24, acc = 0.4819, roc = 0.4738, prc = 0.3975\n",
      "Fold 4, Epoch 25, acc = 0.5783, roc = 0.5565, prc = 0.4646\n",
      "Fold 4, Epoch 26, acc = 0.6024, roc = 0.6839, prc = 0.6065\n",
      "Fold 4, Epoch 27, acc = 0.4458, roc = 0.5494, prc = 0.4527\n",
      "Fold 4, Epoch 28, acc = 0.6145, roc = 0.6137, prc = 0.5235\n",
      "Fold 4, Epoch 29, acc = 0.4096, roc = 0.6631, prc = 0.5691\n",
      "Fold 4, Epoch 30, acc = 0.3976, roc = 0.5268, prc = 0.4366\n",
      "------------ Save best model - ACC: 0.7470 ------------\n",
      "------------ Save best model - ROC: 0.7982 ------------\n",
      "Fold 4, Epoch 31, acc = 0.7470, roc = 0.7982, prc = 0.7924\n",
      "Fold 4, Epoch 32, acc = 0.7229, roc = 0.7768, prc = 0.7529\n",
      "------------ Save best model - ROC: 0.8137 ------------\n",
      "Fold 4, Epoch 33, acc = 0.7470, roc = 0.8137, prc = 0.8016\n",
      "Fold 4, Epoch 34, acc = 0.4819, roc = 0.7369, prc = 0.7434\n",
      "Fold 4, Epoch 35, acc = 0.7229, roc = 0.7369, prc = 0.6969\n",
      "Fold 4, Epoch 36, acc = 0.5663, roc = 0.6780, prc = 0.6763\n",
      "Fold 4, Epoch 37, acc = 0.4337, roc = 0.6887, prc = 0.6328\n",
      "Fold 4, Epoch 38, acc = 0.6265, roc = 0.6696, prc = 0.5639\n",
      "Fold 4, Epoch 39, acc = 0.3494, roc = 0.3423, prc = 0.4176\n",
      "Fold 4, Epoch 40, acc = 0.6506, roc = 0.7607, prc = 0.6929\n",
      "Fold 4, Epoch 41, acc = 0.4217, roc = 0.4571, prc = 0.4708\n",
      "Fold 4, Epoch 42, acc = 0.5181, roc = 0.5202, prc = 0.5234\n",
      "Fold 4, Epoch 43, acc = 0.6506, roc = 0.6024, prc = 0.5748\n",
      "Fold 4, Epoch 44, acc = 0.5060, roc = 0.4702, prc = 0.4834\n",
      "Fold 4, Epoch 45, acc = 0.5904, roc = 0.5970, prc = 0.5468\n",
      "Fold 4, Epoch 46, acc = 0.4819, roc = 0.4732, prc = 0.4022\n",
      "Fold 4, Epoch 47, acc = 0.5422, roc = 0.3351, prc = 0.3476\n",
      "Fold 4, Epoch 48, acc = 0.3373, roc = 0.3756, prc = 0.4007\n",
      "Fold 4, Epoch 49, acc = 0.6024, roc = 0.5827, prc = 0.4959\n",
      "Fold 4, Epoch 50, acc = 0.3494, roc = 0.3815, prc = 0.4115\n",
      "Fold 4, Epoch 51, acc = 0.5904, roc = 0.6321, prc = 0.5253\n",
      "Fold 4, Epoch 52, acc = 0.5663, roc = 0.6298, prc = 0.5621\n",
      "Fold 4, Epoch 53, acc = 0.4699, roc = 0.5232, prc = 0.5468\n",
      "Fold 4, Epoch 54, acc = 0.6506, roc = 0.6833, prc = 0.6087\n",
      "Fold 4, Epoch 55, acc = 0.5301, roc = 0.5762, prc = 0.5870\n",
      "Fold 4, Epoch 56, acc = 0.5663, roc = 0.5935, prc = 0.5983\n",
      "Fold 4, Epoch 57, acc = 0.5542, roc = 0.5911, prc = 0.5924\n",
      "Fold 4, Epoch 58, acc = 0.6867, roc = 0.6571, prc = 0.6636\n",
      "Fold 4, Epoch 59, acc = 0.4578, roc = 0.5042, prc = 0.5455\n",
      "Fold 4, Epoch 60, acc = 0.6506, roc = 0.5881, prc = 0.6096\n",
      "Fold 4, Epoch 61, acc = 0.6627, roc = 0.6119, prc = 0.5973\n",
      "Fold 4, Epoch 62, acc = 0.6265, roc = 0.6280, prc = 0.6299\n",
      "Fold 4, Epoch 63, acc = 0.6747, roc = 0.6315, prc = 0.6158\n",
      "Fold 4, Epoch 64, acc = 0.5060, roc = 0.5506, prc = 0.5963\n",
      "Fold 4, Epoch 65, acc = 0.4819, roc = 0.4476, prc = 0.5078\n",
      "Fold 4, Epoch 66, acc = 0.5422, roc = 0.5756, prc = 0.5736\n",
      "Fold 4, Epoch 67, acc = 0.5181, roc = 0.5893, prc = 0.5874\n",
      "Fold 4, Epoch 68, acc = 0.5542, roc = 0.5411, prc = 0.5487\n",
      "Fold 4, Epoch 69, acc = 0.5181, roc = 0.5393, prc = 0.5478\n",
      "Fold 4, Epoch 70, acc = 0.5542, roc = 0.5464, prc = 0.5554\n",
      "Fold 4, Epoch 71, acc = 0.3735, roc = 0.4524, prc = 0.4797\n",
      "Fold 4, Epoch 72, acc = 0.5181, roc = 0.4851, prc = 0.4913\n",
      "Fold 4, Epoch 73, acc = 0.4819, roc = 0.5446, prc = 0.5409\n",
      "Fold 4, Epoch 74, acc = 0.4337, roc = 0.5405, prc = 0.5577\n",
      "Fold 4, Epoch 75, acc = 0.5422, roc = 0.5429, prc = 0.5674\n",
      "Fold 4, Epoch 76, acc = 0.5904, roc = 0.5190, prc = 0.5431\n",
      "Fold 4, Epoch 77, acc = 0.5542, roc = 0.4976, prc = 0.4894\n",
      "Fold 4, Epoch 78, acc = 0.5663, roc = 0.6095, prc = 0.5716\n",
      "Fold 4, Epoch 79, acc = 0.4217, roc = 0.4768, prc = 0.4859\n",
      "Fold 4, Epoch 80, acc = 0.5060, roc = 0.6036, prc = 0.5711\n",
      "Fold 4, Epoch 81, acc = 0.5663, roc = 0.6220, prc = 0.6019\n",
      "Fold 4, Epoch 82, acc = 0.6265, roc = 0.5994, prc = 0.5924\n",
      "Fold 4, Epoch 83, acc = 0.4578, roc = 0.4976, prc = 0.5174\n",
      "Fold 4, Epoch 84, acc = 0.4096, roc = 0.5000, prc = 0.5602\n",
      "Fold 4, Epoch 85, acc = 0.6145, roc = 0.5298, prc = 0.5695\n",
      "Fold 4, Epoch 86, acc = 0.4217, roc = 0.4673, prc = 0.5345\n",
      "Fold 4, Epoch 87, acc = 0.5060, roc = 0.4946, prc = 0.5291\n",
      "Fold 4, Epoch 88, acc = 0.4699, roc = 0.4881, prc = 0.5277\n",
      "Fold 4, Epoch 89, acc = 0.4578, roc = 0.5143, prc = 0.5262\n",
      "Fold 4, Epoch 90, acc = 0.4819, roc = 0.4917, prc = 0.5234\n",
      "Fold 4, Epoch 91, acc = 0.5301, roc = 0.5161, prc = 0.5351\n",
      "Fold 4, Epoch 92, acc = 0.4699, roc = 0.5399, prc = 0.5501\n",
      "Fold 4, Epoch 93, acc = 0.5542, roc = 0.5369, prc = 0.5373\n",
      "Fold 4, Epoch 94, acc = 0.5542, roc = 0.6071, prc = 0.6036\n",
      "Fold 4, Epoch 95, acc = 0.4819, roc = 0.5589, prc = 0.5697\n",
      "Fold 4, Epoch 96, acc = 0.4940, roc = 0.5435, prc = 0.5634\n",
      "Fold 4, Epoch 97, acc = 0.5181, roc = 0.5702, prc = 0.5709\n",
      "Fold 4, Epoch 98, acc = 0.5301, roc = 0.5798, prc = 0.5597\n",
      "Fold 4, Epoch 99, acc = 0.5301, roc = 0.5298, prc = 0.5378\n",
      "------------ Save best model - ROC: 0.8411 ------------\n",
      "Fold 5, Epoch 0, acc = 0.6506, roc = 0.8411, prc = 0.8628\n",
      "------------ Save best model - ACC: 0.8193 ------------\n",
      "Fold 5, Epoch 1, acc = 0.8193, roc = 0.8107, prc = 0.8150\n",
      "Fold 5, Epoch 2, acc = 0.8193, roc = 0.8006, prc = 0.8083\n",
      "Fold 5, Epoch 3, acc = 0.5422, roc = 0.7131, prc = 0.5351\n",
      "Fold 5, Epoch 4, acc = 0.4096, roc = 0.4929, prc = 0.3730\n",
      "Fold 5, Epoch 21, acc = 0.4096, roc = 0.8036, prc = 0.8361\n",
      "Fold 5, Epoch 22, acc = 0.4578, roc = 0.5994, prc = 0.5926\n",
      "------------ Save best model - ROC: 0.8536 ------------\n",
      "Fold 5, Epoch 23, acc = 0.5181, roc = 0.8536, prc = 0.8734\n",
      "Fold 5, Epoch 24, acc = 0.5542, roc = 0.6607, prc = 0.6111\n",
      "Fold 5, Epoch 25, acc = 0.6867, roc = 0.7780, prc = 0.6373\n",
      "Fold 5, Epoch 26, acc = 0.6627, roc = 0.7905, prc = 0.6347\n",
      "Fold 5, Epoch 27, acc = 0.4578, roc = 0.4417, prc = 0.3774\n",
      "Fold 5, Epoch 28, acc = 0.5060, roc = 0.6071, prc = 0.4439\n",
      "Fold 5, Epoch 29, acc = 0.5542, roc = 0.5536, prc = 0.4278\n",
      "Fold 5, Epoch 30, acc = 0.4819, roc = 0.4649, prc = 0.3812\n",
      "Fold 5, Epoch 31, acc = 0.4458, roc = 0.4542, prc = 0.3740\n",
      "Fold 5, Epoch 32, acc = 0.4096, roc = 0.5143, prc = 0.4429\n",
      "Fold 5, Epoch 33, acc = 0.5542, roc = 0.6167, prc = 0.5142\n",
      "Fold 5, Epoch 34, acc = 0.4458, roc = 0.3458, prc = 0.3319\n",
      "Fold 5, Epoch 35, acc = 0.6145, roc = 0.6577, prc = 0.5453\n",
      "------------ Save best model - ROC: 0.8786 ------------\n",
      "Fold 5, Epoch 36, acc = 0.5181, roc = 0.8786, prc = 0.8885\n",
      "Fold 5, Epoch 37, acc = 0.5301, roc = 0.2839, prc = 0.3124\n",
      "Fold 5, Epoch 38, acc = 0.6747, roc = 0.7185, prc = 0.5938\n",
      "Fold 5, Epoch 39, acc = 0.5663, roc = 0.6643, prc = 0.5889\n",
      "Fold 5, Epoch 40, acc = 0.5422, roc = 0.7875, prc = 0.7573\n",
      "Fold 5, Epoch 41, acc = 0.6627, roc = 0.8202, prc = 0.7777\n",
      "Fold 5, Epoch 42, acc = 0.7349, roc = 0.7423, prc = 0.6753\n",
      "Fold 5, Epoch 43, acc = 0.5422, roc = 0.6077, prc = 0.4986\n",
      "Fold 5, Epoch 44, acc = 0.5904, roc = 0.8351, prc = 0.7956\n",
      "Fold 5, Epoch 45, acc = 0.5783, roc = 0.7923, prc = 0.7332\n",
      "Fold 5, Epoch 46, acc = 0.4819, roc = 0.5917, prc = 0.4440\n",
      "Fold 5, Epoch 47, acc = 0.5422, roc = 0.5405, prc = 0.4103\n",
      "Fold 5, Epoch 48, acc = 0.6988, roc = 0.6821, prc = 0.5033\n",
      "Fold 5, Epoch 49, acc = 0.4337, roc = 0.5863, prc = 0.4819\n",
      "Fold 5, Epoch 50, acc = 0.6627, roc = 0.6250, prc = 0.5664\n",
      "Fold 5, Epoch 51, acc = 0.5422, roc = 0.4631, prc = 0.3899\n",
      "Fold 5, Epoch 52, acc = 0.5542, roc = 0.6018, prc = 0.5991\n",
      "Fold 5, Epoch 53, acc = 0.4458, roc = 0.5256, prc = 0.4124\n",
      "Fold 5, Epoch 54, acc = 0.4699, roc = 0.4774, prc = 0.3810\n",
      "Fold 5, Epoch 55, acc = 0.5060, roc = 0.6375, prc = 0.6315\n",
      "Fold 5, Epoch 56, acc = 0.4819, roc = 0.5060, prc = 0.4094\n",
      "Fold 5, Epoch 57, acc = 0.4337, roc = 0.4798, prc = 0.3828\n",
      "Fold 5, Epoch 58, acc = 0.4217, roc = 0.2940, prc = 0.3059\n",
      "Fold 5, Epoch 59, acc = 0.3855, roc = 0.3607, prc = 0.3340\n",
      "Fold 5, Epoch 60, acc = 0.4217, roc = 0.2089, prc = 0.2841\n",
      "Fold 5, Epoch 61, acc = 0.5542, roc = 0.5935, prc = 0.4840\n",
      "Fold 5, Epoch 62, acc = 0.5422, roc = 0.5637, prc = 0.4508\n",
      "Fold 5, Epoch 63, acc = 0.6024, roc = 0.5649, prc = 0.4422\n",
      "Fold 5, Epoch 64, acc = 0.5542, roc = 0.5798, prc = 0.4563\n",
      "Fold 5, Epoch 65, acc = 0.5542, roc = 0.6149, prc = 0.5053\n",
      "Fold 5, Epoch 66, acc = 0.5422, roc = 0.5869, prc = 0.4561\n",
      "Fold 5, Epoch 67, acc = 0.6386, roc = 0.6089, prc = 0.4931\n",
      "Fold 5, Epoch 68, acc = 0.4819, roc = 0.5923, prc = 0.5075\n",
      "Fold 5, Epoch 69, acc = 0.6145, roc = 0.5702, prc = 0.4428\n",
      "Fold 5, Epoch 70, acc = 0.5181, roc = 0.5417, prc = 0.4310\n",
      "Fold 5, Epoch 71, acc = 0.4940, roc = 0.5256, prc = 0.4290\n",
      "Fold 5, Epoch 72, acc = 0.4819, roc = 0.5036, prc = 0.4078\n",
      "Fold 5, Epoch 73, acc = 0.5783, roc = 0.5869, prc = 0.4658\n",
      "Fold 5, Epoch 74, acc = 0.5422, roc = 0.5440, prc = 0.4426\n",
      "Fold 5, Epoch 75, acc = 0.4819, roc = 0.5060, prc = 0.4026\n",
      "Fold 5, Epoch 76, acc = 0.4940, roc = 0.5179, prc = 0.4158\n",
      "Fold 5, Epoch 77, acc = 0.4819, roc = 0.5417, prc = 0.4304\n",
      "Fold 5, Epoch 78, acc = 0.5060, roc = 0.5149, prc = 0.4148\n",
      "Fold 5, Epoch 79, acc = 0.5301, roc = 0.4923, prc = 0.3963\n",
      "Fold 5, Epoch 80, acc = 0.4217, roc = 0.4857, prc = 0.3931\n",
      "Fold 5, Epoch 81, acc = 0.5181, roc = 0.4982, prc = 0.3995\n",
      "Fold 5, Epoch 82, acc = 0.4699, roc = 0.4976, prc = 0.4012\n",
      "Fold 5, Epoch 83, acc = 0.4578, roc = 0.5185, prc = 0.4163\n",
      "Fold 5, Epoch 84, acc = 0.5422, roc = 0.5399, prc = 0.4208\n",
      "Fold 5, Epoch 85, acc = 0.5181, roc = 0.4964, prc = 0.3916\n",
      "Fold 5, Epoch 86, acc = 0.5060, roc = 0.5393, prc = 0.4315\n",
      "Fold 5, Epoch 87, acc = 0.5301, roc = 0.5107, prc = 0.4061\n",
      "Fold 5, Epoch 88, acc = 0.5301, roc = 0.5149, prc = 0.4121\n",
      "Fold 5, Epoch 89, acc = 0.5783, roc = 0.5560, prc = 0.4433\n",
      "Fold 5, Epoch 90, acc = 0.5904, roc = 0.6167, prc = 0.4879\n",
      "Fold 5, Epoch 91, acc = 0.5783, roc = 0.5339, prc = 0.4444\n",
      "Fold 5, Epoch 92, acc = 0.5301, roc = 0.5077, prc = 0.4034\n",
      "Fold 5, Epoch 93, acc = 0.5181, roc = 0.5060, prc = 0.3954\n",
      "Fold 5, Epoch 94, acc = 0.5181, roc = 0.4881, prc = 0.3863\n",
      "Fold 5, Epoch 95, acc = 0.5060, roc = 0.5167, prc = 0.4139\n",
      "Fold 5, Epoch 96, acc = 0.5301, roc = 0.5369, prc = 0.4244\n",
      "Fold 5, Epoch 97, acc = 0.4940, roc = 0.4798, prc = 0.3817\n",
      "Fold 5, Epoch 98, acc = 0.5422, roc = 0.5173, prc = 0.4076\n",
      "Fold 5, Epoch 99, acc = 0.5422, roc = 0.5095, prc = 0.4032\n",
      "------------ Save best model - ACC: 0.9518 ------------\n",
      "------------ Save best model - ROC: 0.9881 ------------\n",
      "Fold 6, Epoch 0, acc = 0.9518, roc = 0.9881, prc = 0.9850\n",
      "Fold 6, Epoch 1, acc = 0.6145, roc = 0.6256, prc = 0.5855\n",
      "Fold 6, Epoch 2, acc = 0.6386, roc = 0.7161, prc = 0.6490\n",
      "Fold 6, Epoch 3, acc = 0.6024, roc = 0.3887, prc = 0.3862\n",
      "Fold 6, Epoch 4, acc = 0.5783, roc = 0.4173, prc = 0.3769\n",
      "Fold 6, Epoch 5, acc = 0.5542, roc = 0.3970, prc = 0.3919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6, Epoch 6, acc = 0.5783, roc = 0.4351, prc = 0.3988\n",
      "Fold 6, Epoch 7, acc = 0.6145, roc = 0.7685, prc = 0.6979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6, Epoch 8, acc = 0.5783, roc = 0.4887, prc = 0.4305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6, Epoch 9, acc = 0.5783, roc = 0.5393, prc = 0.4605\n",
      "Fold 6, Epoch 10, acc = 0.6145, roc = 0.7476, prc = 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6, Epoch 11, acc = 0.5783, roc = 0.4327, prc = 0.3892\n",
      "Fold 6, Epoch 12, acc = 0.5301, roc = 0.5518, prc = 0.4365\n",
      "Fold 6, Epoch 13, acc = 0.7349, roc = 0.7476, prc = 0.6430\n",
      "Fold 6, Epoch 14, acc = 0.5904, roc = 0.7006, prc = 0.6081\n",
      "Fold 6, Epoch 15, acc = 0.5542, roc = 0.4899, prc = 0.4273\n",
      "Fold 6, Epoch 16, acc = 0.5904, roc = 0.5762, prc = 0.5435\n",
      "Fold 6, Epoch 17, acc = 0.5904, roc = 0.4131, prc = 0.3854\n",
      "Fold 6, Epoch 18, acc = 0.5783, roc = 0.3488, prc = 0.3329\n",
      "Fold 6, Epoch 19, acc = 0.5181, roc = 0.3673, prc = 0.3691\n",
      "Fold 6, Epoch 20, acc = 0.5542, roc = 0.4065, prc = 0.3722\n",
      "Fold 6, Epoch 21, acc = 0.5422, roc = 0.2875, prc = 0.3196\n",
      "Fold 6, Epoch 22, acc = 0.5663, roc = 0.3869, prc = 0.3548\n",
      "Fold 6, Epoch 23, acc = 0.6867, roc = 0.6071, prc = 0.5510\n",
      "Fold 6, Epoch 24, acc = 0.5181, roc = 0.6982, prc = 0.6285\n",
      "Fold 6, Epoch 25, acc = 0.6145, roc = 0.8196, prc = 0.8160\n",
      "Fold 6, Epoch 26, acc = 0.4819, roc = 0.5762, prc = 0.4655\n",
      "Fold 6, Epoch 27, acc = 0.4578, roc = 0.6583, prc = 0.5454\n",
      "Fold 6, Epoch 28, acc = 0.4337, roc = 0.6595, prc = 0.6189\n",
      "Fold 6, Epoch 29, acc = 0.5060, roc = 0.6262, prc = 0.4639\n",
      "Fold 6, Epoch 30, acc = 0.7108, roc = 0.7411, prc = 0.6843\n",
      "Fold 6, Epoch 31, acc = 0.4699, roc = 0.8077, prc = 0.7419\n",
      "Fold 6, Epoch 32, acc = 0.7229, roc = 0.7345, prc = 0.6797\n",
      "Fold 6, Epoch 33, acc = 0.4458, roc = 0.6363, prc = 0.4760\n",
      "Fold 6, Epoch 34, acc = 0.7470, roc = 0.8435, prc = 0.7482\n",
      "Fold 6, Epoch 35, acc = 0.6506, roc = 0.6333, prc = 0.5852\n",
      "Fold 6, Epoch 36, acc = 0.4699, roc = 0.7625, prc = 0.6383\n",
      "Fold 6, Epoch 37, acc = 0.6145, roc = 0.7083, prc = 0.6836\n",
      "Fold 6, Epoch 38, acc = 0.5422, roc = 0.5958, prc = 0.4348\n",
      "Fold 6, Epoch 39, acc = 0.5301, roc = 0.5958, prc = 0.4419\n",
      "Fold 6, Epoch 40, acc = 0.5542, roc = 0.5244, prc = 0.4635\n",
      "Fold 6, Epoch 41, acc = 0.5181, roc = 0.5042, prc = 0.4229\n",
      "Fold 6, Epoch 42, acc = 0.6024, roc = 0.6268, prc = 0.4596\n",
      "Fold 6, Epoch 43, acc = 0.6145, roc = 0.7012, prc = 0.6196\n",
      "Fold 6, Epoch 44, acc = 0.5542, roc = 0.6321, prc = 0.5765\n",
      "Fold 6, Epoch 45, acc = 0.4699, roc = 0.6292, prc = 0.5815\n",
      "Fold 6, Epoch 46, acc = 0.4578, roc = 0.5869, prc = 0.5089\n",
      "Fold 6, Epoch 47, acc = 0.6145, roc = 0.6524, prc = 0.5175\n",
      "Fold 6, Epoch 48, acc = 0.5904, roc = 0.7089, prc = 0.6466\n",
      "Fold 6, Epoch 49, acc = 0.5060, roc = 0.5286, prc = 0.4619\n",
      "Fold 6, Epoch 50, acc = 0.5060, roc = 0.4179, prc = 0.3903\n",
      "Fold 6, Epoch 51, acc = 0.5663, roc = 0.5256, prc = 0.5227\n",
      "Fold 6, Epoch 52, acc = 0.3855, roc = 0.4315, prc = 0.4236\n",
      "Fold 6, Epoch 53, acc = 0.5301, roc = 0.5399, prc = 0.4808\n",
      "Fold 6, Epoch 54, acc = 0.4699, roc = 0.4054, prc = 0.4130\n",
      "Fold 6, Epoch 55, acc = 0.5663, roc = 0.3863, prc = 0.4413\n",
      "Fold 6, Epoch 56, acc = 0.4819, roc = 0.5714, prc = 0.5278\n",
      "Fold 6, Epoch 57, acc = 0.5301, roc = 0.5958, prc = 0.4959\n",
      "Fold 6, Epoch 58, acc = 0.4699, roc = 0.5381, prc = 0.4204\n",
      "Fold 6, Epoch 59, acc = 0.4940, roc = 0.5774, prc = 0.5136\n",
      "Fold 6, Epoch 60, acc = 0.5904, roc = 0.5917, prc = 0.5569\n",
      "Fold 6, Epoch 61, acc = 0.5181, roc = 0.6375, prc = 0.5663\n",
      "Fold 6, Epoch 62, acc = 0.5422, roc = 0.5631, prc = 0.4649\n",
      "Fold 6, Epoch 63, acc = 0.5904, roc = 0.6940, prc = 0.6435\n",
      "Fold 6, Epoch 64, acc = 0.5422, roc = 0.4494, prc = 0.3885\n",
      "Fold 6, Epoch 65, acc = 0.3373, roc = 0.3970, prc = 0.3807\n",
      "Fold 6, Epoch 66, acc = 0.5542, roc = 0.4548, prc = 0.4350\n",
      "Fold 6, Epoch 67, acc = 0.5542, roc = 0.5643, prc = 0.5588\n",
      "Fold 6, Epoch 68, acc = 0.5422, roc = 0.6405, prc = 0.6079\n",
      "Fold 6, Epoch 69, acc = 0.4458, roc = 0.4470, prc = 0.4619\n",
      "Fold 6, Epoch 70, acc = 0.5181, roc = 0.6238, prc = 0.6043\n",
      "Fold 6, Epoch 71, acc = 0.4337, roc = 0.6845, prc = 0.6682\n",
      "Fold 6, Epoch 72, acc = 0.5904, roc = 0.6452, prc = 0.5680\n",
      "Fold 6, Epoch 73, acc = 0.5422, roc = 0.6298, prc = 0.6011\n",
      "Fold 6, Epoch 74, acc = 0.6506, roc = 0.6488, prc = 0.6183\n",
      "Fold 6, Epoch 75, acc = 0.5301, roc = 0.5768, prc = 0.5336\n",
      "Fold 6, Epoch 76, acc = 0.5060, roc = 0.5393, prc = 0.5114\n",
      "Fold 6, Epoch 77, acc = 0.4819, roc = 0.5113, prc = 0.5061\n",
      "Fold 6, Epoch 78, acc = 0.5783, roc = 0.6685, prc = 0.6246\n",
      "Fold 6, Epoch 79, acc = 0.6386, roc = 0.6488, prc = 0.5982\n",
      "Fold 6, Epoch 80, acc = 0.5663, roc = 0.4774, prc = 0.4767\n",
      "Fold 6, Epoch 81, acc = 0.5783, roc = 0.5685, prc = 0.5199\n",
      "Fold 6, Epoch 82, acc = 0.5663, roc = 0.5696, prc = 0.5563\n",
      "Fold 6, Epoch 83, acc = 0.4578, roc = 0.6054, prc = 0.5630\n",
      "Fold 6, Epoch 84, acc = 0.4578, roc = 0.6196, prc = 0.5716\n",
      "Fold 6, Epoch 85, acc = 0.5542, roc = 0.6631, prc = 0.5768\n",
      "Fold 6, Epoch 86, acc = 0.5181, roc = 0.6494, prc = 0.6238\n",
      "Fold 6, Epoch 87, acc = 0.5301, roc = 0.4185, prc = 0.4402\n",
      "Fold 6, Epoch 88, acc = 0.5181, roc = 0.4542, prc = 0.4555\n",
      "Fold 6, Epoch 89, acc = 0.5301, roc = 0.4310, prc = 0.4243\n",
      "Fold 6, Epoch 90, acc = 0.4940, roc = 0.4089, prc = 0.4015\n",
      "Fold 6, Epoch 91, acc = 0.4940, roc = 0.5411, prc = 0.4995\n",
      "Fold 6, Epoch 92, acc = 0.4699, roc = 0.4845, prc = 0.4677\n",
      "Fold 6, Epoch 93, acc = 0.5422, roc = 0.5131, prc = 0.4806\n",
      "Fold 6, Epoch 94, acc = 0.5542, roc = 0.5786, prc = 0.5011\n",
      "Fold 6, Epoch 95, acc = 0.5181, roc = 0.5643, prc = 0.5063\n",
      "Fold 6, Epoch 96, acc = 0.5060, roc = 0.5190, prc = 0.4754\n",
      "Fold 6, Epoch 97, acc = 0.5060, roc = 0.5732, prc = 0.5140\n",
      "Fold 6, Epoch 98, acc = 0.4699, roc = 0.6417, prc = 0.6025\n",
      "Fold 6, Epoch 99, acc = 0.5422, roc = 0.6839, prc = 0.6577\n",
      "------------ Save best model - ROC: 0.8387 ------------\n",
      "Fold 7, Epoch 0, acc = 0.4699, roc = 0.8387, prc = 0.8476\n",
      "------------ Save best model - ACC: 0.6024 ------------\n",
      "Fold 7, Epoch 1, acc = 0.6024, roc = 0.6143, prc = 0.5290\n",
      "Fold 7, Epoch 2, acc = 0.4940, roc = 0.6315, prc = 0.6045\n",
      "Fold 7, Epoch 3, acc = 0.5663, roc = 0.5500, prc = 0.5098\n",
      "Fold 7, Epoch 4, acc = 0.4819, roc = 0.4357, prc = 0.4360\n",
      "Fold 7, Epoch 5, acc = 0.4699, roc = 0.4548, prc = 0.4517\n",
      "Fold 7, Epoch 6, acc = 0.5181, roc = 0.4988, prc = 0.5126\n",
      "Fold 7, Epoch 7, acc = 0.5904, roc = 0.4500, prc = 0.4553\n",
      "Fold 7, Epoch 8, acc = 0.5663, roc = 0.3946, prc = 0.4046\n",
      "Fold 7, Epoch 9, acc = 0.5904, roc = 0.4923, prc = 0.4850\n",
      "Fold 7, Epoch 10, acc = 0.5301, roc = 0.4863, prc = 0.4811\n",
      "------------ Save best model - ACC: 0.6627 ------------\n",
      "Fold 7, Epoch 11, acc = 0.6627, roc = 0.7208, prc = 0.6595\n",
      "Fold 7, Epoch 12, acc = 0.5783, roc = 0.5280, prc = 0.4391\n",
      "Fold 7, Epoch 13, acc = 0.6024, roc = 0.6048, prc = 0.5169\n",
      "Fold 7, Epoch 14, acc = 0.5904, roc = 0.5387, prc = 0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7, Epoch 15, acc = 0.5783, roc = 0.5095, prc = 0.4442\n",
      "Fold 7, Epoch 16, acc = 0.4699, roc = 0.5232, prc = 0.4585\n",
      "Fold 7, Epoch 17, acc = 0.4940, roc = 0.4089, prc = 0.4060\n",
      "Fold 7, Epoch 18, acc = 0.5904, roc = 0.4494, prc = 0.4288\n",
      "Fold 7, Epoch 19, acc = 0.4578, roc = 0.4774, prc = 0.4539\n",
      "Fold 7, Epoch 20, acc = 0.5663, roc = 0.4685, prc = 0.4406\n",
      "Fold 7, Epoch 21, acc = 0.6265, roc = 0.5440, prc = 0.5263\n",
      "Fold 7, Epoch 22, acc = 0.5783, roc = 0.5429, prc = 0.4717\n",
      "Fold 7, Epoch 23, acc = 0.5783, roc = 0.5238, prc = 0.4407\n",
      "Fold 7, Epoch 24, acc = 0.5542, roc = 0.5274, prc = 0.4749\n",
      "Fold 7, Epoch 25, acc = 0.4819, roc = 0.7250, prc = 0.7349\n",
      "Fold 7, Epoch 26, acc = 0.4458, roc = 0.7821, prc = 0.7976\n",
      "Fold 7, Epoch 27, acc = 0.4578, roc = 0.7911, prc = 0.7479\n",
      "------------ Save best model - ACC: 0.6867 ------------\n",
      "Fold 7, Epoch 28, acc = 0.6867, roc = 0.7542, prc = 0.7711\n",
      "Fold 7, Epoch 29, acc = 0.4819, roc = 0.7113, prc = 0.6771\n",
      "Fold 7, Epoch 30, acc = 0.6867, roc = 0.8000, prc = 0.7846\n",
      "Fold 7, Epoch 31, acc = 0.5663, roc = 0.5923, prc = 0.4550\n",
      "Fold 7, Epoch 32, acc = 0.4699, roc = 0.5024, prc = 0.4572\n",
      "Fold 7, Epoch 33, acc = 0.5060, roc = 0.6202, prc = 0.4739\n",
      "Fold 7, Epoch 34, acc = 0.5542, roc = 0.7810, prc = 0.7726\n",
      "Fold 7, Epoch 35, acc = 0.6627, roc = 0.7280, prc = 0.6513\n",
      "Fold 7, Epoch 36, acc = 0.6024, roc = 0.7833, prc = 0.7568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:22: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec0 = cf[0][0] / (cf[0][0] + cf[1][0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7, Epoch 37, acc = 0.4217, roc = 0.8107, prc = 0.7940\n",
      "Fold 7, Epoch 38, acc = 0.4819, roc = 0.7482, prc = 0.6957\n",
      "Fold 7, Epoch 39, acc = 0.5422, roc = 0.8190, prc = 0.7929\n",
      "Fold 7, Epoch 40, acc = 0.4699, roc = 0.7827, prc = 0.7794\n",
      "Fold 7, Epoch 41, acc = 0.5542, roc = 0.7304, prc = 0.5817\n",
      "Fold 7, Epoch 42, acc = 0.6265, roc = 0.7327, prc = 0.5801\n",
      "Fold 7, Epoch 43, acc = 0.5181, roc = 0.7982, prc = 0.7951\n",
      "Fold 7, Epoch 44, acc = 0.6506, roc = 0.6673, prc = 0.5486\n",
      "Fold 7, Epoch 45, acc = 0.5422, roc = 0.5827, prc = 0.4670\n",
      "Fold 7, Epoch 46, acc = 0.5783, roc = 0.6315, prc = 0.5073\n",
      "Fold 7, Epoch 47, acc = 0.4699, roc = 0.6815, prc = 0.5685\n",
      "Fold 7, Epoch 48, acc = 0.5783, roc = 0.6851, prc = 0.5758\n",
      "Fold 7, Epoch 49, acc = 0.5422, roc = 0.6964, prc = 0.5931\n",
      "Fold 7, Epoch 50, acc = 0.5181, roc = 0.7440, prc = 0.7174\n",
      "Fold 7, Epoch 51, acc = 0.5301, roc = 0.7161, prc = 0.5871\n",
      "Fold 7, Epoch 52, acc = 0.4940, roc = 0.7083, prc = 0.6100\n",
      "Fold 7, Epoch 53, acc = 0.5422, roc = 0.6690, prc = 0.5166\n",
      "Fold 7, Epoch 54, acc = 0.4578, roc = 0.6631, prc = 0.5164\n",
      "Fold 7, Epoch 55, acc = 0.5301, roc = 0.6732, prc = 0.5404\n",
      "Fold 7, Epoch 56, acc = 0.4699, roc = 0.6857, prc = 0.5969\n",
      "Fold 7, Epoch 57, acc = 0.5542, roc = 0.6536, prc = 0.5342\n",
      "Fold 7, Epoch 58, acc = 0.5783, roc = 0.7363, prc = 0.6144\n",
      "Fold 7, Epoch 59, acc = 0.5181, roc = 0.6946, prc = 0.5505\n",
      "Fold 7, Epoch 60, acc = 0.6265, roc = 0.6631, prc = 0.5747\n",
      "Fold 7, Epoch 61, acc = 0.5904, roc = 0.7292, prc = 0.6411\n",
      "Fold 7, Epoch 62, acc = 0.5181, roc = 0.7107, prc = 0.6643\n",
      "Fold 7, Epoch 63, acc = 0.6747, roc = 0.6940, prc = 0.5665\n",
      "Fold 7, Epoch 64, acc = 0.5542, roc = 0.7458, prc = 0.6521\n",
      "Fold 7, Epoch 65, acc = 0.4578, roc = 0.7268, prc = 0.6825\n",
      "Fold 7, Epoch 66, acc = 0.4940, roc = 0.7208, prc = 0.6753\n",
      "Fold 7, Epoch 67, acc = 0.5060, roc = 0.6845, prc = 0.5706\n",
      "Fold 7, Epoch 68, acc = 0.6386, roc = 0.7357, prc = 0.6230\n",
      "Fold 7, Epoch 69, acc = 0.5060, roc = 0.7262, prc = 0.6594\n",
      "Fold 7, Epoch 70, acc = 0.4819, roc = 0.7351, prc = 0.6412\n",
      "Fold 7, Epoch 71, acc = 0.5060, roc = 0.6387, prc = 0.5143\n",
      "Fold 7, Epoch 72, acc = 0.5060, roc = 0.7161, prc = 0.6188\n",
      "Fold 7, Epoch 73, acc = 0.5542, roc = 0.6417, prc = 0.4883\n",
      "------------ Save best model - ACC: 0.7229 ------------\n",
      "Fold 7, Epoch 74, acc = 0.7229, roc = 0.7125, prc = 0.6167\n",
      "Fold 7, Epoch 75, acc = 0.5783, roc = 0.6458, prc = 0.4988\n",
      "Fold 7, Epoch 76, acc = 0.5422, roc = 0.7060, prc = 0.6537\n",
      "Fold 7, Epoch 77, acc = 0.5181, roc = 0.7637, prc = 0.6782\n",
      "Fold 7, Epoch 78, acc = 0.4819, roc = 0.7455, prc = 0.7197\n",
      "Fold 7, Epoch 79, acc = 0.6627, roc = 0.7304, prc = 0.6557\n",
      "Fold 7, Epoch 80, acc = 0.6265, roc = 0.5917, prc = 0.5357\n",
      "Fold 7, Epoch 81, acc = 0.4940, roc = 0.7476, prc = 0.6938\n",
      "Fold 7, Epoch 82, acc = 0.5542, roc = 0.6899, prc = 0.5888\n",
      "Fold 7, Epoch 83, acc = 0.5542, roc = 0.7048, prc = 0.6168\n",
      "Fold 7, Epoch 84, acc = 0.5060, roc = 0.7131, prc = 0.6614\n",
      "Fold 7, Epoch 85, acc = 0.5783, roc = 0.5744, prc = 0.4508\n",
      "Fold 7, Epoch 86, acc = 0.5542, roc = 0.6702, prc = 0.5618\n",
      "Fold 7, Epoch 87, acc = 0.5542, roc = 0.6958, prc = 0.6084\n",
      "Fold 7, Epoch 88, acc = 0.6386, roc = 0.6500, prc = 0.4983\n",
      "Fold 7, Epoch 89, acc = 0.5422, roc = 0.6232, prc = 0.4762\n",
      "Fold 7, Epoch 90, acc = 0.5301, roc = 0.6286, prc = 0.4776\n",
      "Fold 7, Epoch 91, acc = 0.5542, roc = 0.5946, prc = 0.4912\n",
      "Fold 7, Epoch 92, acc = 0.5301, roc = 0.7518, prc = 0.6965\n",
      "Fold 7, Epoch 93, acc = 0.6386, roc = 0.7542, prc = 0.6612\n",
      "Fold 7, Epoch 94, acc = 0.5904, roc = 0.6756, prc = 0.5590\n",
      "Fold 7, Epoch 95, acc = 0.6265, roc = 0.6405, prc = 0.5017\n",
      "Fold 7, Epoch 96, acc = 0.5663, roc = 0.6077, prc = 0.4947\n",
      "Fold 7, Epoch 97, acc = 0.5542, roc = 0.5696, prc = 0.4755\n",
      "Fold 7, Epoch 98, acc = 0.6265, roc = 0.6524, prc = 0.5342\n",
      "Fold 7, Epoch 99, acc = 0.6265, roc = 0.6381, prc = 0.4976\n",
      "------------ Save best model - ACC: 0.8795 ------------\n",
      "------------ Save best model - ROC: 0.9370 ------------\n",
      "Fold 8, Epoch 0, acc = 0.8795, roc = 0.9370, prc = 0.9468\n",
      "Fold 8, Epoch 1, acc = 0.6627, roc = 0.7047, prc = 0.6631\n",
      "Fold 8, Epoch 2, acc = 0.6867, roc = 0.7239, prc = 0.6737\n",
      "Fold 8, Epoch 3, acc = 0.6024, roc = 0.5276, prc = 0.4813\n",
      "Fold 8, Epoch 4, acc = 0.6265, roc = 0.4850, prc = 0.4582\n",
      "Fold 8, Epoch 5, acc = 0.6024, roc = 0.6741, prc = 0.6279\n",
      "Fold 8, Epoch 6, acc = 0.6265, roc = 0.6092, prc = 0.5900\n",
      "Fold 8, Epoch 7, acc = 0.6024, roc = 0.5852, prc = 0.4884\n",
      "Fold 8, Epoch 8, acc = 0.6024, roc = 0.6116, prc = 0.5731\n",
      "Fold 8, Epoch 9, acc = 0.6024, roc = 0.6549, prc = 0.6081\n",
      "Fold 8, Epoch 10, acc = 0.6145, roc = 0.8001, prc = 0.7546\n",
      "Fold 8, Epoch 11, acc = 0.6145, roc = 0.7071, prc = 0.6383\n",
      "Fold 8, Epoch 12, acc = 0.6265, roc = 0.6735, prc = 0.5778\n",
      "Fold 8, Epoch 13, acc = 0.5904, roc = 0.5690, prc = 0.5056\n",
      "Fold 8, Epoch 14, acc = 0.6145, roc = 0.5516, prc = 0.5239\n",
      "Fold 8, Epoch 15, acc = 0.5663, roc = 0.6423, prc = 0.5687\n",
      "Fold 8, Epoch 16, acc = 0.6265, roc = 0.7089, prc = 0.6716\n",
      "Fold 8, Epoch 17, acc = 0.6265, roc = 0.5168, prc = 0.4831\n",
      "Fold 8, Epoch 18, acc = 0.5542, roc = 0.5540, prc = 0.4507\n",
      "Fold 8, Epoch 19, acc = 0.5181, roc = 0.5888, prc = 0.4759\n",
      "Fold 8, Epoch 20, acc = 0.4819, roc = 0.5714, prc = 0.4239\n",
      "Fold 8, Epoch 21, acc = 0.5301, roc = 0.5822, prc = 0.5308\n",
      "Fold 8, Epoch 22, acc = 0.6386, roc = 0.5186, prc = 0.4973\n",
      "Fold 8, Epoch 23, acc = 0.5422, roc = 0.6573, prc = 0.5841\n",
      "Fold 8, Epoch 24, acc = 0.6145, roc = 0.7005, prc = 0.6294\n",
      "Fold 8, Epoch 25, acc = 0.6265, roc = 0.6975, prc = 0.5950\n",
      "Fold 8, Epoch 26, acc = 0.5060, roc = 0.6531, prc = 0.5425\n",
      "Fold 8, Epoch 27, acc = 0.4940, roc = 0.6477, prc = 0.5872\n",
      "Fold 8, Epoch 28, acc = 0.4458, roc = 0.6110, prc = 0.5762\n",
      "Fold 8, Epoch 29, acc = 0.6627, roc = 0.7053, prc = 0.6501\n",
      "Fold 8, Epoch 30, acc = 0.5060, roc = 0.5528, prc = 0.5094\n",
      "Fold 8, Epoch 44, acc = 0.4578, roc = 0.7305, prc = 0.6384\n",
      "Fold 8, Epoch 45, acc = 0.4337, roc = 0.7893, prc = 0.8006\n",
      "Fold 8, Epoch 46, acc = 0.6747, roc = 0.8331, prc = 0.8040\n",
      "Fold 8, Epoch 47, acc = 0.6867, roc = 0.6735, prc = 0.4736\n",
      "Fold 8, Epoch 48, acc = 0.4699, roc = 0.5930, prc = 0.4187\n",
      "Fold 8, Epoch 49, acc = 0.5181, roc = 0.6068, prc = 0.5248\n",
      "Fold 8, Epoch 50, acc = 0.7470, roc = 0.7767, prc = 0.7260\n",
      "Fold 8, Epoch 51, acc = 0.6024, roc = 0.5666, prc = 0.5340\n",
      "Fold 8, Epoch 52, acc = 0.5663, roc = 0.6242, prc = 0.4992\n",
      "Fold 8, Epoch 53, acc = 0.4699, roc = 0.6453, prc = 0.5167\n",
      "Fold 8, Epoch 54, acc = 0.5181, roc = 0.5312, prc = 0.4131\n",
      "Fold 8, Epoch 55, acc = 0.4819, roc = 0.4346, prc = 0.3518\n",
      "Fold 8, Epoch 56, acc = 0.4217, roc = 0.4316, prc = 0.3940\n",
      "Fold 8, Epoch 57, acc = 0.5783, roc = 0.5300, prc = 0.4738\n",
      "Fold 8, Epoch 58, acc = 0.6145, roc = 0.4202, prc = 0.4355\n",
      "Fold 8, Epoch 59, acc = 0.4819, roc = 0.5120, prc = 0.4052\n",
      "Fold 8, Epoch 60, acc = 0.6145, roc = 0.5888, prc = 0.4480\n",
      "Fold 8, Epoch 61, acc = 0.5542, roc = 0.7107, prc = 0.6972\n",
      "Fold 8, Epoch 62, acc = 0.5181, roc = 0.5582, prc = 0.5069\n",
      "Fold 8, Epoch 63, acc = 0.5181, roc = 0.5678, prc = 0.4721\n",
      "Fold 8, Epoch 64, acc = 0.5422, roc = 0.6200, prc = 0.5762\n",
      "Fold 8, Epoch 65, acc = 0.4699, roc = 0.5048, prc = 0.4866\n",
      "Fold 8, Epoch 66, acc = 0.6265, roc = 0.3121, prc = 0.4301\n",
      "Fold 8, Epoch 67, acc = 0.6145, roc = 0.6345, prc = 0.4494\n",
      "Fold 8, Epoch 68, acc = 0.5663, roc = 0.6267, prc = 0.4407\n",
      "Fold 8, Epoch 69, acc = 0.5422, roc = 0.5816, prc = 0.4131\n",
      "Fold 8, Epoch 70, acc = 0.4819, roc = 0.5624, prc = 0.4061\n",
      "Fold 8, Epoch 71, acc = 0.4458, roc = 0.4100, prc = 0.3864\n",
      "Fold 8, Epoch 72, acc = 0.5301, roc = 0.4934, prc = 0.4322\n",
      "Fold 8, Epoch 73, acc = 0.5301, roc = 0.5288, prc = 0.4440\n",
      "Fold 8, Epoch 74, acc = 0.4819, roc = 0.5114, prc = 0.4122\n",
      "Fold 8, Epoch 75, acc = 0.3735, roc = 0.3830, prc = 0.4036\n",
      "Fold 8, Epoch 76, acc = 0.5181, roc = 0.4760, prc = 0.4212\n",
      "Fold 8, Epoch 77, acc = 0.4699, roc = 0.4904, prc = 0.4542\n",
      "Fold 8, Epoch 78, acc = 0.5301, roc = 0.5006, prc = 0.4474\n",
      "Fold 8, Epoch 79, acc = 0.5181, roc = 0.5162, prc = 0.4517\n",
      "Fold 8, Epoch 80, acc = 0.5301, roc = 0.5084, prc = 0.4646\n",
      "Fold 8, Epoch 81, acc = 0.4819, roc = 0.4694, prc = 0.3612\n",
      "Fold 8, Epoch 82, acc = 0.6506, roc = 0.4544, prc = 0.4617\n",
      "Fold 8, Epoch 83, acc = 0.5181, roc = 0.5324, prc = 0.4230\n",
      "Fold 8, Epoch 84, acc = 0.4819, roc = 0.5078, prc = 0.4340\n",
      "Fold 8, Epoch 85, acc = 0.4819, roc = 0.5066, prc = 0.4480\n",
      "Fold 8, Epoch 86, acc = 0.5060, roc = 0.5084, prc = 0.4458\n",
      "Fold 8, Epoch 87, acc = 0.5783, roc = 0.5684, prc = 0.5099\n",
      "Fold 8, Epoch 88, acc = 0.4940, roc = 0.5738, prc = 0.4999\n",
      "Fold 8, Epoch 89, acc = 0.5542, roc = 0.5606, prc = 0.4900\n",
      "Fold 8, Epoch 90, acc = 0.5181, roc = 0.5534, prc = 0.4838\n",
      "Fold 8, Epoch 91, acc = 0.5181, roc = 0.5456, prc = 0.4688\n",
      "Fold 8, Epoch 92, acc = 0.5181, roc = 0.5546, prc = 0.4521\n",
      "Fold 8, Epoch 93, acc = 0.5301, roc = 0.5432, prc = 0.4642\n",
      "Fold 8, Epoch 94, acc = 0.5181, roc = 0.5288, prc = 0.4517\n",
      "Fold 8, Epoch 95, acc = 0.5181, roc = 0.5420, prc = 0.4732\n",
      "Fold 8, Epoch 96, acc = 0.5060, roc = 0.5282, prc = 0.4419\n",
      "Fold 8, Epoch 97, acc = 0.5542, roc = 0.5336, prc = 0.4515\n",
      "Fold 8, Epoch 98, acc = 0.4699, roc = 0.5108, prc = 0.4357\n",
      "Fold 8, Epoch 99, acc = 0.6145, roc = 0.4784, prc = 0.4945\n",
      "------------ Save best model - ROC: 0.8013 ------------\n",
      "Fold 9, Epoch 0, acc = 0.6265, roc = 0.8013, prc = 0.7260\n",
      "Fold 9, Epoch 1, acc = 0.5663, roc = 0.5702, prc = 0.4595\n",
      "------------ Save best model - ACC: 0.6506 ------------\n",
      "------------ Save best model - ROC: 0.8025 ------------\n",
      "Fold 9, Epoch 2, acc = 0.6506, roc = 0.8025, prc = 0.7729\n",
      "Fold 9, Epoch 3, acc = 0.6265, roc = 0.7275, prc = 0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 4, acc = 0.5904, roc = 0.6549, prc = 0.5416\n",
      "Fold 9, Epoch 5, acc = 0.5904, roc = 0.3031, prc = 0.3307\n",
      "Fold 9, Epoch 6, acc = 0.5542, roc = 0.3313, prc = 0.3158\n",
      "Fold 9, Epoch 7, acc = 0.6024, roc = 0.5216, prc = 0.4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 8, acc = 0.5904, roc = 0.5330, prc = 0.4209\n",
      "Fold 9, Epoch 9, acc = 0.6024, roc = 0.6327, prc = 0.5033\n",
      "Fold 9, Epoch 10, acc = 0.5904, roc = 0.6459, prc = 0.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 11, acc = 0.5904, roc = 0.6387, prc = 0.5190\n",
      "Fold 9, Epoch 12, acc = 0.6024, roc = 0.5738, prc = 0.4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 13, acc = 0.5783, roc = 0.6128, prc = 0.4798\n",
      "Fold 9, Epoch 14, acc = 0.6024, roc = 0.4604, prc = 0.4200\n",
      "Fold 9, Epoch 15, acc = 0.6145, roc = 0.7587, prc = 0.6914\n",
      "Fold 9, Epoch 16, acc = 0.6145, roc = 0.6711, prc = 0.5783\n",
      "Fold 9, Epoch 17, acc = 0.6265, roc = 0.6945, prc = 0.6092\n",
      "Fold 9, Epoch 18, acc = 0.5904, roc = 0.5210, prc = 0.4542\n",
      "Fold 9, Epoch 19, acc = 0.6145, roc = 0.4820, prc = 0.4810\n",
      "Fold 9, Epoch 20, acc = 0.5422, roc = 0.4934, prc = 0.4469\n",
      "Fold 9, Epoch 21, acc = 0.5904, roc = 0.5342, prc = 0.4613\n",
      "Fold 9, Epoch 22, acc = 0.5904, roc = 0.4598, prc = 0.3978\n",
      "Fold 9, Epoch 23, acc = 0.5663, roc = 0.5228, prc = 0.4430\n",
      "Fold 9, Epoch 24, acc = 0.5904, roc = 0.5930, prc = 0.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 25, acc = 0.5663, roc = 0.4028, prc = 0.3563\n",
      "Fold 9, Epoch 26, acc = 0.5904, roc = 0.5090, prc = 0.4168\n",
      "Fold 9, Epoch 27, acc = 0.5904, roc = 0.5612, prc = 0.4651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 28, acc = 0.5663, roc = 0.3926, prc = 0.3358\n",
      "Fold 9, Epoch 29, acc = 0.5422, roc = 0.5582, prc = 0.4325\n",
      "Fold 9, Epoch 30, acc = 0.5542, roc = 0.5528, prc = 0.4211\n",
      "Fold 9, Epoch 31, acc = 0.5422, roc = 0.4430, prc = 0.3587\n",
      "Fold 9, Epoch 32, acc = 0.4940, roc = 0.5246, prc = 0.4116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 33, acc = 0.5663, roc = 0.5012, prc = 0.3981\n",
      "Fold 9, Epoch 34, acc = 0.4940, roc = 0.4760, prc = 0.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 35, acc = 0.5663, roc = 0.4232, prc = 0.3587\n",
      "Fold 9, Epoch 36, acc = 0.6145, roc = 0.5906, prc = 0.4775\n",
      "Fold 9, Epoch 37, acc = 0.5422, roc = 0.6242, prc = 0.4666\n",
      "Fold 9, Epoch 38, acc = 0.5904, roc = 0.6267, prc = 0.4824\n",
      "Fold 9, Epoch 39, acc = 0.6145, roc = 0.5210, prc = 0.4280\n",
      "Fold 9, Epoch 40, acc = 0.5422, roc = 0.5252, prc = 0.4170\n",
      "Fold 9, Epoch 41, acc = 0.4819, roc = 0.5330, prc = 0.4033\n",
      "Fold 9, Epoch 42, acc = 0.4578, roc = 0.4742, prc = 0.3730\n",
      "Fold 9, Epoch 43, acc = 0.4578, roc = 0.4748, prc = 0.4275\n",
      "Fold 9, Epoch 44, acc = 0.4096, roc = 0.6267, prc = 0.4768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 45, acc = 0.5904, roc = 0.4580, prc = 0.4285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, Epoch 46, acc = 0.5783, roc = 0.4742, prc = 0.4102\n",
      "Fold 9, Epoch 47, acc = 0.5904, roc = 0.4814, prc = 0.4619\n",
      "Fold 9, Epoch 48, acc = 0.6145, roc = 0.5678, prc = 0.4777\n",
      "------------ Save best model - ACC: 0.6747 ------------\n",
      "Fold 9, Epoch 49, acc = 0.6747, roc = 0.6236, prc = 0.5737\n",
      "Fold 9, Epoch 50, acc = 0.6506, roc = 0.6975, prc = 0.5597\n",
      "Fold 9, Epoch 51, acc = 0.6024, roc = 0.5588, prc = 0.4625\n",
      "------------ Save best model - ACC: 0.6988 ------------\n",
      "Fold 9, Epoch 52, acc = 0.6988, roc = 0.6939, prc = 0.6427\n",
      "Fold 9, Epoch 53, acc = 0.6386, roc = 0.6381, prc = 0.5757\n",
      "Fold 9, Epoch 54, acc = 0.6627, roc = 0.6609, prc = 0.5908\n",
      "------------ Save best model - ACC: 0.7229 ------------\n",
      "Fold 9, Epoch 55, acc = 0.7229, roc = 0.6855, prc = 0.6284\n",
      "Fold 9, Epoch 56, acc = 0.6506, roc = 0.6717, prc = 0.5881\n",
      "Fold 9, Epoch 57, acc = 0.6265, roc = 0.6429, prc = 0.5459\n",
      "Fold 9, Epoch 58, acc = 0.6024, roc = 0.6369, prc = 0.5392\n",
      "Fold 9, Epoch 59, acc = 0.6747, roc = 0.6477, prc = 0.5731\n",
      "Fold 9, Epoch 60, acc = 0.6506, roc = 0.6513, prc = 0.5617\n",
      "Fold 9, Epoch 61, acc = 0.6386, roc = 0.6026, prc = 0.5256\n",
      "Fold 9, Epoch 62, acc = 0.5542, roc = 0.6170, prc = 0.5085\n",
      "Fold 9, Epoch 63, acc = 0.6265, roc = 0.6255, prc = 0.5023\n",
      "Fold 9, Epoch 64, acc = 0.6145, roc = 0.6759, prc = 0.5537\n",
      "Fold 9, Epoch 65, acc = 0.6627, roc = 0.7119, prc = 0.5598\n",
      "Fold 9, Epoch 66, acc = 0.5663, roc = 0.6375, prc = 0.4966\n",
      "Fold 9, Epoch 67, acc = 0.5904, roc = 0.6285, prc = 0.4831\n",
      "Fold 9, Epoch 68, acc = 0.5663, roc = 0.5846, prc = 0.4448\n",
      "Fold 9, Epoch 69, acc = 0.5542, roc = 0.6038, prc = 0.4456\n",
      "Fold 9, Epoch 70, acc = 0.5783, roc = 0.5894, prc = 0.4612\n",
      "Fold 9, Epoch 71, acc = 0.5542, roc = 0.6164, prc = 0.4903\n",
      "Fold 9, Epoch 72, acc = 0.5422, roc = 0.5690, prc = 0.4705\n",
      "Fold 9, Epoch 73, acc = 0.6145, roc = 0.6681, prc = 0.5334\n",
      "Fold 9, Epoch 74, acc = 0.5783, roc = 0.6945, prc = 0.6295\n",
      "Fold 9, Epoch 75, acc = 0.4940, roc = 0.5720, prc = 0.4902\n",
      "Fold 9, Epoch 76, acc = 0.5301, roc = 0.5264, prc = 0.4764\n",
      "Fold 9, Epoch 77, acc = 0.5181, roc = 0.5876, prc = 0.5095\n",
      "Fold 9, Epoch 78, acc = 0.5663, roc = 0.5804, prc = 0.5084\n",
      "Fold 9, Epoch 79, acc = 0.5663, roc = 0.5522, prc = 0.5002\n",
      "Fold 9, Epoch 80, acc = 0.5301, roc = 0.5678, prc = 0.4749\n",
      "Fold 9, Epoch 81, acc = 0.5422, roc = 0.6164, prc = 0.5330\n",
      "Fold 9, Epoch 82, acc = 0.5301, roc = 0.5660, prc = 0.5033\n",
      "Fold 9, Epoch 83, acc = 0.5301, roc = 0.5636, prc = 0.4755\n",
      "Fold 9, Epoch 84, acc = 0.5783, roc = 0.5912, prc = 0.5184\n",
      "Fold 9, Epoch 85, acc = 0.6386, roc = 0.6381, prc = 0.5703\n",
      "Fold 9, Epoch 86, acc = 0.5904, roc = 0.6224, prc = 0.5637\n",
      "Fold 9, Epoch 87, acc = 0.5542, roc = 0.6026, prc = 0.5552\n",
      "Fold 9, Epoch 88, acc = 0.4578, roc = 0.4250, prc = 0.3436\n",
      "Fold 9, Epoch 89, acc = 0.5422, roc = 0.5744, prc = 0.4888\n",
      "Fold 9, Epoch 90, acc = 0.5301, roc = 0.5720, prc = 0.4764\n",
      "Fold 9, Epoch 91, acc = 0.5904, roc = 0.5720, prc = 0.4931\n",
      "Fold 9, Epoch 92, acc = 0.5783, roc = 0.5318, prc = 0.4533\n",
      "Fold 9, Epoch 93, acc = 0.5904, roc = 0.5858, prc = 0.5156\n",
      "Fold 9, Epoch 94, acc = 0.5663, roc = 0.5588, prc = 0.4621\n",
      "Fold 9, Epoch 95, acc = 0.5663, roc = 0.5546, prc = 0.4568\n",
      "Fold 9, Epoch 96, acc = 0.5783, roc = 0.5858, prc = 0.5075\n",
      "Fold 9, Epoch 97, acc = 0.5301, roc = 0.5450, prc = 0.4787\n",
      "Fold 9, Epoch 98, acc = 0.5783, roc = 0.5528, prc = 0.4713\n",
      "Fold 9, Epoch 99, acc = 0.5060, roc = 0.5108, prc = 0.4376\n",
      "------------ Save best model - ROC: 0.3313 ------------\n",
      "Fold 10, Epoch 0, acc = 0.6024, roc = 0.3313, prc = 0.3562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 1, acc = 0.5904, roc = 0.3247, prc = 0.3778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ROC: 0.3319 ------------\n",
      "Fold 10, Epoch 2, acc = 0.5904, roc = 0.3319, prc = 0.3820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 3, acc = 0.5904, roc = 0.2815, prc = 0.3592\n",
      "Fold 10, Epoch 4, acc = 0.6024, roc = 0.3025, prc = 0.3815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 5, acc = 0.5904, roc = 0.2497, prc = 0.3396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 6, acc = 0.5904, roc = 0.2473, prc = 0.3691\n",
      "------------ Save best model - ACC: 0.6386 ------------\n",
      "Fold 10, Epoch 7, acc = 0.6386, roc = 0.1951, prc = 0.3881\n",
      "Fold 10, Epoch 8, acc = 0.6024, roc = 0.2179, prc = 0.3600\n",
      "Fold 10, Epoch 9, acc = 0.6386, roc = 0.2803, prc = 0.4024\n",
      "------------ Save best model - ROC: 0.3896 ------------\n",
      "Fold 10, Epoch 10, acc = 0.6145, roc = 0.3896, prc = 0.4283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ROC: 0.4124 ------------\n",
      "Fold 10, Epoch 11, acc = 0.5904, roc = 0.4124, prc = 0.4408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 12, acc = 0.5904, roc = 0.2821, prc = 0.3920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 13, acc = 0.5904, roc = 0.3517, prc = 0.3808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 14, acc = 0.5904, roc = 0.3643, prc = 0.3908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 15, acc = 0.5904, roc = 0.2929, prc = 0.3526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 16, acc = 0.5904, roc = 0.3505, prc = 0.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 17, acc = 0.5904, roc = 0.4040, prc = 0.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ROC: 0.5480 ------------\n",
      "Fold 10, Epoch 18, acc = 0.5904, roc = 0.5480, prc = 0.4424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 19, acc = 0.5904, roc = 0.4856, prc = 0.4641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 20, acc = 0.5904, roc = 0.3884, prc = 0.3579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 21, acc = 0.5904, roc = 0.2215, prc = 0.3016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 22, acc = 0.5904, roc = 0.3151, prc = 0.3361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 23, acc = 0.5904, roc = 0.3175, prc = 0.3378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 24, acc = 0.5904, roc = 0.2875, prc = 0.3432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 25, acc = 0.5904, roc = 0.3397, prc = 0.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 26, acc = 0.5904, roc = 0.3914, prc = 0.3358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 27, acc = 0.5542, roc = 0.3181, prc = 0.3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 28, acc = 0.5904, roc = 0.2989, prc = 0.3381\n",
      "Fold 10, Epoch 29, acc = 0.6024, roc = 0.3709, prc = 0.3748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 30, acc = 0.5904, roc = 0.2905, prc = 0.3450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 31, acc = 0.5904, roc = 0.3631, prc = 0.3547\n",
      "Fold 10, Epoch 32, acc = 0.5783, roc = 0.3193, prc = 0.3455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 33, acc = 0.5904, roc = 0.3217, prc = 0.3167\n",
      "Fold 10, Epoch 34, acc = 0.3614, roc = 0.2947, prc = 0.3063\n",
      "Fold 10, Epoch 35, acc = 0.5904, roc = 0.2545, prc = 0.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 36, acc = 0.5904, roc = 0.3487, prc = 0.3399\n",
      "Fold 10, Epoch 37, acc = 0.4458, roc = 0.2503, prc = 0.2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 38, acc = 0.5783, roc = 0.2917, prc = 0.3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 39, acc = 0.5783, roc = 0.3175, prc = 0.3192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 40, acc = 0.5783, roc = 0.4292, prc = 0.3531\n",
      "Fold 10, Epoch 41, acc = 0.5904, roc = 0.4298, prc = 0.3685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 42, acc = 0.5783, roc = 0.4478, prc = 0.3716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 43, acc = 0.5783, roc = 0.4838, prc = 0.4331\n",
      "Fold 10, Epoch 44, acc = 0.3253, roc = 0.3289, prc = 0.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 45, acc = 0.5542, roc = 0.3703, prc = 0.3449\n",
      "Fold 10, Epoch 46, acc = 0.5904, roc = 0.3739, prc = 0.3380\n",
      "Fold 10, Epoch 47, acc = 0.6145, roc = 0.3031, prc = 0.3532\n",
      "Fold 10, Epoch 48, acc = 0.5663, roc = 0.2401, prc = 0.2856\n",
      "Fold 10, Epoch 49, acc = 0.6265, roc = 0.4760, prc = 0.4361\n",
      "Fold 10, Epoch 50, acc = 0.5783, roc = 0.2509, prc = 0.3433\n",
      "Fold 10, Epoch 51, acc = 0.6024, roc = 0.3938, prc = 0.3824\n",
      "Fold 10, Epoch 52, acc = 0.6265, roc = 0.2263, prc = 0.3320\n",
      "Fold 10, Epoch 53, acc = 0.3855, roc = 0.1441, prc = 0.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 54, acc = 0.5783, roc = 0.2653, prc = 0.3068\n",
      "Fold 10, Epoch 55, acc = 0.5904, roc = 0.2239, prc = 0.2879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 56, acc = 0.5904, roc = 0.2317, prc = 0.2842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 57, acc = 0.5542, roc = 0.1717, prc = 0.2666\n",
      "Fold 10, Epoch 58, acc = 0.4458, roc = 0.1723, prc = 0.2646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 59, acc = 0.5904, roc = 0.2341, prc = 0.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 60, acc = 0.5783, roc = 0.1447, prc = 0.2637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 61, acc = 0.5904, roc = 0.2005, prc = 0.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 62, acc = 0.5904, roc = 0.1747, prc = 0.2932\n",
      "Fold 10, Epoch 63, acc = 0.5663, roc = 0.1753, prc = 0.2734\n",
      "Fold 10, Epoch 64, acc = 0.5783, roc = 0.1771, prc = 0.2725\n",
      "Fold 10, Epoch 65, acc = 0.5663, roc = 0.2749, prc = 0.3064\n",
      "Fold 10, Epoch 66, acc = 0.5301, roc = 0.4292, prc = 0.3973\n",
      "Fold 10, Epoch 67, acc = 0.6265, roc = 0.4046, prc = 0.4034\n",
      "Fold 10, Epoch 68, acc = 0.6265, roc = 0.3589, prc = 0.3707\n",
      "Fold 10, Epoch 69, acc = 0.4940, roc = 0.3415, prc = 0.3784\n",
      "Fold 10, Epoch 70, acc = 0.6145, roc = 0.4736, prc = 0.4455\n",
      "------------ Save best model - ROC: 0.5750 ------------\n",
      "Fold 10, Epoch 71, acc = 0.6145, roc = 0.5750, prc = 0.5007\n",
      "Fold 10, Epoch 72, acc = 0.5060, roc = 0.4244, prc = 0.4053\n",
      "Fold 10, Epoch 73, acc = 0.5301, roc = 0.2965, prc = 0.3016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/EMR2/EMR/1Jiangsu/human_stat/utils/metrics.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_score=2*prec1*rec1/(prec1+rec1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10, Epoch 74, acc = 0.5783, roc = 0.3127, prc = 0.3200\n",
      "Fold 10, Epoch 75, acc = 0.6386, roc = 0.4604, prc = 0.4380\n",
      "Fold 10, Epoch 76, acc = 0.6386, roc = 0.3770, prc = 0.4229\n",
      "Fold 10, Epoch 77, acc = 0.6386, roc = 0.4790, prc = 0.4672\n",
      "Fold 10, Epoch 78, acc = 0.5542, roc = 0.4700, prc = 0.4605\n",
      "Fold 10, Epoch 79, acc = 0.6024, roc = 0.4358, prc = 0.4453\n",
      "Fold 10, Epoch 80, acc = 0.5663, roc = 0.3884, prc = 0.3610\n",
      "Fold 10, Epoch 81, acc = 0.5663, roc = 0.5654, prc = 0.5729\n",
      "------------ Save best model - ROC: 0.5792 ------------\n",
      "Fold 10, Epoch 82, acc = 0.5904, roc = 0.5792, prc = 0.5600\n",
      "------------ Save best model - ACC: 0.6747 ------------\n",
      "------------ Save best model - ROC: 0.6549 ------------\n",
      "Fold 10, Epoch 83, acc = 0.6747, roc = 0.6549, prc = 0.5956\n",
      "Fold 10, Epoch 84, acc = 0.5663, roc = 0.5432, prc = 0.4942\n",
      "Fold 10, Epoch 85, acc = 0.6265, roc = 0.5084, prc = 0.4788\n",
      "Fold 10, Epoch 86, acc = 0.5904, roc = 0.5600, prc = 0.5187\n",
      "Fold 10, Epoch 87, acc = 0.6265, roc = 0.5630, prc = 0.5126\n",
      "Fold 10, Epoch 88, acc = 0.6024, roc = 0.5726, prc = 0.5408\n",
      "Fold 10, Epoch 89, acc = 0.4940, roc = 0.5240, prc = 0.5002\n",
      "Fold 10, Epoch 90, acc = 0.5542, roc = 0.4682, prc = 0.4831\n",
      "Fold 10, Epoch 91, acc = 0.6145, roc = 0.4808, prc = 0.4759\n",
      "Fold 10, Epoch 92, acc = 0.6145, roc = 0.4754, prc = 0.4503\n",
      "Fold 10, Epoch 93, acc = 0.5422, roc = 0.4922, prc = 0.4600\n",
      "Fold 10, Epoch 94, acc = 0.5783, roc = 0.5558, prc = 0.5155\n",
      "Fold 10, Epoch 95, acc = 0.5663, roc = 0.5432, prc = 0.5127\n",
      "Fold 10, Epoch 96, acc = 0.5663, roc = 0.5846, prc = 0.5410\n",
      "Fold 10, Epoch 97, acc = 0.6145, roc = 0.6224, prc = 0.5530\n",
      "Fold 10, Epoch 98, acc = 0.5904, roc = 0.6026, prc = 0.5503\n",
      "Fold 10, Epoch 99, acc = 0.6145, roc = 0.5984, prc = 0.5283\n",
      "acc 0.7740(0.0892)\n",
      "auroc 0.8012(0.1072)\n",
      "auprc 0.7637(0.1361)\n",
      "minpse 0.7161(0.1170)\n",
      "==========================================================\n",
      "_acc 0.6706(0.1625)\n",
      "_auroc 0.8521(0.0858)\n",
      "_auprc 0.8354(0.1007)\n",
      "_minpse 0.7504(0.1074)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "global_best = 0\n",
    "_global_best = 0\n",
    "\n",
    "\n",
    "acc = []\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "\n",
    "\n",
    "_acc = []\n",
    "_auroc = []\n",
    "_auprc = []\n",
    "_minpse = []\n",
    "\n",
    "history = []\n",
    "pad_token = np.zeros(853)\n",
    "pad_token2 = np.zeros(28)\n",
    "    # pad_token = np.zeros(17)\n",
    "    # begin_time = time.time()\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "_y_true_all = []\n",
    "_y_pred_all = []\n",
    "for train, test in kfold.split(new_X, new_Y):\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "    \n",
    "    _best_y_true = None\n",
    "    _best_y_pred = None\n",
    "        \n",
    "    model = M3Care(input_dim = new_X.shape[-1], hidden_dim = 256, embed_size = 256, output_dim = 1, keep_prob=0.5).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay = 1e-5)\n",
    "\n",
    "    fold_count += 1\n",
    "#     if fold_count !=3:\n",
    "#         continue\n",
    "# #     print(train)\n",
    "\n",
    "    train_x = [new_X[i] for i in train ]\n",
    "    train_x_pd = [cls_procceed_dp[i] for i in train ]\n",
    "    train_y = [new_Y[i] for i in train ]\n",
    "    train_x_med = [x_med[i] for i in train ]\n",
    "    train_x_med_len = [x_med_len[i] for i in train ]\n",
    "    \n",
    "    train_x_sili = [x_silicon[i] for i in train ]\n",
    "    train_x_sili_len = [x_silicon_len[i] for i in train ]\n",
    "    \n",
    "    train_x_in = [cls_procceed_in[i] for i in train ]\n",
    "    train_x_out = [cls_procceed_out[i] for i in train ]\n",
    "    \n",
    "    train_x_dp_maks = [dp_mask[i] for i in train ]\n",
    "    train_x_in_mask = [in_mask[i] for i in train ]\n",
    "    train_x_out_mask = [out_mask[i] for i in train ]\n",
    "    train_x_med_mask = [med_mask[i] for i in train ]\n",
    "    train_x_sili_mask = [sili_mask[i] for i in train ]\n",
    "\n",
    "\n",
    "#     train_x_len = [all_x_len[i] for i in train ]\n",
    "\n",
    "\n",
    "    test_x = [new_X[i] for i in test ]\n",
    "    test_x_pd = [cls_procceed_dp[i] for i in test ]\n",
    "\n",
    "    test_y = [new_Y[i] for i in test ]\n",
    "    test_x_med = [x_med[i] for i in test ]\n",
    "    test_x_med_len = [x_med_len[i] for i in test ]\n",
    "    \n",
    "    test_x_sili = [x_silicon[i] for i in test ]\n",
    "    test_x_sili_len = [x_silicon_len[i] for i in test ]\n",
    "    \n",
    "    test_x_in = [cls_procceed_in[i] for i in test ]\n",
    "    test_x_out = [cls_procceed_out[i] for i in test ]\n",
    "    \n",
    "    test_x_dp_maks = [dp_mask[i] for i in test ]\n",
    "    test_x_in_mask = [in_mask[i] for i in test ]\n",
    "    test_x_out_mask = [out_mask[i] for i in test ]\n",
    "    test_x_med_mask = [med_mask[i] for i in test ]\n",
    "    test_x_sili_mask = [sili_mask[i] for i in test ]\n",
    "#     test_x_len = [all_x_len[i] for i in test ]\n",
    "\n",
    "    file_name = './model/fix-test-new-M30Care-200' \n",
    "\n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_acc = 0\n",
    "    best_auroc = 0\n",
    "    best_auprc = 0\n",
    "    best_minpse = 0\n",
    "    \n",
    "    \n",
    "    _best_acc = 0\n",
    "    _best_auroc = 0\n",
    "    _best_auprc = 0\n",
    "    _best_minpse = 0\n",
    "\n",
    "    for each_epoch in range(epochs):\n",
    "\n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "\n",
    "        for step, (batch_x, batch_y, batch_dp, batch_in_hos, batch_out_hos, \n",
    "                   batch_med, batch_med_lens, batch_sili, batch_sili_lens, d_i_o_m_s_masks) in enumerate(\n",
    "                batch_iter(train_x, train_y, train_x_pd,train_x_in,train_x_out,\n",
    "                           train_x_med, train_x_med_len, train_x_sili, train_x_sili_len, \\\n",
    "                           train_x_dp_maks, train_x_in_mask, train_x_out_mask, train_x_med_mask, \\\n",
    "                           train_x_sili_mask, batch_size, shuffle=True)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_x = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            \n",
    "            batch_med = torch.tensor(pad_sents(batch_med, pad_token), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_med_lens = torch.tensor(batch_med_lens, dtype=torch.float32).to(device).int()\n",
    "    \n",
    "            batch_sili = torch.tensor(pad_sents(batch_sili, pad_token2), dtype=torch.float32).to(device)\n",
    "#             batch_sili = torch.tensor(batch_sili, dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_sili_lens = torch.tensor(batch_sili_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "\n",
    "\n",
    "            opt,sum_of_diff = model(batch_x, batch_dp, batch_in_hos, batch_out_hos, batch_med, batch_med_lens, \\\n",
    "                        batch_sili, batch_sili_lens, d_i_o_m_s_masks)\n",
    "#             print(simi)\n",
    "\n",
    "#             weight = torch.zeros_like(opt).to(device)\n",
    "#             #print(opt.shape)\n",
    "#             for i in range(opt.shape[0]):\n",
    "#                 if  batch_y.unsqueeze(-1)[i,0] == torch.zeros(1).to(device):\n",
    "#                     weight[i,0] = 1\n",
    "#                 else:\n",
    "#                     weight[i,0] = 3\n",
    "           # print(weight)\n",
    "#             print(\"batch_y.unsqueeze(-1)\")\n",
    "#             print(batch_y.unsqueeze(-1))\n",
    "#             print(\"opt\")\n",
    "#             print(opt)\n",
    "            BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             print(\"BCE_Loss\")\n",
    "#             print(BCE_Loss)\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "#             print(BCE_Loss)\n",
    "            model_loss =  BCE_Loss + sum_of_diff*1e-8\n",
    "#     + 0.001*simi.sum()/batch_size\n",
    "#             print(BCE_Loss)\n",
    "#             print(simi.sum()/batch_size)\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "\n",
    "#             if step % 50 == 0:\n",
    "#                 print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "#             print(test_x_pd)\n",
    "            for batch_x, batch_y, batch_dp, batch_in_hos, batch_out_hos, batch_med, batch_med_lens, \\\n",
    "    batch_sili, batch_sili_lens, d_i_o_m_s_masks in  \\\n",
    "                batch_iter(test_x, test_y, test_x_pd, test_x_in,test_x_out, test_x_med, test_x_med_len, \n",
    "                   test_x_sili, test_x_sili_len, test_x_dp_maks, test_x_in_mask, test_x_out_mask, \\\n",
    "                           test_x_med_mask, test_x_sili_mask, batch_size):\n",
    "\n",
    "                batch_x = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "\n",
    "                batch_med = torch.tensor(pad_sents(batch_med, pad_token), dtype=torch.float32).to(device)\n",
    "    #             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_med_lens = torch.tensor(batch_med_lens, dtype=torch.float32).to(device).int()\n",
    "        \n",
    "                batch_sili = torch.tensor(pad_sents(batch_sili, pad_token2), dtype=torch.float32).to(device)\n",
    "#                 batch_sili = torch.tensor(batch_sili, dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_sili_lens = torch.tensor(batch_sili_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                opt,_ = model(batch_x, batch_dp, batch_in_hos, batch_out_hos,batch_med, batch_med_lens, \\\n",
    "                            batch_sili, batch_sili_lens, d_i_o_m_s_masks)\n",
    "\n",
    "\n",
    "                BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#                 print(\"BCE_Loss\")\n",
    "#                 print(BCE_Loss)\n",
    "#                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "                valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "                y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "                y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "\n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "#             y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "#             y_true += list(batch_y.cpu().numpy().flatten())\n",
    "#             y_true_all.append(y_true)\n",
    "#             y_pred_all.append(y_pred)\n",
    "            ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "            history.append(ret)\n",
    "            #print()\n",
    "\n",
    "#             if each_epoch % 10 == 0:\n",
    "#                 print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f acc = %.4f roc = %.4f'%(fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['acc'], ret['auroc']))\n",
    "#                 metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "            cur_acc = ret['acc']\n",
    "            if cur_acc > best_acc:\n",
    "                best_y_true = y_true\n",
    "                best_y_pred = y_pred\n",
    "                \n",
    "                best_acc = cur_acc\n",
    "                best_auroc = ret['auroc']\n",
    "                best_auprc = ret['auprc']\n",
    "                best_minpse = ret['minpse']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, file_name+'_'+str(fold_count))\n",
    "\n",
    "                if cur_acc > global_best:\n",
    "                    global_best = cur_acc\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, file_name)\n",
    "                    print('------------ Save best model - ACC: %.4f ------------'%cur_acc)\n",
    "                    \n",
    "            _cur_roc = ret['auroc']\n",
    "            if _cur_roc > _best_auroc:\n",
    "                _best_y_true = y_true\n",
    "                _best_y_pred = y_pred\n",
    "                \n",
    "                _best_acc = ret['acc']\n",
    "                _best_auroc = _cur_roc\n",
    "                _best_auprc = ret['auprc']\n",
    "                _best_minpse = ret['minpse']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, file_name+'_roc_'+str(fold_count))\n",
    "\n",
    "                if _cur_roc > _global_best:\n",
    "                    global_best = cur_acc\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, file_name)\n",
    "                    print('------------ Save best model - ROC: %.4f ------------'%_cur_roc)\n",
    "\n",
    "        print('Fold %d, Epoch %d, acc = %.4f, roc = %.4f, prc = %.4f'%(fold_count, each_epoch, ret['acc'], ret['auroc'], ret['auprc']))\n",
    "\n",
    "    acc.append(best_acc)\n",
    "    auroc.append(best_auroc)\n",
    "    auprc.append(best_auprc)\n",
    "    minpse.append(best_minpse)\n",
    "    \n",
    "    _acc.append(_best_acc)\n",
    "    _auroc.append(_best_auroc)\n",
    "    _auprc.append(_best_auprc)\n",
    "    _minpse.append(_best_minpse)\n",
    "    \n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "    \n",
    "    y_true_all.append(best_y_true)\n",
    "    y_pred_all.append(best_y_pred)\n",
    "    \n",
    "    _y_true_all.append(_best_y_true)\n",
    "    _y_pred_all.append(_best_y_pred)\n",
    "\n",
    "print('acc %.4f(%.4f)'%(np.mean(acc), np.std(acc)))\n",
    "print('auroc %.4f(%.4f)'%(np.mean(auroc), np.std(auroc)))\n",
    "print('auprc %.4f(%.4f)'%(np.mean(auprc), np.std(auprc)))\n",
    "print('minpse %.4f(%.4f)'%(np.mean(minpse), np.std(minpse)))  \n",
    "    \n",
    "print(\"==========================================================\")\n",
    "\n",
    "print('_acc %.4f(%.4f)'%(np.mean(_acc), np.std(_acc)))\n",
    "print('_auroc %.4f(%.4f)'%(np.mean(_auroc), np.std(_auroc)))\n",
    "print('_auprc %.4f(%.4f)'%(np.mean(_auprc), np.std(_auprc)))\n",
    "print('_minpse %.4f(%.4f)'%(np.mean(_minpse), np.std(_minpse)))  \n",
    "#     return np.mean(acc), np.std(acc), np.mean(auroc), np.std(auroc),np.mean(auprc), np.std(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7740(0.0892)\n",
      "auroc 0.8012(0.1072)\n",
      "auprc 0.7637(0.1361)\n",
      "minpse 0.7161(0.1170)\n",
      "==========================================================\n",
      "_acc 0.6706(0.1625)\n",
      "_auroc 0.8521(0.0858)\n",
      "_auprc 0.8354(0.1007)\n",
      "_minpse 0.7504(0.1074)\n"
     ]
    }
   ],
   "source": [
    "print('acc %.4f(%.4f)'%(np.mean(acc), np.std(acc)))\n",
    "print('auroc %.4f(%.4f)'%(np.mean(auroc), np.std(auroc)))\n",
    "print('auprc %.4f(%.4f)'%(np.mean(auprc), np.std(auprc)))\n",
    "print('minpse %.4f(%.4f)'%(np.mean(minpse), np.std(minpse)))  \n",
    "    \n",
    "print(\"==========================================================\")\n",
    "\n",
    "print('_acc %.4f(%.4f)'%(np.mean(_acc), np.std(_acc)))\n",
    "print('_auroc %.4f(%.4f)'%(np.mean(_auroc), np.std(_auroc)))\n",
    "print('_auprc %.4f(%.4f)'%(np.mean(_auprc), np.std(_auprc)))\n",
    "print('_minpse %.4f(%.4f)'%(np.mean(_minpse), np.std(_minpse))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
