{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modality interpolation\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "# from utils.readers import InHospitalMortalityReader\n",
    "# from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pdid = pickle.load(open(\"pdid.pkl\", \"rb\"))\n",
    "\n",
    "left_fundus = pickle.load(open(\"left_fundus.pkl\", \"rb\"))\n",
    "right_fundus = pickle.load(open(\"right_fundus.pkl\", \"rb\"))\n",
    "\n",
    "left_fundus_images = pickle.load(open(\"left_fundus_images.pkl\", \"rb\"))\n",
    "right_fundus_images = pickle.load(open(\"right_fundus_images.pkl\", \"rb\"))\n",
    "\n",
    "left_diag = pickle.load(open(\"left_diag.pkl\", \"rb\"))\n",
    "right_diag = pickle.load(open(\"right_diag.pkl\", \"rb\"))\n",
    "\n",
    "X = pickle.load(open(\"X.pkl\", \"rb\"))\n",
    "Y = pickle.load(open(\"Y.pkl\", \"rb\"))\n",
    "\n",
    "feature = pickle.load(open(\"feature.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "left_diag_mask = pickle.load(open(\"left_diag_mask.pkl\", \"rb\"))\n",
    "right_diag_mask = pickle.load(open(\"right_diag_mask.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_left_diag = []\n",
    "for i in range(len(left_diag)):\n",
    "    cur_cur_dp = '[CLS] ' + left_diag[i]\n",
    "    cls_left_diag.append(cur_cur_dp)\n",
    "    \n",
    "cls_right_diag = []\n",
    "for i in range(len(right_diag)):\n",
    "    cur_cur_dp = '[CLS] ' + right_diag[i]\n",
    "    cls_right_diag.append(cur_cur_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_left_diag_li = []\n",
    "for i in cls_left_diag:\n",
    "        \n",
    "    cls_left_diag_li.append(i.split(' '))\n",
    "    \n",
    "cls_right_diag_li = []\n",
    "for i in cls_right_diag:\n",
    "\n",
    "        \n",
    "    cls_right_diag_li.append(i.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06142857 0.06057143 0.04685714]\n",
      "[0.06107143 0.0625     0.0475    ]\n",
      "[0.06571429 0.05428571 0.04857143]\n",
      "[0.06       0.05142857 0.04      ]\n"
     ]
    }
   ],
   "source": [
    "train_idx = pickle.load(open(\"train_idx\",'rb'))\n",
    "dev_idx = pickle.load(open(\"dev_idx\",'rb'))\n",
    "test_idx = pickle.load(open(\"test_idx\",'rb'))\n",
    "\n",
    "\n",
    "train_x = [X[i] for i in train_idx]\n",
    "train_left_fundus_images = [left_fundus_images[i] for i in train_idx]\n",
    "train_right_fundus_images = [right_fundus_images[i] for i in train_idx]\n",
    "\n",
    "train_left_diag = [cls_left_diag_li[i] for i in train_idx]\n",
    "train_right_diag = [cls_right_diag_li[i] for i in train_idx]\n",
    "train_left_diag_mask = [left_diag_mask[i] for i in train_idx]\n",
    "train_right_diag_mask = [right_diag_mask[i] for i in train_idx]\n",
    "\n",
    "train_y = [Y[i] for i in train_idx]\n",
    "\n",
    "\n",
    "dev_x = [X[i] for i in dev_idx]\n",
    "dev_left_fundus_images = [left_fundus_images[i] for i in dev_idx]\n",
    "dev_right_fundus_images = [right_fundus_images[i] for i in dev_idx]\n",
    "\n",
    "dev_left_diag = [cls_left_diag_li[i] for i in dev_idx]\n",
    "dev_right_diag = [cls_right_diag_li[i] for i in dev_idx]\n",
    "dev_left_diag_mask = [left_diag_mask[i] for i in dev_idx]\n",
    "dev_right_diag_mask = [right_diag_mask[i] for i in dev_idx]\n",
    "\n",
    "dev_y = [Y[i] for i in dev_idx]\n",
    "\n",
    "\n",
    "test_x = [X[i] for i in test_idx]\n",
    "test_left_fundus_images = [left_fundus_images[i] for i in test_idx]\n",
    "test_right_fundus_images = [right_fundus_images[i] for i in test_idx]\n",
    "\n",
    "test_left_diag = [cls_left_diag_li[i] for i in test_idx]\n",
    "test_right_diag = [cls_right_diag_li[i] for i in test_idx]\n",
    "test_left_diag_mask = [left_diag_mask[i] for i in test_idx]\n",
    "test_right_diag_mask = [right_diag_mask[i] for i in test_idx]\n",
    "\n",
    "\n",
    "test_y = [Y[i] for i in test_idx]\n",
    "\n",
    "\n",
    "print(sum(Y)/len(Y))\n",
    "print(sum(train_y)/len(train_y))\n",
    "# print(sum(big_test_Y)/len(big_test_Y))\n",
    "print(sum(dev_y)/len(dev_y))\n",
    "print(sum(test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "# device = torch.device('cuda')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "# from torchvision.io import read_image\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "# torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x, y, left_fundus, right_fundus, left_diag, left_diag_mask, right_diag, right_diag_mask, \\\n",
    "               batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size)  # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]  # fetch out all the induces\n",
    "\n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx], left_fundus[idx], right_fundus[idx], left_diag[idx], \\\n",
    "                             left_diag_mask[idx], right_diag[idx], right_diag_mask[idx]))\n",
    "\n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_left_fundus = [e[2] for e in examples]\n",
    "        batch_right_fundus = [e[3] for e in examples]\n",
    "        batch_left_diag = [e[4] for e in examples]\n",
    "        batch_left_diag_mask = [e[5] for e in examples]\n",
    "        batch_right_diag = [e[6] for e in examples]\n",
    "        batch_right_diag_mask = [e[7] for e in examples]\n",
    "    \n",
    "\n",
    "        yield batch_x, batch_y, batch_left_fundus, batch_right_fundus, batch_left_diag, \\\n",
    "        batch_right_diag,  [batch_left_diag_mask, batch_right_diag_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import math, copy, time\n",
    "from model_embeddings import ModelEmbeddings\n",
    "# Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "class NMT_tran(nn.Module):\n",
    "   \n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        \"\"\" Init NMT Model.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param hidden_size (int): Hidden Size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        @param dropout_rate (float): Dropout probability, for attention\n",
    "        \"\"\"\n",
    "        super(NMT_tran, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(hidden_size, vocab)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "      \n",
    "        c = copy.deepcopy\n",
    "        attn = MultiHeadedAttention(8, self.hidden_size)\n",
    "        ff = PositionwiseFeedForward(self.hidden_size, self.hidden_size*4, self.dropout_rate)\n",
    "        self.position = PositionalEncoding(embed_size, dropout_rate)\n",
    "        self.encoder = Encoder(EncoderLayer(hidden_size, c(attn), c(ff), dropout_rate), 1)\n",
    "\n",
    "#         self.high_encoder = Encoder(EncoderLayer(hidden_size, c(attn), c(ff), dropout_rate), 1)\n",
    "       \n",
    "        self.opt = nn.Linear(\n",
    "            in_features=(hidden_size), out_features=1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "      \n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "\n",
    "    def forward(self, source: List[List[str]]) -> torch.Tensor:\n",
    "        \n",
    "        # Compute sentence lengths\n",
    "#         print(source)\n",
    "        source_lengths = [len(s) for s in source]\n",
    "\n",
    "\n",
    "        # Convert list of lists into tensors\n",
    "        total_src_padded = self.vocab.src.to_input_tensor(\n",
    "            source, device=self.device)   # Tensor: (src_len, b)\n",
    "        \n",
    "\n",
    "        \n",
    "       \n",
    "        enc_hiddens, first_hidden = self.encode(\n",
    "            total_src_padded)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        return enc_hiddens, source_lengths\n",
    "\n",
    "\n",
    "    \n",
    "    def encode(self, source_padded: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
    "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
    "\n",
    "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
    "                                        b = batch_size, src_len = maximum source sentence length. Note that\n",
    "                                       these have already been sorted in order of longest to shortest sentence.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
    "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
    "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
    "                                                hidden state and cell.\n",
    "        \"\"\"\n",
    "        enc_hiddens, dec_init_state = None, None\n",
    "\n",
    "        # print(source_padded.shape)\n",
    "        source_padded = source_padded.permute(1,0) # b t \n",
    "#         print(source_padded.shape)\n",
    "        src_mask = (source_padded != 0).unsqueeze(-2)\n",
    "\n",
    "        X = self.model_embeddings.source(source_padded)\n",
    "\n",
    "        \n",
    "        enc_hiddens = self.encoder(X, src_mask) # b t h\n",
    "        first_hidden = enc_hiddens[:,0,:]\n",
    "\n",
    "       \n",
    "        return enc_hiddens, first_hidden\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
    "        \"\"\"\n",
    "        return self.model_embeddings.source.weight.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize source vocabulary ..\n",
      "number of word types: 135, number of word types w/ frequency >= 0: 135\n"
     ]
    }
   ],
   "source": [
    "from docopt import docopt\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "from utils1 import read_corpus\n",
    "from vocab import Vocab, VocabEntry\n",
    "\n",
    "vocab = Vocab.build(cls_left_diag_li+cls_right_diag_li, 50000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MM_transformer_encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(MM_transformer_encoder, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        #self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        #self.d_k = d_k  \n",
    "        #self.d_v = d_v # the two can be equal\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.embed = nn.Linear(self.input_dim, self.d_model)\n",
    "\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 5000)\n",
    "       \n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "        self.output = nn.Linear(self.d_model, self.output_dim)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "        #self.sparsemax = Sparsemax(dim=0)\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demographic = demo_input\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "     \n",
    "        input = self.embed(input)\n",
    "#         print(input.shape)\n",
    "        input = self.PositionalEncoding(input)# b t d_model\n",
    "#         posi_input = embed_input# b t d_model\n",
    "\n",
    "        \n",
    "#         mask = subsequent_mask(time_step).to(device) # 1 t t 下三角\n",
    "#         print(mask)\n",
    "        contexts = self.SublayerConnection(input, lambda x: self.MultiHeadedAttention(input, input, input, mask))# b t d_model\n",
    "        #contexts = self.MultiHeadedAttention(qs, ks, vs, mask)# b t h\n",
    "\n",
    "        contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))# b t d_model\n",
    "        \n",
    "#         contexts = contexts.view(batch_size, 16 * time_step)\n",
    "\n",
    "#\n",
    "#         os = []\n",
    "#         for j in range(contexts.shape[0]):\n",
    "#             os.append(contexts[j,lens[j]-1])\n",
    "            \n",
    "#         os = torch.stack(os)\n",
    "    \n",
    "#         output = self.output(os)# b t 1\n",
    "#         output = self.sigmoid(output)\n",
    "          \n",
    "        return contexts # b t h\n",
    "    #, self.MultiHeadedAttention.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    b = x.size(0)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(b, b)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(b, b).t()\n",
    "    dist = xx+yy-2*torch.mm(x, y.t())\n",
    "    return dist \n",
    "\n",
    "def guassian_kernel(source, kernel_mul=2.0, kernel_num=1, fix_sigma=None):\n",
    "    n = source.size(0)\n",
    "    L2_distance = euclidean_dist(source, source)\n",
    "\n",
    "        \n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n**2-n)\n",
    "    \n",
    "    if bandwidth < 1e-3:\n",
    "        bandwidth = 1\n",
    "    \n",
    "    bandwidth /= kernel_mul ** (kernel_num//2)\n",
    "    bandwidth_list = [bandwidth*(kernel_mul**i) for i in range(kernel_num)]\n",
    "    kernel_val = [torch.exp(-L2_distance/bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "    return sum(kernel_val)/len(kernel_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our MAPLE framework\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "#         print(self.dropout)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha# 4 leakyrelu\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.attention = None\n",
    "\n",
    "    def forward(self, input, adj):# V N, V V\n",
    "        h = torch.mm(input, self.W)# V O\n",
    "        N = h.size()[0]# NUM OF V\n",
    "\n",
    "        # V*V O ->123412341234, V*V O -> 111222333444, V V 2O\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))# V V\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)# V V\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        self.attention = attention\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)# V V\n",
    "        h_prime = torch.matmul(attention, h)# V N\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "        \n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "    \n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features).float())\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features).float())\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        std = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, adj, x):\n",
    "        y = torch.mm(x.float(), self.weight.float())\n",
    "        output = torch.mm(adj.float(), y.float())\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias.float()\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class M3Care(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embed_size, output_dim=1, keep_prob=1):\n",
    "        super(M3Care, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "        self.modal_num = 3\n",
    "        \n",
    "        self.NLP_model = NMT_tran(embed_size=embed_size,\n",
    "                hidden_size=hidden_dim,\n",
    "                dropout_rate= 1 - self.keep_prob,\n",
    "                vocab=vocab).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        self.res2hidden1 = nn.Linear(1000, self.hidden_dim)\n",
    "#         self.res2hidden1 = nn.Linear(1000, self.hidden_dim)\n",
    "#         self.linear_s = nn.Linear(28, self.hidden_dim)\n",
    "#         self.linear2 = nn.Linear(self.hidden_dim *5, self.output_dim)\n",
    "        self.MM_model = MM_transformer_encoder(input_dim = self.hidden_dim, d_model = self.hidden_dim,  \\\n",
    "                                                      MHD_num_head = 4 , d_ff = self.hidden_dim*4, output_dim = 1).to(device)\n",
    "        self.MM_model2 = MM_transformer_encoder(input_dim = self.hidden_dim, d_model = self.hidden_dim,  \\\n",
    "                                                      MHD_num_head = 1 , d_ff = self.hidden_dim*4, output_dim = 1).to(device)\n",
    "\n",
    "\n",
    "        self.token_type_embeddings = nn.Embedding(6, self.hidden_dim)\n",
    "        self.token_type_embeddings.apply(init_weights)\n",
    "        \n",
    "        \n",
    "        self.sep_token_embeddings = nn.Embedding(6, self.hidden_dim)\n",
    "        self.sep_token_embeddings.apply(init_weights)\n",
    "        \n",
    "        self.PositionalEncoding = PositionalEncoding(self.hidden_dim, dropout = 0, max_len = 5000)\n",
    "#         emb_data = self.token_type_embeddings.weight.data\n",
    "#         self.token_type_embeddings = nn.Embedding(3, hs)\n",
    "#         self.token_type_embeddings.apply(objectives.init_weights)\n",
    "#         self.token_type_embeddings.weight.data[0, :] = emb_data[0, :]\n",
    "#         self.token_type_embeddings.weight.data[1, :] = emb_data[1, :]\n",
    "#         self.token_type_embeddings.weight.data[2, :] = emb_data[1, :]\n",
    "\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "        self.proj1 = nn.Linear(self.hidden_dim *5, self.hidden_dim*2 )\n",
    "        self.proj2 = nn.Linear(self.hidden_dim *5, self.hidden_dim *5)\n",
    "        self.out_layer = nn.Linear(self.hidden_dim *2, self.output_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.threshold = nn.Parameter(torch.ones(size=(1,))+1)\n",
    "        self.simiProj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        \n",
    "        self.resnet18 =  models.resnet18()\n",
    "        \n",
    "        self.selu=nn.SELU()\n",
    "        \n",
    "    \n",
    "        self.bn = nn.BatchNorm1d(self.hidden_dim)\n",
    "    \n",
    "        self.simiProj = clones(torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "            self.relu,\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "            self.relu,\n",
    "            torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "#             torch.nn.Softplus(),\n",
    "#             torch.nn.Linear(self.hidden_dim, self.hidden_dim, bias=True),\n",
    "        ), self.modal_num)\n",
    "        \n",
    "        self.GCN = clones(GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True),self.modal_num)\n",
    "        self.GCN_2 = clones(GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True),self.modal_num)\n",
    "        self.GCN_3 = clones(GraphConvolution(self.hidden_dim, self.hidden_dim, bias=True),self.modal_num)\n",
    "        \n",
    "        self.eps0 = nn.Parameter(torch.ones(size=(1,))+1)\n",
    "        self.eps1 = nn.Parameter(torch.ones(size=(1,))+1)\n",
    "        self.eps2 = nn.Parameter(torch.ones(size=(1,))+1)\n",
    "\n",
    "        \n",
    "        self.weight1 = clones(nn.Linear(self.hidden_dim, 1), self.modal_num)\n",
    "        self.weight2 = clones(nn.Linear(self.hidden_dim, 1), self.modal_num)\n",
    "\n",
    "    def forward(self, input, left_fundus, right_fundus, left_diag, right_diag, l_r_masks):\n",
    "        \n",
    "        \n",
    "        values_hidden = self.relu(self.linear1(input))# b 1\n",
    "        val_mask = length_to_mask(torch.ones((values_hidden.shape[0],1)).int().squeeze()).unsqueeze(1).to(device).int()\n",
    "       \n",
    "#         left_diag_contexts = torch.cat([self.sep_token_embeddings(torch.zeros_like(left_diag_lens).to(device).long()).unsqueeze(1), pd_contexts], dim=1)\n",
    "#         left_diag_mask = torch.cat([torch.ones_like(left_diag_lens).to(device).unsqueeze(-1).unsqueeze(-1), left_diag_mask.int()], dim=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(left_diag)\n",
    "        left_diag_contexts, left_diag_lens = self.NLP_model(left_diag)# b t h\n",
    "#         left_diag_lens = torch.tensor(left_diag_lens, dtype=torch.float32).to(device).int()\n",
    "        \n",
    "        left_diag_contexts = self.relu(left_diag_contexts)\n",
    "        left_diag_mask = length_to_mask(torch.from_numpy(np.array(left_diag_lens))).unsqueeze(1).to(device)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "#         print(right_diag)\n",
    "        right_diag_contexts, right_diag_lens = self.NLP_model(right_diag)# b t h\n",
    "        right_diag_contexts = self.relu(right_diag_contexts)\n",
    "        right_diag_mask = length_to_mask(torch.from_numpy(np.array(right_diag_lens))).unsqueeze(1).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        left_f = self.relu(self.res2hidden1(self.resnet18(left_fundus)))\n",
    "        left_f_mask = length_to_mask(torch.ones((values_hidden.shape[0],1)).int().squeeze()).unsqueeze(1).to(device).int()\n",
    "        \n",
    "        \n",
    "        \n",
    "        right_f = self.relu(self.res2hidden1(self.resnet18(right_fundus)))\n",
    "        right_f_mask = length_to_mask(torch.ones((values_hidden.shape[0],1)).int().squeeze()).unsqueeze(1).to(device).int()\n",
    "        \n",
    "        \n",
    "        \n",
    "        values_hidden00 = values_hidden\n",
    "        left_diag_hidden00 = torch.zeros_like(left_diag_contexts[:, 0 ])\n",
    "        right_diag_hidden00 = torch.zeros_like(left_diag_contexts[:, 0 ])\n",
    "        left_f_hidden00 = torch.zeros_like(left_diag_contexts[:, 0 ])\n",
    "        right_f_hidden00 = torch.zeros_like(left_diag_contexts[:, 0 ])\n",
    "        for j in range(values_hidden00.shape[0]):\n",
    "            left_diag_hidden00[j] = left_diag_contexts[j, 0 ]\n",
    "            right_diag_hidden00[j] = right_diag_contexts[j, 0]\n",
    "            left_f_hidden00[j] = left_f[j]\n",
    "            right_f_hidden00[j] = right_f[j]\n",
    "            \n",
    "#         left_diag_hidden00 = torch.stack(left_diag_hidden0)\n",
    "#         right_diag_hidden00 = torch.stack(right_diag_hidden0)\n",
    "#         left_f_hidden00 = torch.stack(left_f_hidden0)\n",
    "#         right_f_hidden00 = torch.stack(right_f_hidden0)\n",
    "        \n",
    "        \n",
    "        left_diag_mask_ = torch.from_numpy(np.array(l_r_masks[0])).to(device).unsqueeze(1)# b 1\n",
    "        right_diag_mask_ = torch.from_numpy(np.array(l_r_masks[1])).to(device).unsqueeze(1)\n",
    "        \n",
    "        left_diag_mask2 = left_diag_mask_ * left_diag_mask_.permute(1,0)\n",
    "        right_diag_mask2 = right_diag_mask_ * right_diag_mask_.permute(1,0)\n",
    "        \n",
    "        \n",
    "        values_hidden_mat = guassian_kernel(self.bn(self.relu(self.simiProj[0](values_hidden00))), kernel_mul=2.0, kernel_num=3)\n",
    "        values_hidden_mat2 = guassian_kernel(self.bn(values_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        values_hidden_mat = ((1-self.sigmoid(self.eps0))*values_hidden_mat+self.sigmoid(self.eps0))*values_hidden_mat2\n",
    "#         values_hidden_mat = values_hidden_mat*dp_mask2\n",
    "\n",
    "        \n",
    "        left_diag_hidden_mat = guassian_kernel(self.bn(self.relu(self.simiProj[1](left_diag_hidden00))), kernel_mul=2.0, kernel_num=3)\n",
    "        left_diag_hidden_mat2 = guassian_kernel(self.bn(left_diag_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        left_diag_hidden_mat = ((1-self.sigmoid(self.eps1))*left_diag_hidden_mat+self.sigmoid(self.eps1))*left_diag_hidden_mat2\n",
    "        left_diag_hidden_mat = left_diag_hidden_mat*left_diag_mask2\n",
    "        \n",
    "        \n",
    "        right_diag_hidden_mat = guassian_kernel(self.bn(self.relu(self.simiProj[1](right_diag_hidden00))), kernel_mul=2.0, kernel_num=3)\n",
    "        right_diag_hidden_mat2 = guassian_kernel(self.bn(right_diag_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        right_diag_hidden_mat = ((1-self.sigmoid(self.eps1))*right_diag_hidden_mat+self.sigmoid(self.eps1))*right_diag_hidden_mat2\n",
    "        right_diag_hidden_mat = right_diag_hidden_mat*right_diag_mask2\n",
    "    \n",
    "\n",
    "    \n",
    "        left_f_hidden_mat = guassian_kernel(self.bn(self.relu(self.simiProj[2](left_f_hidden00))), kernel_mul=2.0, kernel_num=3)\n",
    "        left_f_hidden_mat2 = guassian_kernel(self.bn(left_f_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        left_f_hidden_mat = ((1-self.sigmoid(self.eps2))*left_f_hidden_mat+self.sigmoid(self.eps2))*left_f_hidden_mat2\n",
    "        \n",
    "        right_f_hidden_mat = guassian_kernel(self.bn(self.relu(self.simiProj[2](right_f_hidden00))), kernel_mul=2.0, kernel_num=3)\n",
    "        right_f_hidden_mat2 = guassian_kernel(self.bn(right_f_hidden00), kernel_mul=2.0, kernel_num=3)\n",
    "        right_f_hidden_mat = ((1-self.sigmoid(self.eps2))*right_f_hidden_mat+self.sigmoid(self.eps2))*right_f_hidden_mat2\n",
    "        \n",
    "        diff1 = torch.abs(torch.norm(self.simiProj[0](values_hidden00)) - torch.norm(values_hidden00))\n",
    "        diff2 = torch.abs(torch.norm(self.simiProj[1](left_diag_hidden00)) - torch.norm(left_diag_hidden00))\n",
    "        diff22 = torch.abs(torch.norm(self.simiProj[1](right_diag_hidden00)) - torch.norm(right_diag_hidden00))\n",
    "        \n",
    "        diff3 = torch.abs(torch.norm(self.simiProj[2](left_f_hidden00)) - torch.norm(left_f_hidden00))\n",
    "        diff33 = torch.abs(torch.norm(self.simiProj[2](right_f_hidden00)) - torch.norm(right_f_hidden00))\n",
    "        \n",
    "        sum_of_diff = diff1+diff2+diff22+diff3+diff33\n",
    "        \n",
    "        similar_score = (values_hidden_mat+ left_diag_hidden_mat + right_diag_hidden_mat + left_f_hidden_mat +\\\n",
    "                         right_f_hidden_mat)/ \\\n",
    "        (torch.ones_like(right_diag_mask2)+ torch.ones_like(right_diag_mask2) + torch.ones_like(right_diag_mask2) \\\n",
    "         + right_diag_mask2 +  left_diag_mask2 )\n",
    "        \n",
    "\n",
    "\n",
    "        similar_score = self.relu(similar_score - self.sigmoid(self.threshold)[0])  \n",
    "        temp_thresh = self.sigmoid(self.threshold)[0]\n",
    "        bin_mask = similar_score>0\n",
    "        similar_score = similar_score + bin_mask * temp_thresh.detach()\n",
    "\n",
    " \n",
    "        \n",
    "        left_diag_hidden0 = self.relu(self.GCN[0](similar_score*left_diag_mask2, left_diag_hidden00))\n",
    "        left_diag_hidden0 = self.relu(self.GCN_2[0](similar_score*left_diag_mask2, left_diag_hidden0))\n",
    "\n",
    "        right_diag_hidden0 = self.relu(self.GCN[1](similar_score*right_diag_mask2, right_diag_hidden00))\n",
    "        right_diag_hidden0 = self.relu(self.GCN_2[1](similar_score*right_diag_mask2, right_diag_hidden0))  \n",
    "        \n",
    "        \n",
    "        left_diag_weight1=torch.sigmoid(self.weight1[0](left_diag_hidden0))\n",
    "        left_diag_weight2 = torch.sigmoid(self.weight2[0](left_diag_hidden00 ))\n",
    "        left_diag_weight1 = left_diag_weight1/(left_diag_weight1+left_diag_weight2)\n",
    "        left_diag_weight2= 1-left_diag_weight1\n",
    "        \n",
    "        right_diag_weight1=torch.sigmoid(self.weight1[1](right_diag_hidden0))\n",
    "        right_diag_weight2 = torch.sigmoid(self.weight2[1](right_diag_hidden00 ))\n",
    "        right_diag_weight1 = right_diag_weight1/(right_diag_weight1+right_diag_weight2)\n",
    "        right_diag_weight2= 1-right_diag_weight1\n",
    "        \n",
    "        \n",
    "\n",
    "        final_left_diag = left_diag_weight1*left_diag_hidden0+left_diag_weight2*left_diag_hidden00\n",
    "        final_right_diag = right_diag_weight1*right_diag_hidden0+right_diag_weight2*right_diag_hidden00\n",
    "        \n",
    "        \n",
    "        right_diag_contexts_ = torch.zeros_like(right_diag_contexts)\n",
    "        for i in range(values_hidden00.shape[0]):\n",
    "            right_diag_contexts_[i] = right_diag_contexts[i]\n",
    "            if right_diag_mask_[i][0] != 1:\n",
    "                right_diag_contexts_[i,0] = right_diag_hidden0[i]\n",
    "\n",
    "            else:\n",
    "                 right_diag_contexts_[i,0] = final_right_diag[i]\n",
    "                    \n",
    "        left_diag_contexts_ = torch.zeros_like(left_diag_contexts)\n",
    "        for i in range(values_hidden00.shape[0]):\n",
    "            left_diag_contexts_[i] = left_diag_contexts[i]\n",
    "            if left_diag_mask_[i][0] != 1:\n",
    "                left_diag_contexts_[i,0] = left_diag_hidden0[i]\n",
    "\n",
    "            else:\n",
    "                 left_diag_contexts_[i,0] = final_left_diag[i]\n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "        left_diag_contexts = self.PositionalEncoding(left_diag_contexts_)  +  \\\n",
    "            self.token_type_embeddings(torch.ones_like(left_diag_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "#         left_diag_contexts = torch.cat([self.sep_token_embeddings(torch.zeros_like(torch.from_numpy(np.array(left_diag_lens))).to(device).long()).unsqueeze(1), left_diag_contexts], dim=1)\n",
    "#         left_diag_mask = torch.cat([torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).int().to(device).unsqueeze(-1).unsqueeze(-1), left_diag_mask.int()], dim=-1)\n",
    "        \n",
    "        \n",
    "        values_hidden = values_hidden.unsqueeze(1)  +  \\\n",
    "            self.token_type_embeddings(torch.zeros_like(val_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "        \n",
    "        right_diag_contexts = self.PositionalEncoding(right_diag_contexts_)  +  \\\n",
    "            self.token_type_embeddings(2*torch.ones_like(right_diag_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "\n",
    "#         right_diag_contexts = torch.cat([self.sep_token_embeddings(torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).to(device).long()).unsqueeze(1), right_diag_contexts], dim=1)\n",
    "#         right_diag_mask = torch.cat([torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).int().to(device).unsqueeze(-1).unsqueeze(-1), right_diag_mask.int()], dim=-1)\n",
    "        \n",
    "        left_f_contexts = left_f.unsqueeze(1)  +  \\\n",
    "            self.token_type_embeddings(3* torch.ones_like(left_f_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "#         left_f_contexts = torch.cat([self.sep_token_embeddings(2* torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).to(device).long()).unsqueeze(1), left_f], dim=1)\n",
    "#         left_f_mask = torch.cat([torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).int().to(device).unsqueeze(-1).unsqueeze(-1), left_f_mask.int()], dim=-1)\n",
    "        \n",
    "        \n",
    "        right_f_contexts = right_f.unsqueeze(1)  +  \\\n",
    "            self.token_type_embeddings(4* torch.ones_like(right_f_mask.permute(0,2,1).squeeze(-1)).to(device).long())\n",
    "#         right_f_contexts = torch.cat([self.sep_token_embeddings(3* torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).to(device).long()).unsqueeze(1), right_f], dim=1)\n",
    "#         right_f_mask = torch.cat([torch.ones_like(torch.from_numpy(np.array(left_diag_lens))).int().to(device).unsqueeze(-1).unsqueeze(-1), right_f_mask.int()], dim=-1)\n",
    "           \n",
    "        \n",
    "        \n",
    "        z0 = torch.cat([values_hidden, left_diag_contexts, right_diag_contexts, left_f_contexts, \\\n",
    "                        right_f_contexts], dim=1)\n",
    "        z0_mask = torch.cat([val_mask.int(), left_diag_mask.int(), right_diag_mask.int(), left_f_mask.int(), \\\n",
    "                             right_f_mask.int()], dim=-1).int()\n",
    "        \n",
    "        z1 = self.relu(self.MM_model(z0, z0_mask))# b 1\n",
    "\n",
    "\n",
    "        z2 = self.relu(self.MM_model2(z1, z0_mask))# b 1\n",
    "    \n",
    "    \n",
    "        val_hidden = z2[:,0,:]\n",
    "        left_diag_hidden = []\n",
    "        right_diag_hidden = []\n",
    "        left_f_hidden = []\n",
    "        right_f_hidden = []\n",
    "        for j in range(z2.shape[0]):\n",
    "            left_diag_hidden.append(z2[j,1  ])\n",
    "            right_diag_hidden.append(z2[j,1  + left_diag_contexts.shape[1]])\n",
    "            left_f_hidden.append(z2[j,1 +left_diag_contexts.shape[1] + right_diag_contexts.shape[1] ])\n",
    "            right_f_hidden.append(z2[j,1 +left_diag_contexts.shape[1] + right_diag_contexts.shape[1] \\\n",
    "                                    + left_f_contexts.shape[1]])\n",
    "            \n",
    "        left_diag_hidden = torch.stack(left_diag_hidden)\n",
    "        right_diag_hidden = torch.stack(right_diag_hidden)\n",
    "        left_f_hidden = torch.stack(left_f_hidden)\n",
    "        right_f_hidden = torch.stack(right_f_hidden)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        combined_hidden = torch.cat((val_hidden,left_diag_hidden,right_diag_hidden,left_f_hidden, \\\n",
    "                                     right_f_hidden),-1)#b n h\n",
    "\n",
    "\n",
    "        \n",
    "        last_hs_proj = self.dropout(F.relu(self.proj1(combined_hidden)))\n",
    "\n",
    "        \n",
    "        output = self.out_layer(last_hs_proj).squeeze(-1)\n",
    "\n",
    "        return output, sum_of_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true, weight=None):\n",
    "    loss = torch.nn.BCEWithLogitsLoss(weight=weight)\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([ # C H W input shape needed\n",
    "            T.ToPILImage(),\n",
    "#             T.Resize(size = (224,224)),\n",
    "#             T.Resize(255),  # We use single int value inside a list due to torchscript type restrictions\n",
    "            T.CenterCrop(1728),\n",
    "            T.Resize(size = (224,224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./archive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.4554 ------------\n",
      "Fold 0, Epoch 0, ave_auc_micro = 0.4554, ave_auc_macro = 0.3945, coverage_error = 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.5387 ------------\n",
      "Fold 0, Epoch 1, ave_auc_micro = 0.5387, ave_auc_macro = 0.5415, coverage_error = 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.5628 ------------\n",
      "Fold 0, Epoch 2, ave_auc_micro = 0.5628, ave_auc_macro = 0.6033, coverage_error = 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.5813 ------------\n",
      "Fold 0, Epoch 3, ave_auc_micro = 0.5813, ave_auc_macro = 0.5688, coverage_error = 0.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 4, ave_auc_micro = 0.5370, ave_auc_macro = 0.5509, coverage_error = 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 5, ave_auc_micro = 0.5498, ave_auc_macro = 0.5890, coverage_error = 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.5854 ------------\n",
      "Fold 0, Epoch 6, ave_auc_micro = 0.5854, ave_auc_macro = 0.6510, coverage_error = 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.6075 ------------\n",
      "Fold 0, Epoch 7, ave_auc_micro = 0.6075, ave_auc_macro = 0.6841, coverage_error = 0.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.6720 ------------\n",
      "Fold 0, Epoch 8, ave_auc_micro = 0.6720, ave_auc_macro = 0.6725, coverage_error = 0.3114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.6883 ------------\n",
      "Fold 0, Epoch 9, ave_auc_micro = 0.6883, ave_auc_macro = 0.6598, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 10, ave_auc_micro = 0.6594, ave_auc_macro = 0.6782, coverage_error = 0.3143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.7025 ------------\n",
      "Fold 0, Epoch 11, ave_auc_micro = 0.7025, ave_auc_macro = 0.6747, coverage_error = 0.2743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.7035 ------------\n",
      "Fold 0, Epoch 12, ave_auc_micro = 0.7035, ave_auc_macro = 0.7170, coverage_error = 0.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.7247 ------------\n",
      "Fold 0, Epoch 13, ave_auc_micro = 0.7247, ave_auc_macro = 0.6947, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 14, ave_auc_micro = 0.7036, ave_auc_macro = 0.7159, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 15, ave_auc_micro = 0.7093, ave_auc_macro = 0.7348, coverage_error = 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.7417 ------------\n",
      "Fold 0, Epoch 16, ave_auc_micro = 0.7417, ave_auc_macro = 0.7060, coverage_error = 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.7982 ------------\n",
      "Fold 0, Epoch 17, ave_auc_micro = 0.7982, ave_auc_macro = 0.7871, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 18, ave_auc_micro = 0.7496, ave_auc_macro = 0.7171, coverage_error = 0.2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.7999 ------------\n",
      "Fold 0, Epoch 19, ave_auc_micro = 0.7999, ave_auc_macro = 0.7583, coverage_error = 0.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - ave_auc_micro: 0.8063 ------------\n",
      "Fold 0, Epoch 20, ave_auc_micro = 0.8063, ave_auc_macro = 0.7867, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 21, ave_auc_micro = 0.7516, ave_auc_macro = 0.7340, coverage_error = 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 22, ave_auc_micro = 0.7781, ave_auc_macro = 0.7268, coverage_error = 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 23, ave_auc_micro = 0.7882, ave_auc_macro = 0.7597, coverage_error = 0.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 24, ave_auc_micro = 0.7593, ave_auc_macro = 0.7350, coverage_error = 0.2371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 25, ave_auc_micro = 0.7568, ave_auc_macro = 0.7302, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 26, ave_auc_micro = 0.7843, ave_auc_macro = 0.7728, coverage_error = 0.2314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 27, ave_auc_micro = 0.7756, ave_auc_macro = 0.7450, coverage_error = 0.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 28, ave_auc_micro = 0.7586, ave_auc_macro = 0.7202, coverage_error = 0.2771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 29, ave_auc_micro = 0.7525, ave_auc_macro = 0.7181, coverage_error = 0.2371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 30, ave_auc_micro = 0.7647, ave_auc_macro = 0.7447, coverage_error = 0.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 31, ave_auc_micro = 0.7669, ave_auc_macro = 0.7469, coverage_error = 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 32, ave_auc_micro = 0.7672, ave_auc_macro = 0.7466, coverage_error = 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 33, ave_auc_micro = 0.7673, ave_auc_macro = 0.7478, coverage_error = 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 34, ave_auc_micro = 0.7650, ave_auc_macro = 0.7428, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 35, ave_auc_micro = 0.7518, ave_auc_macro = 0.7402, coverage_error = 0.2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 36, ave_auc_micro = 0.7864, ave_auc_macro = 0.7796, coverage_error = 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 37, ave_auc_micro = 0.7275, ave_auc_macro = 0.7173, coverage_error = 0.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 38, ave_auc_micro = 0.7362, ave_auc_macro = 0.7289, coverage_error = 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 39, ave_auc_micro = 0.7494, ave_auc_macro = 0.7272, coverage_error = 0.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 40, ave_auc_micro = 0.7393, ave_auc_macro = 0.7231, coverage_error = 0.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 41, ave_auc_micro = 0.7271, ave_auc_macro = 0.7100, coverage_error = 0.2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 42, ave_auc_micro = 0.7483, ave_auc_macro = 0.7373, coverage_error = 0.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 43, ave_auc_micro = 0.7251, ave_auc_macro = 0.7147, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 44, ave_auc_micro = 0.7407, ave_auc_macro = 0.7273, coverage_error = 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 45, ave_auc_micro = 0.7538, ave_auc_macro = 0.7384, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 46, ave_auc_micro = 0.7325, ave_auc_macro = 0.7219, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 47, ave_auc_micro = 0.7668, ave_auc_macro = 0.7587, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 48, ave_auc_micro = 0.7493, ave_auc_macro = 0.7409, coverage_error = 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 49, ave_auc_micro = 0.7698, ave_auc_macro = 0.7598, coverage_error = 0.2571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 50, ave_auc_micro = 0.7788, ave_auc_macro = 0.7684, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 51, ave_auc_micro = 0.7766, ave_auc_macro = 0.7661, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 52, ave_auc_micro = 0.7452, ave_auc_macro = 0.7400, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 53, ave_auc_micro = 0.7629, ave_auc_macro = 0.7523, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 54, ave_auc_micro = 0.7265, ave_auc_macro = 0.7184, coverage_error = 0.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 55, ave_auc_micro = 0.7420, ave_auc_macro = 0.7364, coverage_error = 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 56, ave_auc_micro = 0.7548, ave_auc_macro = 0.7518, coverage_error = 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 57, ave_auc_micro = 0.7536, ave_auc_macro = 0.7495, coverage_error = 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 58, ave_auc_micro = 0.7249, ave_auc_macro = 0.7139, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 59, ave_auc_micro = 0.7377, ave_auc_macro = 0.7255, coverage_error = 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 60, ave_auc_micro = 0.7264, ave_auc_macro = 0.7134, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 61, ave_auc_micro = 0.7036, ave_auc_macro = 0.6980, coverage_error = 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 62, ave_auc_micro = 0.7225, ave_auc_macro = 0.7104, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 63, ave_auc_micro = 0.7151, ave_auc_macro = 0.7126, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 64, ave_auc_micro = 0.7376, ave_auc_macro = 0.7289, coverage_error = 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 65, ave_auc_micro = 0.7364, ave_auc_macro = 0.7321, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 66, ave_auc_micro = 0.7314, ave_auc_macro = 0.7311, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 67, ave_auc_micro = 0.7418, ave_auc_macro = 0.7352, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 68, ave_auc_micro = 0.7383, ave_auc_macro = 0.7369, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 69, ave_auc_micro = 0.7300, ave_auc_macro = 0.7290, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 70, ave_auc_micro = 0.7268, ave_auc_macro = 0.7271, coverage_error = 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 71, ave_auc_micro = 0.7332, ave_auc_macro = 0.7295, coverage_error = 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 72, ave_auc_micro = 0.7187, ave_auc_macro = 0.7158, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 73, ave_auc_micro = 0.7155, ave_auc_macro = 0.7106, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 74, ave_auc_micro = 0.7063, ave_auc_macro = 0.7040, coverage_error = 0.2771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 75, ave_auc_micro = 0.7096, ave_auc_macro = 0.7070, coverage_error = 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 76, ave_auc_micro = 0.7026, ave_auc_macro = 0.7034, coverage_error = 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 77, ave_auc_micro = 0.7161, ave_auc_macro = 0.7129, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 78, ave_auc_micro = 0.7167, ave_auc_macro = 0.7134, coverage_error = 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 79, ave_auc_micro = 0.7367, ave_auc_macro = 0.7293, coverage_error = 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 80, ave_auc_micro = 0.7204, ave_auc_macro = 0.7152, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 81, ave_auc_micro = 0.7278, ave_auc_macro = 0.7157, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 82, ave_auc_micro = 0.7167, ave_auc_macro = 0.7066, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 83, ave_auc_micro = 0.7412, ave_auc_macro = 0.7342, coverage_error = 0.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 84, ave_auc_micro = 0.7007, ave_auc_macro = 0.7048, coverage_error = 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 85, ave_auc_micro = 0.7152, ave_auc_macro = 0.7105, coverage_error = 0.2829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 86, ave_auc_micro = 0.7104, ave_auc_macro = 0.7130, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 87, ave_auc_micro = 0.7211, ave_auc_macro = 0.7244, coverage_error = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 89, ave_auc_micro = 0.7156, ave_auc_macro = 0.7150, coverage_error = 0.2571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 90, ave_auc_micro = 0.7261, ave_auc_macro = 0.7235, coverage_error = 0.2571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 91, ave_auc_micro = 0.7043, ave_auc_macro = 0.7011, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 92, ave_auc_micro = 0.7157, ave_auc_macro = 0.7117, coverage_error = 0.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 93, ave_auc_micro = 0.7216, ave_auc_macro = 0.7143, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 94, ave_auc_micro = 0.7102, ave_auc_macro = 0.7034, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 95, ave_auc_micro = 0.7173, ave_auc_macro = 0.7126, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 96, ave_auc_micro = 0.7226, ave_auc_macro = 0.7139, coverage_error = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 97, ave_auc_micro = 0.7179, ave_auc_macro = 0.7140, coverage_error = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 98, ave_auc_micro = 0.7138, ave_auc_macro = 0.7094, coverage_error = 0.2514\n",
      "Fold 0, Epoch 99, ave_auc_micro = 0.7157, ave_auc_macro = 0.7094, coverage_error = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "global_best = 0\n",
    "_global_best = 0\n",
    "\n",
    "\n",
    "\n",
    "history = []\n",
    "\n",
    "        \n",
    "model = M3Care(input_dim = X.shape[-1], hidden_dim = 128, embed_size = 128, output_dim = 3, keep_prob=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay = 1e-4)\n",
    "\n",
    "    \n",
    "\n",
    "file_name = './model/M3Care-model2' \n",
    "\n",
    "fold_train_loss = []\n",
    "fold_valid_loss = []\n",
    "\n",
    "best_auc_scores = 0\n",
    "best_ave_auc_micro = 0\n",
    "best_ave_auc_macro = 0\n",
    "best_coverage_error = 0\n",
    "best_label_ranking_loss = 0\n",
    "\n",
    "\n",
    "_best_auc_scores = 0\n",
    "_best_ave_auc_micro = 0\n",
    "_best_ave_auc_macro = 0\n",
    "_best_coverage_error = 0\n",
    "_best_label_ranking_loss = 0\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = []\n",
    "    counter_batch = 0\n",
    "    model.train()  \n",
    "#         print(0)\n",
    "    for step, (batch_x, batch_y, batch_left_fundus_images, batch_right_fundus_images, batch_left_diag, \\\n",
    "    batch_right_diag, l_r_masks) in enumerate(\n",
    "            batch_iter(train_x, train_y, train_left_fundus_images, train_right_fundus_images, train_left_diag,\n",
    "                        train_left_diag_mask, train_right_diag, train_right_diag_mask, \\\n",
    "                       batch_size, shuffle=True)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device).squeeze(-1)\n",
    "\n",
    "        batch_left_fundus_images = torch.tensor(batch_left_fundus_images, dtype=torch.float32).to(device)\n",
    "        batch_right_fundus_images = torch.tensor(batch_right_fundus_images, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "        opt, sum_of_diff= model(batch_x, batch_left_fundus_images, batch_right_fundus_images, \\\n",
    "                    batch_left_diag, batch_right_diag, l_r_masks)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y)\n",
    "#         print(BCE_Loss, sum_of_diff*1e-5)\n",
    "        model_loss =  BCE_Loss \n",
    "        loss = model_loss   + sum_of_diff*1e-7\n",
    "\n",
    "\n",
    "        epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    fold_train_loss.append(epoch_loss)\n",
    "\n",
    "    #Validation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        valid_true = []\n",
    "        valid_pred = []\n",
    "\n",
    "        for step, (batch_x, batch_y, batch_left_fundus_images, batch_right_fundus_images, batch_left_diag, \\\n",
    "    batch_right_diag, l_r_masks) in enumerate(\n",
    "            batch_iter(dev_x, dev_y, dev_left_fundus_images, dev_right_fundus_images, dev_left_diag,\n",
    "                        dev_left_diag_mask, dev_right_diag, dev_right_diag_mask, batch_size)):\n",
    "\n",
    "            batch_x = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device).squeeze(-1)\n",
    "\n",
    "            batch_left_fundus_images = torch.tensor(batch_left_fundus_images, dtype=torch.float32).to(device)\n",
    "            batch_right_fundus_images = torch.tensor(batch_right_fundus_images, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "            opt, sum_of_diff = model(batch_x, batch_left_fundus_images, batch_right_fundus_images, \\\n",
    "                        batch_left_diag, batch_right_diag, l_r_masks)\n",
    "\n",
    "\n",
    "            BCE_Loss = get_loss(opt, batch_y)\n",
    "\n",
    "            valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            y_pred += list(F.sigmoid(opt).cpu().detach().numpy())\n",
    "            y_true += list(batch_y.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        fold_valid_loss.append(valid_loss)\n",
    "\n",
    "        ret = metrics.print_metrics_multilabel(y_true, y_pred,verbose = 0)\n",
    "\n",
    "        history.append(ret)\n",
    "\n",
    "        cur_ave_auc_micro = ret['ave_auc_micro']\n",
    "        if cur_ave_auc_micro > best_ave_auc_micro:\n",
    "            best_y_true = y_true\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "            best_ave_auc_micro = cur_ave_auc_micro\n",
    "\n",
    "            best_auc_scores = ret['auc_scores']\n",
    "            best_ave_auc_macro = ret['ave_auc_macro']\n",
    "            best_coverage_error = ret['coverage_error']\n",
    "            best_label_ranking_loss = ret['label_ranking_loss']\n",
    "\n",
    "            state = {\n",
    "                'net': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': each_epoch\n",
    "            }\n",
    "            torch.save(state, file_name)\n",
    "\n",
    "\n",
    "            print('------------ Save best model - ave_auc_micro: %.4f ------------'%cur_ave_auc_micro)\n",
    "\n",
    "        cur_ave_auc_macro = ret['ave_auc_macro']\n",
    "        \n",
    "\n",
    "    print('Fold %d, Epoch %d, ave_auc_micro = %.4f, ave_auc_macro = %.4f, coverage_error = %.4f' \\\n",
    "          %(fold_count, each_epoch, ret['ave_auc_micro'], ret['ave_auc_macro'], ret['coverage_error']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M3Care(\n",
       "  (NLP_model): NMT_tran(\n",
       "    (model_embeddings): ModelEmbeddings(\n",
       "      (source): Embedding(139, 128, padding_idx=0)\n",
       "    )\n",
       "    (position): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): MultiHeadedAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): PositionwiseFeedForward(\n",
       "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.5, inplace=False)\n",
       "          )\n",
       "          (sublayer): ModuleList(\n",
       "            (0): SublayerConnection(\n",
       "              (norm): LayerNorm()\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "            (1): SublayerConnection(\n",
       "              (norm): LayerNorm()\n",
       "              (dropout): Dropout(p=0.5, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm()\n",
       "    )\n",
       "    (opt): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (linear1): Linear(in_features=6, out_features=128, bias=True)\n",
       "  (res2hidden1): Linear(in_features=1000, out_features=128, bias=True)\n",
       "  (MM_model): MM_transformer_encoder(\n",
       "    (embed): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (PositionalEncoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (MultiHeadedAttention): MultiHeadedAttention(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (SublayerConnection): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "      (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (MM_model2): MM_transformer_encoder(\n",
       "    (embed): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (PositionalEncoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (MultiHeadedAttention): MultiHeadedAttention(\n",
       "      (linears): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (SublayerConnection): SublayerConnection(\n",
       "      (norm): LayerNorm()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "      (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (token_type_embeddings): Embedding(6, 128)\n",
       "  (sep_token_embeddings): Embedding(6, 128)\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (proj1): Linear(in_features=640, out_features=256, bias=True)\n",
       "  (proj2): Linear(in_features=640, out_features=640, bias=True)\n",
       "  (out_layer): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (simiProj): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (resnet18): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (selu): SELU()\n",
       "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (GCN): ModuleList(\n",
       "    (0): GraphConvolution()\n",
       "    (1): GraphConvolution()\n",
       "    (2): GraphConvolution()\n",
       "  )\n",
       "  (GCN_2): ModuleList(\n",
       "    (0): GraphConvolution()\n",
       "    (1): GraphConvolution()\n",
       "    (2): GraphConvolution()\n",
       "  )\n",
       "  (GCN_3): ModuleList(\n",
       "    (0): GraphConvolution()\n",
       "    (1): GraphConvolution()\n",
       "    (2): GraphConvolution()\n",
       "  )\n",
       "  (weight1): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (weight2): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(file_name+'')# try bmm\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1484, device='cuda:0')\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1484\n",
      "auc_scores [0.75771 0.97875 0.75978]\n",
      "ave_auc_micro 0.84934\n",
      "ave_auc_macro 0.83208\n",
      "ave_auc_weighted 0.83333\n",
      "ave_f1_micro 0.29851\n",
      "ave_f1_macro 0.20833\n",
      "coverage_error 0.19714\n",
      "label_ranking_loss 0.02286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zch/.local/lib/python3.6/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "batch_loss = [] \n",
    "y_true = []\n",
    "y_pred = []\n",
    "opts = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_left_fundus_images, batch_right_fundus_images, batch_left_diag, \\\n",
    "    batch_right_diag, l_r_masks) in enumerate(\n",
    "            batch_iter(test_x, test_y, test_left_fundus_images, test_right_fundus_images, test_left_diag,\n",
    "                        test_left_diag_mask, test_right_diag, test_right_diag_mask, batch_size)):\n",
    "\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device).squeeze(-1)\n",
    "\n",
    "        batch_left_fundus_images = torch.tensor(batch_left_fundus_images, dtype=torch.float32).to(device)\n",
    "        batch_right_fundus_images = torch.tensor(batch_right_fundus_images, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "        opt, sum_of_diff = model(batch_x, batch_left_fundus_images, batch_right_fundus_images, \\\n",
    "                    batch_left_diag, batch_right_diag, l_r_masks)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y)\n",
    "        print(BCE_Loss)\n",
    "#                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "        batch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "        opts += list(opt.cpu().detach().numpy())\n",
    "        y_pred += list(F.sigmoid(opt).cpu().detach().numpy())\n",
    "        y_true += list(batch_y.cpu().numpy())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "# y_pred = np.array(y_pred)\n",
    "# y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "ret = metrics.print_metrics_multilabel(y_true, y_pred, verbose = 0)\n",
    "for k, v in ret.items():\n",
    "    print(k, v.round(5))\n",
    "# print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n",
      "900/1000\n",
      "1000/1000\n",
      "test_loss 0.1477(0.0162)\n",
      "auc_micro 0.8503(0.0253)\n",
      "auc_macro 0.8328(0.0279)\n",
      "auc_weighted 0.8343(0.0296)\n",
      "coverage_error 0.1959(0.0272)\n",
      "label_ranking_loss 0.0226(0.0059)\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap\n",
    "N = len(y_true)\n",
    "N_idx = np.arange(N)\n",
    "K = 1000\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "opts = np.array(opts)\n",
    "\n",
    "auc_micro = []\n",
    "auc_macro = []\n",
    "auc_weighted = []\n",
    "coverage_error = []\n",
    "test_loss = []\n",
    "label_ranking_loss = []\n",
    "\n",
    "for i in range(K):\n",
    "    boot_idx = np.random.choice(N_idx, N, replace=True)\n",
    "    boot_true = np.array(y_true)[boot_idx]\n",
    "    boot_pred = y_pred[boot_idx, :]\n",
    "    boot_opt = opts[boot_idx, :]\n",
    "    \n",
    "    BCE_Loss = get_loss(torch.tensor(boot_opt), torch.tensor(boot_true)).detach().numpy()\n",
    "    test_loss.append(BCE_Loss)\n",
    "    \n",
    "    test_ret = metrics.print_metrics_multilabel(boot_true, boot_pred, verbose=0)\n",
    "    auc_micro.append(test_ret['ave_auc_micro'])\n",
    "    auc_macro.append(test_ret['ave_auc_macro'])\n",
    "    auc_weighted.append(test_ret['ave_auc_weighted'])\n",
    "    \n",
    "    coverage_error.append(test_ret['coverage_error'])\n",
    "    label_ranking_loss.append(test_ret['label_ranking_loss'])\n",
    "#     pre1.append(test_ret['prec1'])\n",
    "#     rec0.append(test_ret['rec0'])\n",
    "#     rec1.append(test_ret['rec1'])\n",
    "#     f1.append(test_ret['f1_score'])\n",
    "    if (i+1)%100 == 0:\n",
    "        print('%d/%d'%(i+1,K))\n",
    "\n",
    "print('test_loss %.4f(%.4f)'%(np.mean(test_loss), np.std(test_loss)))\n",
    "print('auc_micro %.4f(%.4f)'%(np.mean(auc_micro), np.std(auc_micro)))\n",
    "print('auc_macro %.4f(%.4f)'%(np.mean(auc_macro), np.std(auc_macro)))\n",
    "print('auc_weighted %.4f(%.4f)'%(np.mean(auc_weighted), np.std(auc_weighted)))\n",
    "\n",
    "print('coverage_error %.4f(%.4f)'%(np.mean(coverage_error), np.std(coverage_error)))\n",
    "print('label_ranking_loss %.4f(%.4f)'%(np.mean(label_ranking_loss), np.std(label_ranking_loss)))\n",
    "# print('pre1 = PPV = precision %.4f(%.4f)'%(np.mean(pre1), np.std(pre1)))\n",
    "# print('rec0 = TNR = specificity = selectivity %.4f(%.4f)'%(np.mean(rec0), np.std(rec0)))\n",
    "# print('rec1 = recall = TPR = sensitivity %.4f(%.4f)'%(np.mean(rec1), np.std(rec1)))\n",
    "# print('f1 %.4f(%.4f)'%(np.mean(f1), np.std(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
